<?xml version="1.0" ?><cherrytree><node name="脚本" prog_lang="custom-colors" readonly="False" tags="" unique_id="46"><rich_text>常用windows下脚本主要是bat和vbs脚本,因为我经常还原系统,带来桌面快捷方式遗失的情况
初始化环境变量，使软件可以正常使用
复制配置文件，包含密码或者软件配置
初始化快捷方式,还原系统后可以正常使用
notepad++编写脚本在windows下用ANSI编码
</rich_text><node name="win" prog_lang="sh" readonly="False" tags="" unique_id="48"><rich_text>########################################################java初始化环境变量########################################################
Dim MyVar
Set shell = CreateObject(&quot;Shell.Application&quot;)   
Set selFolder = shell.BrowseForFolder(0, &quot;请选择JAVA_HOME目录(如:D:/jdk1.5.0_16):&quot;, 0, ssfDRIVES)   
Set selFolderItem = selFolder.Self   
selPath = selFolderItem.Path   
dim wsh   
If (selPath  &lt;&gt;&quot;&quot;) Then
Set wsh = WScript.CreateObject(&quot;WScript.Shell&quot;)   
wsh.Environment(&quot;system&quot;).Item(&quot;JAVA_HOME&quot;)=selPath
wsh.Environment(&quot;system&quot;).Item(&quot;CLASSPATH&quot;)=wsh.Environment(&quot;system&quot;).Item(&quot;CLASSPATH&quot;)+&quot;;.;%JAVA_HOME%/lib/tools.jar;%JAVA_HOME%/lib/dt.jar;%JAVA_HOME%/bin;&quot;
wsh.Environment(&quot;system&quot;).Item(&quot;Path&quot;) = &quot;%JAVA_HOME%/bin;%JAVA_HOME%/jre/bin;&quot; +wsh.Environment(&quot;system&quot;).Item(&quot;path&quot;)
MyVar = MsgBox (&quot;恭喜，完成设置！&quot;, , &quot;MSG&quot;)
End If
########################################################git初始化环境变量########################################################
Dim MyVar
Set shell = CreateObject(&quot;Shell.Application&quot;)
Set selFolder = shell.BrowseForFolder(0, &quot;选择git的bin目录&quot;, 0, ssfDRIVES)
Set selFolderItem = selFolder.Self
gitPath = selFolderItem.Path
If (gitPath  &lt;&gt;&quot;&quot;) Then
dim wsh
Set wsh = WScript.CreateObject(&quot;WScript.Shell&quot;)
wsh.Environment(&quot;system&quot;).Item(&quot;Path&quot;) = gitPath +&quot;;&quot; +wsh.Environment(&quot;system&quot;).Item(&quot;path&quot;)
MyVar = MsgBox (&quot;恭喜，完成设置！&quot;, , &quot;MSG&quot;)
End If
########################################################git生成密钥########################################################
####ssh key在生成后保存在C:\Users\Administrator\.ssh 这个路径 #win7环境的路径####                                                      
####而全局name和email则保存在C:\Users\Administrator\.gitconfig 这个路径 #win7环境的路径####
####所以我们可以把这个目录的key和gitconfig这个文件进行备份，在恢复完系统或者其他原因利用批处理则可以恢复git环境####      
ssh-keygen -t rsa -C “78619808@qq.com” #生成密钥 必须在git-bash中运行
git config --global user.name &quot;kylinshine&quot;   #设置全局变量name
git config --global user.email &quot;78619808@qq.com&quot; #设置全局变量email
########################################################git复制配置文件########################################################
@echo off
if exist C:\Users\Administrator\.ssh rd /s /q &quot;C:\Users\Administrator\.ssh&quot;
if exist C:\Users\Administrator\.gitconfig del /s /q &quot;C:\Users\Administrator\.gitconfig&quot;
md C:\Users\Administrator\.ssh
xcopy .ssh C:\Users\Administrator\.ssh
copy .gitconfig C:\Users\Administrator\
echo 完成..
pause &gt;nul
########################################################初始化python的VBS脚本文件########################################################
Set shell = CreateObject(&quot;Shell.Application&quot;)
Set selFolder = shell.BrowseForFolder(0, &quot;选择python目录&quot;, 0, ssfDRIVES)
Set selFolderItem = selFolder.Self
pyPath = selFolderItem.Path
If (pyPath  &lt;&gt;&quot;&quot;) Then
dim wsh
Set wsh = WScript.CreateObject(&quot;WScript.Shell&quot;)
wsh.Environment(&quot;system&quot;).Item(&quot;Path&quot;) = pyPath +&quot;;&quot; +wsh.Environment(&quot;system&quot;).Item(&quot;path&quot;)
MyVar = MsgBox (&quot;恭喜，完成设置！&quot;, , &quot;MSG&quot;)
End If  
########################################################复制cherrytree配置文件########################################################
@echo off
if exist C:\Users\Administrator\AppData\Roaming\cherrytree\ rd /s /q &quot;C:\Users\Administrator\AppData\Roaming\cherrytree&quot;
md C:\Users\Administrator\AppData\Roaming\cherrytree
copy config.cfg C:\Users\Administrator\AppData\Roaming\cherrytree\
echo 完成..
pause &gt;nul
########################################################复制winbox配置文件########################################################
@echo off
if exist C:\Users\Administrator\AppData\Roaming\Mikrotik\Winbox\winbox.cfg del /s /q &quot;C:\Users\Administrator\AppData\Roaming\Mikrotik\Winbox\winbox.cfg&quot;
copy winbox.cfg C:\Users\Administrator\AppData\Roaming\Mikrotik\Winbox\
echo 完成..
pause &gt;nul
########################################################自动添加快捷方式到桌面########################################################
set WshShell=WScript.CreateObject(&quot;WScript.Shell&quot;)  
set fso=CreateObject(&quot;Scripting.FileSystemObject&quot;) 
strDesktop=WshShell.SpecialFolders(&quot;Desktop&quot;)  
If fso.fileExists(strDesktop &amp; &quot;\应用软件\360se.lnk&quot;) Then    '删除旧的'
fso.DeleteFile(strDesktop &amp; &quot;\应用软件\360se.lnk&quot;)
End If
set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\IntelliJ IDEA 2017.lnk&quot;)  '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\JetBrains\IntelliJ IDEA 2017.3\bin\idea64.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;IntelliJ IDEA &quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save
set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\TIM.lnk&quot;)  '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\Tencent\TIM\Bin\tim.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;tim&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save
set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\TIM.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\Tencent\TIM\Bin\tim.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;tim&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save
########################################################自动添加日期备注提交git########################################################
set THISDATETIME=add new content at %DATE:~0,4%%DATE:~5,2%%DATE:~8,2%%TIME:~0,2%%TIME:~3,2%%TIME:~6,2% 
git add mydata.ctd
git commit -m &quot;%THISDATETIME%&quot;
git push origin master
start https://github.com/kylinshine/cherry
########################################################maven初始化环境变量########################################################
Dim MyVar
Set shell = CreateObject(&quot;Shell.Application&quot;)
Set selFolder = shell.BrowseForFolder(0, &quot;选择maven的bin目录&quot;, 0, ssfDRIVES)
Set selFolderItem = selFolder.Self
mavenPath = selFolderItem.Path
If (mavenPath  &lt;&gt;&quot;&quot;) Then
dim wsh
Set wsh = WScript.CreateObject(&quot;WScript.Shell&quot;)
wsh.Environment(&quot;system&quot;).Item(&quot;Path&quot;) = mavenPath +&quot;;&quot; +wsh.Environment(&quot;system&quot;).Item(&quot;path&quot;)
MyVar = MsgBox (&quot;恭喜，完成设置！&quot;, , &quot;MSG&quot;)
End If
########################################################初始化环境变量和桌面图标初始化########################################################
set WshShell=WScript.CreateObject(&quot;WScript.Shell&quot;)  
set fso=CreateObject(&quot;Scripting.FileSystemObject&quot;) 
strDesktop=WshShell.SpecialFolders(&quot;Desktop&quot;)  
If fso.fileExists(strDesktop &amp; &quot;\应用软件\IntelliJ IDEA 2017.2.4 x64.lnk&quot;) Then    '删除旧的'
fso.DeleteFile(strDesktop &amp; &quot;\应用软件\IntelliJ IDEA 2017.2.4 x64.lnk&quot;)
End If
If fso.fileExists(strDesktop &amp; &quot;\应用软件\JetBrains PyCharm 2017.2.4 x64.lnk&quot;) Then    '删除旧的'
fso.DeleteFile(strDesktop &amp; &quot;\应用软件\JetBrains PyCharm 2017.2.4 x64.lnk&quot;)
End If
set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\IntelliJ IDEA 2017.lnk&quot;)  '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\JetBrains\IntelliJ IDEA 2017.3\bin\idea64.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;IntelliJ IDEA &quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save
set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\TIM.lnk&quot;)  '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\Tencent\TIM\Bin\tim.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;tim&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save
set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\TIM.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\Tencent\TIM\Bin\tim.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;tim&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save
set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\jdk1.6.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\jdkapi\jdk.chm&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;jdk1.6&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save

set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\JDownloader.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\JDownloader\JDownloader2.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;JDownloader2&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save

set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\Eclipse.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\eclipse\Eclipse.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;eclipse&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save

set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\JetBrains PyCharm 2017.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\JetBrains\PyCharm\bin\pycharm64.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;JetBrains PyCharm 2017&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save

set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\Telegram.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\Telegram\Telegram.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;Telegram&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save

set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\ipython.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\Anaconda3\Scripts\ipython.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;ipython&quot;  
oShellLink.WorkingDirectory=strDesktop
oShellLink.IconLocation=&quot;D:\Program Files (x86)\Anaconda3\Menu\jupyter.ico,0&quot;  '这是图标'
oShellLink.Save

set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\spyder.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\Anaconda3\Scripts\spyder.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;spyder&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.IconLocation=&quot;D:\Program Files (x86)\Anaconda3\Menu\spyder.ico,0&quot;  '这是图标'
oShellLink.Save

set oShellLink=WshShell.CreateShortcut(strDesktop &amp; &quot;\应用软件\Visual Studio Code.lnk&quot;)   '创建新的'
oShellLink.TargetPath=&quot;D:\Program Files (x86)\Microsoft VS Code\Code.exe&quot;  
oShellLink.WindowStyle=1  
oShellLink.Description=&quot;Visual Studio Code&quot;  
oShellLink.WorkingDirectory=strDesktop  
oShellLink.Save


AnacondaPath =&quot;D:\Program Files (x86)\Anaconda3\&quot; '添加python环境变量'
AnacondaScripts=&quot;D:\Program Files (x86)\Anaconda3\Scripts\&quot;
WshShell.Environment(&quot;system&quot;).Item(&quot;Path&quot;) = AnacondaPath + &quot;;&quot; + AnacondaScripts +&quot;;&quot; + WshShell.Environment(&quot;system&quot;).Item(&quot;path&quot;)
########################################################下载nod32更新包########################################################
if exist &quot;D:\nginx\html&quot; del /s /q &quot;D:\nginx\html\*.*&quot;   
if exist offline_update_eav.zip del /s /q offline_update_eav.zip
wget -nc http://ftp.ivanovo.ac.ru/updates/eset/offline_update_eav.zip
7za x offline_update_eav.zip -oD:\nginx\html -aoa
########################################################代理下载tumblr视频########################################################
python gettumblr.py #这个脚本还没写
wget --no-check-certificate -c -e &quot;https_proxy=http://127.0.0.1:1090&quot; -P E:\tumblr\1 -i 1.txt



</rich_text></node><node name="linux" prog_lang="sh" readonly="False" tags="" unique_id="28"><rich_text>
</rich_text></node></node><node name="Python" prog_lang="python" readonly="False" tags="Python" unique_id="1"><rich_text>
</rich_text><node name="包管理" prog_lang="python" readonly="False" tags="" unique_id="19"><rich_text>########################################################pip相关语法########################################################
安装完python 2.7之后,用pip安装常用模块，不常用的可以用virtualenv在虚拟环境中安装,默认安装完python2.7后先升级pip和setuptolls
pip list --outdated #列出过期的第三方包
pip install --upgrade pip setuptools wheel #一次升级多个
pip install virtualenv #安装virtualenv虚拟环境
pip install ipython #会安装相关关联模块
pip install pyreadline #提供自动补全和颜色标识
#http://www.lfd.uci.edu/~gohlke/pythonlibs/ 并不知道是什么源,用豆瓣源或者清华园可以在这个源下载whl包离线安装
#http://repo.continuum.io/pkgs/ Anaconda源,可以在anaconda中应用,原python没有试过能不能用
#http://pypi.zenlogic.net/simple/ #不知名源
pip install PyQt4-4.11.4-cp27-none-win_amd64.whl #本地安装pyqt4,32位自行下载
pip freeze &gt; requirements.txt #导出本地的包和版本到到文件
pip install -r requirements.txt #安装导出的包
pip install scrapy #一个爬虫框架
pip install flask #一个web框架
pip install requests #一个http客户端框架
pip install gunicorn #gunicorn是一个wsgi容器
pip install supervisor #supervisor是来管理python的进程，保证其在后台一直运行不中断
pip install MySQL-python #安装支持mysql的包
pip install PIL #基于Python的图像处理库，功能强大，对图形文件的格式支持广泛
pip install BeautifulSoup #基于Python的HTML/XML解析器，简单易用
pip install Django #开源web开发框架，它鼓励快速开发,并遵循MVC设计
pip install wxPython #GUI框架
pip install pyGtk #GUI框架
pip install Tkinter #GUI框架
pip install Tix #GUI框架
pip install SQLAlchemy #一个ORM框架
pip install scipy #科学数学统计
pip install NumPy #科学数学统计
pip install numarray #科学数学统计
pip install matplotlib #科学数学统计 
pip install pywin32 #操作win32包
pip install reportlab #操作PDF报表包
pip install pyttsx #pyttsx是Python的一个关于文字转语音方面的很不错的库
pip install tushare #一个获得股票的接口(还包括一些电影票房信息)
pip install -i https://pypi.douban.com/simple pandas #换豆瓣源安装
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple virtualenv #官方源不稳定改用清华的源
virtualenv env --no-site-packages   #建立一个env虚拟环境如果不想依赖这些package，那么可以加上参数--no-site-packages　
cd env #进入虚拟环境
scripts\activate #激活虚拟环境,其实就是环境(env)\srcipts\activate.bat这个批处理
(env)---&gt;pip list #查看虚拟环境的安装包
(env)---&gt;pip install -i https://pypi.tuna.tsinghua.edu.cn/simple django  #虚拟环境安装django
########################################################conda相关语法########################################################
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ #添加清华园
conda config --set show_channel_urls yes #显示URL,设置不设置都无关重要
conda update conda #升级conda
conda list  #列出安装的第三方包
conda install MySQL-python #安装mysql第三方包
conda list -n myenv #列出虚拟环境myenv的第三方包
conda list --export &gt; package-list.txt #导出安装的第三方包
conda create -n myenv --file package-list.txt #创建一个myenv虚拟环境 并安装导出的第三方包
conda env list #列出所有的虚拟环境
conda create --name hero --clone root #通过克隆来复制一个环境。这儿将通过克隆root来创建一个称为hero的副本 注意语法书两个- 
activate snowflake` #切换到虚拟环境hero下
conda create -n bunnies python=3 Astroid Babel #创建一个虚拟环境叫bunnies python是3版本 安装Astroid Babel这两个包
conda create -n hero pip #创建一个虚拟环境叫hero版本是系统默认版本兵安在pip包
conda info -envis #列出虚拟环境
conda remove -n flowers --all #删除虚拟环境flowers
conda search beautifulsoup4 #查询beautifulsoup4这个包


</rich_text></node><node name="学习代码集合" prog_lang="python" readonly="False" tags="" unique_id="14"><rich_text>##########################采用pyqt4+python 编写的下载链接转化链接的工具##########################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:qt4.py
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/11

# -*- coding:utf-8 -*-
import sys
import base64
from PyQt4 import QtCore,QtGui

class Window(QtGui.QWidget):
    def __init__(self):
        QtGui.QWidget.__init__(self)
        self.setWindowTitle(u'专用链接转换')
        self.setFixedSize(300,200)
        vbox=QtGui.QVBoxLayout()
        self.inputbox=QtGui.QTextEdit()
        vbox.addWidget(self.inputbox)
        hbox=QtGui.QHBoxLayout()
        tranbtn=QtGui.QPushButton(u'转换')
        aboutbtn=QtGui.QPushButton(u'关于')
        hbox.addWidget(aboutbtn)
        hbox.addWidget(tranbtn)
        self.connect(aboutbtn,QtCore.SIGNAL('clicked()'),self.OnAbout)
        self.connect(tranbtn,QtCore.SIGNAL('clicked()'),self.OnTran)
        vbox.addLayout(hbox)
        self.outputbox=QtGui.QTextEdit()
        vbox.addWidget(self.outputbox)
        self.setLayout(vbox)

    def OnAbout(self):
        QtGui.QMessageBox.about(self,u'关于',u'迅雷、QQ旋风、flashget专用链接转换工具 by ckh')
    def OnTran(self):
        url=self.inputbox.toPlainText()
        if url.isEmpty():
            QtGui.QMessageBox.warning(self,'warning',u'没有输入链接')
            return
        tranurl=url.split('://')
        if tranurl[0].toUpper()=='THUNDER':
            res=base64.decodestring(tranurl[1])
            self.outputbox.setText(unicode(res[2:-2],'cp936'))
        elif tranurl[0].toUpper()=='QQDL':
            res=base64.decodestring(tranurl[1])
            self.outputbox.setText(unicode(res,'cp936'))
        elif tranurl[0].toUpper()=='FLASHGET':
            res=base64.decodestring(tranurl[1])
            self.outputbox.setText(unicode(res[10:-10],'cp936'))
        else:
            QtGui.QMessageBox.warning(self,u'警告',u'输入的地址不是迅雷、QQ旋风或者flashget专用链接')


if __name__=='__main__':
    app=QtGui.QApplication(sys.argv)
    window = Window()
    window.show()
    sys.exit(app.exec_())

##########################采用pyqt4+python2.7 demo##########################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:helloword.py
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/10/14

import sys
from PyQt4 import QtCore, QtGui

class HelloPyQt(QtGui.QWidget):
    def __init__(self, parent = None):
        super(HelloPyQt, self).__init__(parent)  #调用父类的初始化函数
        self.setWindowTitle(&quot;PyQt Test&quot;)  #设置标题
        self.setFixedSize(400,300)    #设置窗口大小
        self.textHello = QtGui.QTextEdit(&quot;This is a test program written in python with PyQt lib!&quot;) #添加一个文本框
        self.textworld = QtGui.QTextEdit(&quot;This is second text!&quot;)
        self.btnPress = QtGui.QPushButton(u&quot;按我&quot;) #添加一个按钮
        self.btnAbout = QtGui.QPushButton(u&quot;关于&quot;) #添加一个按钮
        v_layout = QtGui.QVBoxLayout()  #添加一个垂直布局
        h_layout = QtGui.QHBoxLayout()  #添加一个水平布局
        v_layout.addWidget(self.textHello) #把文本框放进垂直布局
        v_layout.addWidget(self.textworld) #把按钮放进垂直布局
        h_layout.addWidget(self.btnPress) #把按钮放进水平布局
        h_layout.addWidget(self.btnAbout) #把按钮放进水平布局
        v_layout.addLayout(h_layout) #把水平布局放进垂直布局
        self.setLayout(v_layout) #利用布局来水平布局
        self.connect(self.btnPress,QtCore.SIGNAL('clicked()'),self.btnPress_Clicked) #信号连接槽函数，挡在按钮上按时触发后面的函数
        self.connect(self.btnAbout,QtCore.SIGNAL('clicked()'),self.btnAbout_Clicked) #信号连接槽函数，挡在按钮上按时触发后面的函数

    def btnPress_Clicked(self):
        self.textHello.setText(&quot;Hello PyQt!\nThe button has been pressed.&quot;)
        self.textworld.setText(&quot;Hello World!\nThe text is change.&quot;)

    def btnAbout_Clicked(self):
          QtGui.QMessageBox.about(self,u'关于',u'我的名字如雷贯耳!')


if __name__=='__main__':
    app = QtGui.QApplication(sys.argv) #初始化一个app
    mainWindow = HelloPyQt() #初始化类
    mainWindow.show() #显示框口
    sys.exit(app.exec_()) #进入消息循环
    
#########################################采用python写的多线程sendmail#########################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# threading test
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2015/9/23
#-*- encoding: utf-8 -*-
import threading
import time,datetime
import sys, os
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
from email.header import Header
import Queue
from time import time,sleep

'''
list.txt文件格式：
XXX@163.com
XXX@yahoo.com.cn
XXX@qq.com
XXX@126.com
'''
sub=&quot;python api&quot;
content=&quot;python api,请看附件&quot;
#设置服务器，用户名、口令以及邮箱的后缀
mail_host=&quot;smtp.sina.com&quot;
mail_user=&quot;XXX@sina.com&quot;
mail_pass=&quot;XXX&quot;



queue=Queue.Queue()
class send_mail(threading.Thread):
    def __init__(self,threadname,queue):
        threading.Thread.__init__(self)
        self.threadname=threadname
        self.queue = queue
        self.start()
    def run(self):
        while True:
                if self.queue.empty():break
                to_list=self.queue.get()
                file_list=self.att_file('file')
                self.send(to_list,file_list)



    def send(self,lsTo,attfile = []):
        ok = False
        for _send  in range(5):
            try:
                msg = MIMEMultipart()
                msg[&quot;Subject&quot;] = Header(sub,'utf-8').encode()
                msg[&quot;From&quot;]    = mail_user
                msg[&quot;To&quot;]      = lsTo
                msg.attach(MIMEText(content, 'plain', 'utf-8'))
                for plugin in attfile:
                    att = MIMEApplication(open(plugin,'rb').read())
                    att_name = os.path.basename(plugin)
                    att.add_header( 'content-disposition', 'attachment', filename=att_name)
                    msg.attach( att)
                s = smtplib.SMTP()
                s.connect(mail_host)
                s.login(mail_user,mail_pass)
                s.sendmail(mail_user, lsTo, msg.as_string())
                print '发送成功:'+lsTo
                self.queue.task_done()
                ok = True
                break
            except:
                sleep(1)
                continue
        if not ok:
            open('errors.txt').write(lsTo+'\n')
            return ''

# 返回指定目录下所有的文件名

    def att_file(self,file_dir):
        file_name = []
        dir =  os.getcwd()
        path = os.path.join(dir, file_dir)
        for root, dirs, files in os.walk(path):
            for name in files:
                file_name.append(os.path.join(root, name))
        return file_name



if __name__ == '__main__':
    #读取邮件列表
    with open('list.txt') as f:
            all_mail=f.readlines()
    # 产生线程序列
    for i in all_mail:
        print i
        queue.put(i)
    for i in range(10):
        threadname='Thread'+str(i)
        send_mail(threadname,queue)
    queue.join()    


#########################################生产者消费者问题#########################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test.py
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/23

import os
# producer_consumer_queue
from Queue import Queue
import random
import threading
import time

#Producer thread
class Producer(threading.Thread):

    def __init__(self, t_name, queue):
        threading.Thread.__init__(self, name=t_name)
        self.data=queue

    def run(self):

        for i in range(5):
            print &quot;%s: %s is producing %d to the queue!\n&quot; %(time.ctime(), self.getName(), i)
            self.data.put(i)
            time.sleep(random.randrange(10)/5)
        print &quot;%s: %s finished!&quot; %(time.ctime(), self.getName())



#Consumer thread
class Consumer(threading.Thread):
    def __init__(self, t_name, queue):
        threading.Thread.__init__(self, name=t_name)
        self.data=queue

    def run(self):
        for i in range(5):
            val = self.data.get()
            print &quot;%s: %s is consuming. %d in the queue is consumed!\n&quot; %(time.ctime(), self.getName(), val)
            time.sleep(random.randrange(10))
        print &quot;%s: %s finished!&quot; %(time.ctime(), self.getName())

#Main thread
def main():
    queue = Queue()
    producer = Producer('Pro.', queue)
    consumer = Consumer('Con.', queue)
    producer.start()
    consumer.start()
    producer.join()
    consumer.join()
    print 'All threads terminate!'

if __name__ == '__main__':
    main()  
    
    
#########################################豆瓣爬top250#########################################    
#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:douban.py
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/23

import urllib2, re, string
import threading, Queue, time
import sys

reload(sys)
sys.setdefaultencoding('utf8')
_DATA = []
# FILE_LOCK = threading.Lock()
SHARE_Q = Queue.Queue()  #构造一个不限制大小的的队列
_WORKER_THREAD_NUM = 3  #设置线程的个数

class MyThread(threading.Thread) :

    def __init__(self, func) :
        super(MyThread, self).__init__()  #调用父类的构造函数
        self.func = func  #传入线程函数逻辑

    def run(self) :
        self.func()

def worker() :
    global SHARE_Q
    while not SHARE_Q.empty():
        url = SHARE_Q.get() #获得任务
        my_page = get_page(url)
        find_title(my_page)  #获得当前页面的电影名
        #write_into_file(temp_data)
        time.sleep(1)
        SHARE_Q.task_done()

def get_page(url) :
    &quot;&quot;&quot;
    根据所给的url爬取网页HTML
    Args:
        url: 表示当前要爬取页面的url
    Returns:
        返回抓取到整个页面的HTML(unicode编码)
    Raises:
        URLError:url引发的异常
    &quot;&quot;&quot;
    try :
        my_page = urllib2.urlopen(url).read().decode(&quot;utf-8&quot;)
    except urllib2.URLError, e :
        if hasattr(e, &quot;code&quot;):
            print &quot;The server couldn't fulfill the request.&quot;
            print &quot;Error code: %s&quot; % e.code
        elif hasattr(e, &quot;reason&quot;):
            print &quot;We failed to reach a server. Please check your url and read the Reason&quot;
            print &quot;Reason: %s&quot; % e.reason
    return my_page

def find_title(my_page) :
    &quot;&quot;&quot;
    通过返回的整个网页HTML, 正则匹配前100的电影名称

    Args:
        my_page: 传入页面的HTML文本用于正则匹配
    &quot;&quot;&quot;
    temp_data = []
    movie_items = re.findall(r'&lt;span.*?class=&quot;title&quot;&gt;(.*?)&lt;/span&gt;', my_page, re.S)
    for index, item in enumerate(movie_items) :
        if item.find(&quot;&amp;nbsp&quot;) == -1 :
            #print item,
            temp_data.append(item)
    _DATA.append(temp_data)


def main() :
    global SHARE_Q
    threads = []
    douban_url = &quot;http://movie.douban.com/top250?start={page}&amp;filter=&amp;type=&quot;
    #向队列中放入任务, 真正使用时, 应该设置为可持续的放入任务
    for index in xrange(10) :
        SHARE_Q.put(douban_url.format(page = index * 25))
    for i in xrange(_WORKER_THREAD_NUM) :
        thread = MyThread(worker)
        thread.start()  #线程开始处理任务
        threads.append(thread)
    for thread in threads :
        thread.join()
    SHARE_Q.join()
    with open(&quot;movie.txt&quot;, &quot;w+&quot;) as my_file :
        for page in _DATA :
            for movie_name in page:
                my_file.write(movie_name + &quot;\n&quot;)
    print &quot;Spider Successful!!!&quot;

if __name__ == '__main__':
    main()



    
#########################################生产者消费者#########################################    

#!/usr/bin/env python
# coding:utf8
import random, threading, time
from Queue import Queue


# Producer thread
class Producer(threading.Thread):
    def __init__(self, t_name, queue):
        threading.Thread.__init__(self, name=t_name)
        self.data = queue

    def run(self):
        for i in range(10):  # 随机产生10个数字 ，可以修改为任意大小
            randomnum = random.randint(1, 99)
            print &quot;%s: %s is producing %d to the queue!&quot; % (time.ctime(), self.getName(), randomnum)
            self.data.put(randomnum)  # 将数据依次存入队列
            time.sleep(1)
        print &quot;%s: %s finished!&quot; % (time.ctime(), self.getName())


# Consumer thread
class Consumer_even(threading.Thread):
    def __init__(self, t_name, queue):
        threading.Thread.__init__(self, name=t_name)
        self.data = queue

    def run(self):
        while 1:
            try:
                val_even = self.data.get(1, 5)  # get(self, block=True, timeout=None) ,1就是阻塞等待,5是超时5秒
                if val_even % 2 == 0:
                    print &quot;%s: %s is consuming. %d in the queue is consumed!&quot; % (time.ctime(), self.getName(), val_even)
                    time.sleep(2)
                else:
                    self.data.put(val_even)
                    time.sleep(2)
            except:  # 等待输入，超过5秒 就报异常
                print &quot;%s: %s finished!&quot; % (time.ctime(), self.getName())
                break


class Consumer_odd(threading.Thread):
    def __init__(self, t_name, queue):
        threading.Thread.__init__(self, name=t_name)
        self.data = queue

    def run(self):
        while 1:
            try:
                val_odd = self.data.get(1, 5)
                if val_odd % 2 != 0:
                    print &quot;%s: %s is consuming. %d in the queue is consumed!&quot; % (time.ctime(), self.getName(), val_odd)
                    time.sleep(2)
                else:
                    self.data.put(val_odd)
                    time.sleep(2)
            except:
                print &quot;%s: %s finished!&quot; % (time.ctime(), self.getName())
                break


# Main thread
def main():
    queue = Queue()
    producer = Producer('Pro.', queue)
    consumer_even = Consumer_even('Con_even.', queue)
    consumer_odd = Consumer_odd('Con_odd.', queue)
    producer.start()
    consumer_even.start()
    consumer_odd.start()
    producer.join()
    consumer_even.join()
    consumer_odd.join()
    print 'All threads terminate!'


if __name__ == '__main__':
    main()



#############################################################操作pdf#############################################################
# -*- coding: utf-8 -*-
#字体库
import reportlab.lib.fonts
#canvas画图的类库
from reportlab.pdfgen.canvas import Canvas
#用于定位的inch库，inch将作为我们的高度宽度的单位
from reportlab.lib.units import inch
def pdf_head(canvas, headtext):
    #setFont是字体设置的函数，第一个参数是类型，第二个是大小
    canvas.setFont(&quot;Helvetica-Bold&quot;, 11.5)
    #向一张pdf页面上写string
    canvas.drawString(1*inch, 10.5*inch, headtext)
    #画一个矩形，并填充为黑色
    canvas.rect(1*inch, 10.3*inch, 6.5*inch, 0.12*inch,fill=1)
    #画一条直线
    canvas.line(1*inch, 10*inch, 7.5*inch, 10*inch)

if __name__ == &quot;__main__&quot;:
    #声明Canvas类对象，传入的就是要生成的pdf文件名字
    can = Canvas('report.pdf')
    pdf_head(can, &quot;test for REPORTLAB!&quot;)
    #showpage将保留之前的操作内容之后新建一张空白页
    can.showPage()
    #将所有的页内容存到打开的pdf文件里面。
    can.save()







</rich_text></node><node name="pycharm" prog_lang="python" readonly="False" tags="" unique_id="21"><rich_text>pycharm模板存储在
C:\Users\Administrator\.PyCharm40\config\fileTemplates\internal\Python Script.py #这是win7的存储位置
###############################################################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:${NAME}.py
# IDE:Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit ${DATE}

import os

class User(object):

    def __init__(self,name,age):
        self.name = name
        self.age = age

    def __str__(self):
        return 'My name is {0},My age is {1}'.format(self.name,self.age)


if __name__ == '__main__':
    user = User('hero',28)
    print user

###############################################################################
可以在系统设置好之后备份相关配置文件Python Script.py,在系统发生还原之后编辑相关批处理复制覆盖此脚本文件,即可达到设计的python模板不用重新编辑
下面是一个批处理，把脚本文件文件自动复制到程序默认目录中
###############################################################################
@echo off
if exist C:\Users\Administrator\.PyCharm40\config\fileTemplates rd /s /q &quot;C:\Users\Administrator\.PyCharm40\config\fileTemplates&quot;
md C:\Users\Administrator\.PyCharm40\config\fileTemplates\internal
copy &quot;Python Script.py&quot; &quot;C:\Users\Administrator\.PyCharm40\config\fileTemplates\internal\&quot;
echo 完成..
pause &gt;nul




</rich_text></node><node name="学习笔记" prog_lang="python" readonly="False" tags="" unique_id="11"><rich_text>hashlib,md5模块：
    hashlib.md5('md5_str').hexdigest()      对指定字符串md5加密
    md5.md5('md5_str').hexdigest()          对指定字符串md5加密


types模块：
    保存了所有数据类型名称。
    if type('1111') == types.StringType:
MySQLdb模块：
    MySQLdb.get_client_info()           获取API版本
    MySQLdb.Binary('string')            转为二进制数据形式
    MySQLdb.escape_string('str')        针对mysql的字符转义函数
    MySQLdb.DateFromTicks(1395842548)   把时间戳转为datetime.date对象实例
    MySQLdb.TimestampFromTicks(1395842548)   把时间戳转为datetime.datetime对象实例
    MySQLdb.string_literal('str')       字符转义
    MySQLdb.cursor()游标对象上的方法：《python核心编程》P624

atexit模块：
    atexit.register(fun,args,args2..)   注册函数func，在解析器退出前调用该函数






urllib2模块：
urlparse模块：


re模块：
    正则表达式模块函数：《python核心编程》P472
math,cmath模块：
    数学运算，复数运算函数
operator模块:
    一些数值操作函数集合。参考CSDN收藏
copy模块：
    copy.copy(a)        复制对象
    copy.deepcopy(a)    复制集合

fileinput 模块：
    处理文件内容模块
shutil 模块：
    包含一些复制文件和文件夹的函数：









</rich_text><node name="OS" prog_lang="python" readonly="False" tags="" unique_id="32"><rich_text>os模块:
    os.remove()         #删除文件
    os.unlink()         #删除文件
    os.rename()         #重命名文件
    os.listdir()        #列出指定目录下所有文件
    os.chdir()          #改变当前工作目录
    os.getcwd()         #获取当前文件路径
    os.mkdir()          #新建目录
    os.rmdir()          #删除空目录(删除非空目录, 使用shutil.rmtree())
    os.makedirs()       #创建多级目录
    os.removedirs()     #删除多级目录
    os.stat(file)       #获取文件属性
    os.chmod(file)      #修改文件权限
    os.utime(file)      #修改文件时间戳
    os.name(file)       #获取操作系统标识
    os.system()         #执行操作系统命令
    os.execvp()         #启动一个新进程
    os.fork()           #获取父进程ID，在子进程返回中返回0
    os.execvp()         #执行外部程序脚本（Uinx）
    os.spawn()          #执行外部程序脚本（Windows）
    os.access(path, mode) #判断文件权限(详细参考cnblogs)
    os.wait()           #暂时未知
    os.path.split(filename)         #将文件路径和文件名分割(会将最后一个目录作为文件名而分离)
    os.path.splitext(filename)      #将文件路径和文件扩展名分割成一个元组
    os.path.dirname(filename)       #返回文件路径的目录部分
    os.path.basename(filename)      #返回文件路径的文件名部分
    os.path.join(dirname,basename)  #将文件路径和文件名凑成完整文件路径
    os.path.abspath(name)           #获得绝对路径
    os.path.splitunc(path)          #把路径分割为挂载点和文件名
    os.path.normpath(path)          #规范path字符串形式
    os.path.exists()                #判断文件或目录是否存在
    os.path.isabs()                 #如果path是绝对路径，返回True
    os.path.realpath(path)          #返回path的真实路径
    os.path.relpath(path[, start])  #从start开始计算相对路径
    os.path.normcase(path)          #转换path的大小写和斜杠
    os.path.isdir()                 #判断name是不是一个目录，name不是目录就返回false
    os.path.isfile()                #判断name是不是一个文件，不存在返回false
    os.path.islink()                #判断文件是否连接文件,返回boolean
    os.path.ismount()               #指定路径是否存在且为一个挂载点，返回boolean
    os.path.samefile()              #是否相同路径的文件，返回boolean
    os.path.getatime()              #返回最近访问时间 浮点型
    os.path.getmtime()              #返回上一次修改时间 浮点型
    os.path.getctime()              #返回文件创建时间 浮点型
    os.path.getsize()               #返回文件大小 字节单位
    os.path.commonprefix(list)      #返回list(多个路径)中，所有path共有的最长的路径
    os.path.lexists                 #路径存在则返回True,路径损坏也返回True
    os.path.expanduser(path)        #把path中包含的&quot;~&quot;和&quot;~user&quot;转换成用户目录
    os.path.expandvars(path)        #根据环境变量的值替换path中包含的”$name”和”${name}”
    os.path.sameopenfile(fp1, fp2)  #判断fp1和fp2是否指向同一文件
    os.path.samestat(stat1, stat2)  #判断stat tuple stat1和stat2是否指向同一个文件
    os.path.splitdrive(path)        #一般用在windows下，返回驱动器名和路径组成的元组
    os.path.walk(path, visit, arg)  #遍历path，给每个path执行一个函数详细见手册
    os.path.supports_unicode_filenames()     #设置是否支持unicode路径名</rich_text></node><node name="SYS" prog_lang="python" readonly="False" tags="" unique_id="33"><rich_text>sys模块：
    sys.argv                命令行参数List，第一个元素是程序本身路径
    sys.path                返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值
    sys.modules.keys()      返回所有已经导入的模块列表
    sys.modules             返回系统导入的模块字段，key是模块名，value是模块
    sys.exc_info()          获取当前正在处理的异常类,exc_type、exc_value、exc_traceback当前处理的异常详细信息
    sys.exit(n)             退出程序，正常退出时exit(0)
    sys.hexversion          获取Python解释程序的版本值，16进制格式如：0x020403F0
    sys.version             获取Python解释程序的版本信息
    sys.platform            返回操作系统平台名称
    sys.stdout              标准输出
    sys.stdout.write('aaa') 标准输出内容
    sys.stdout.writelines() 无换行输出
    sys.stdin               标准输入
    sys.stdin.read()        输入一行
    sys.stderr              错误输出
    sys.exc_clear()         用来清除当前线程所出现的当前的或最近的错误信息
    sys.exec_prefix         返回平台独立的python文件安装的位置
    sys.byteorder           本地字节规则的指示器，big-endian平台的值是'big',little-endian平台的值是'little'
    sys.copyright           记录python版权相关的东西
    sys.api_version         解释器的C的API版本
    sys.version_info        'final'表示最终,也有'candidate'表示候选，表示版本级别，是否有后继的发行
    sys.getdefaultencoding()        返回当前你所用的默认的字符编码格式
    sys.getfilesystemencoding()     返回将Unicode文件名转换成系统文件名的编码的名字
    sys.builtin_module_names        Python解释器导入的内建模块列表
    sys.executable                  Python解释程序路径
    sys.getwindowsversion()         获取Windows的版本
    sys.stdin.readline()            从标准输入读一行，sys.stdout.write(&quot;a&quot;) 屏幕输出a
    sys.setdefaultencoding(name)    用来设置当前默认的字符编码(详细使用参考文档)
    sys.displayhook(value)          如果value非空，这个函数会把他输出到sys.stdout(详细使用参考文档)</rich_text></node><node name="datetime" prog_lang="python" readonly="False" tags="" unique_id="34"><rich_text>datetime,date,time模块：
    datetime.date.today()           本地日期对象,(用str函数可得到它的字面表示(2014-03-24))
    datetime.date.isoformat(obj)    当前[年-月-日]字符串表示(2014-03-24)
    datetime.date.fromtimestamp()   返回一个日期对象，参数是时间戳,返回 [年-月-日]
    datetime.date.weekday(obj)      返回一个日期对象的星期数,周一是0
    datetime.date.isoweekday(obj)   返回一个日期对象的星期数,周一是1
    datetime.date.isocalendar(obj)  把日期对象返回一个带有年月日的元组
    datetime对象：
    datetime.datetime.today()       返回一个包含本地时间(含微秒数)的datetime对象 2014-03-24 23:31:50.419000
    datetime.datetime.now([tz])     返回指定时区的datetime对象 2014-03-24 23:31:50.419000
    datetime.datetime.utcnow()      返回一个零时区的datetime对象
    datetime.fromtimestamp(timestamp[,tz])      按时间戳返回一个datetime对象，可指定时区,可用于strftime转换为日期表示
    datetime.utcfromtimestamp(timestamp)        按时间戳返回一个UTC-datetime对象
    datetime.datetime.strptime('2014-03-16 12:21:21',&quot;%Y-%m-%d %H:%M:%S&quot;) 将字符串转为datetime对象
    datetime.datetime.strftime(datetime.datetime.now(), '%Y%m%d %H%M%S') 将datetime对象转换为str表示形式
    datetime.date.today().timetuple()           转换为时间戳datetime元组对象，可用于转换时间戳
    datetime.datetime.now().timetuple()
    time.mktime(timetupleobj)                   将datetime元组对象转为时间戳
    time.time()                     当前时间戳
    time.localtime
    time.gmtime</rich_text></node><node name="string" prog_lang="python" readonly="False" tags="" unique_id="35"><rich_text>string模块：
    str.capitalize()            把字符串的第一个字符大写
    str.center(width)           返回一个原字符串居中，并使用空格填充到width长度的新字符串
    str.ljust(width)            返回一个原字符串左对齐，用空格填充到指定长度的新字符串
    str.rjust(width)            返回一个原字符串右对齐，用空格填充到指定长度的新字符串
    str.zfill(width)            返回字符串右对齐，前面用0填充到指定长度的新字符串
    str.count(str,[beg,len])    返回子字符串在原字符串出现次数，beg,len是范围
    str.decode(encodeing[,replace]) 解码string,出错引发ValueError异常
    str.encode(encodeing[,replace]) 解码string
    str.endswith(substr[,beg,end])  字符串是否以substr结束，beg,end是范围
    str.startswith(substr[,beg,end])  字符串是否以substr开头，beg,end是范围
    str.expandtabs(tabsize = 8)     把字符串的tab转为空格，默认为8个
    str.find(str,[stat,end])        查找子字符串在字符串第一次出现的位置，否则返回-1
    str.index(str,[beg,end])        查找子字符串在指定字符中的位置，不存在报异常
    str.isalnum()               检查字符串是否以字母和数字组成，是返回true否则False
    str.isalpha()               检查字符串是否以纯字母组成，是返回true,否则false
    str.isdecimal()             检查字符串是否以纯十进制数字组成，返回布尔值
    str.isdigit()               检查字符串是否以纯数字组成，返回布尔值
    str.islower()               检查字符串是否全是小写，返回布尔值
    str.isupper()               检查字符串是否全是大写，返回布尔值
    str.isnumeric()             检查字符串是否只包含数字字符，返回布尔值
    str.isspace()               如果str中只包含空格，则返回true,否则FALSE
    str.title()                 返回标题化的字符串（所有单词首字母大写，其余小写）
    str.istitle()               如果字符串是标题化的(参见title())则返回true,否则false
    str.join(seq)               以str作为连接符，将一个序列中的元素连接成字符串
    str.split(str='',num)       以str作为分隔符，将一个字符串分隔成一个序列，num是被分隔的字符串
    str.splitlines(num)         以行分隔，返回各行内容作为元素的列表
    str.lower()                 将大写转为小写
    str.upper()                 转换字符串的小写为大写
    str.swapcase()              翻换字符串的大小写
    str.lstrip()                去掉字符左边的空格和回车换行符
    str.rstrip()                去掉字符右边的空格和回车换行符
    str.strip()                 去掉字符两边的空格和回车换行符
    str.partition(substr)       从substr出现的第一个位置起，将str分割成一个3元组。
    str.replace(str1,str2,num)  查找str1替换成str2，num是替换次数
    str.rfind(str[,beg,end])    从右边开始查询子字符串
    str.rindex(str,[beg,end])   从右边开始查找子字符串位置
    str.rpartition(str)         类似partition函数，不过从右边开始查找
    str.translate(str,del='')   按str给出的表转换string的字符，del是要过虑的字符</rich_text></node><node name="urllib" prog_lang="python" readonly="False" tags="" unique_id="36"><rich_text>urllib模块：
    urllib.quote(string[,safe])             对字符串进行编码。参数safe指定了不需要编码的字符
    urllib.unquote(string)                  对字符串进行解码
    urllib.quote_plus(string[,safe])        与urllib.quote类似，但这个方法用'+'来替换' '，而quote用'%20'来代替' '
    urllib.unquote_plus(string )            对字符串进行解码
    urllib.urlencode(query[,doseq])         将dict或者包含两个元素的元组列表转换成url参数。
                                            例如 字典{'name':'wklken','pwd':'123'}将被转换为&quot;name=wklken&amp;pwd=123&quot;
    urllib.pathname2url(path)               将本地路径转换成url路径
    urllib.url2pathname(path)               将url路径转换成本地路径
    urllib.urlretrieve(url[,filename[,reporthook[,data]]])  下载远程数据到本地
        filename：指定保存到本地的路径（若未指定该，urllib生成一个临时文件保存数据）
        reporthook：回调函数，当连接上服务器、以及相应的数据块传输完毕的时候会触发该回调
        data：指post到服务器的数据
    rulrs = urllib.urlopen(url[,data[,proxies]])     抓取网页信息，[data]post数据到Url,proxies设置的代理
    urlrs.readline()    跟文件对象使用一样
    urlrs.readlines()   跟文件对象使用一样
    urlrs.fileno()      跟文件对象使用一样
    urlrs.close()       跟文件对象使用一样
    urlrs.info()        返回一个httplib.HTTPMessage对象，表示远程服务器返回的头信息
    urlrs.getcode()     获取请求返回状态HTTP状态码
    urlrs.geturl()      返回请求的URL</rich_text></node><node name="random" prog_lang="python" readonly="False" tags="" unique_id="37"><rich_text>random模块：
    random.random()             产生0-1的随机浮点数
    random.uniform(a, b)        产生指定范围内的随机浮点数
    random.randint(a, b)        产生指定范围内的随机整数
    random.randrange([start], stop[, step]) 从一个指定步长的集合中产生随机数
    random.choice(sequence)     从序列中产生一个随机数
    random.shuffle(x[, random]) 将一个列表中的元素打乱
    random.sample(sequence, k)  从序列中随机获取指定长度的片断</rich_text></node><node name="list tuple dict set" prog_lang="python" readonly="False" tags="" unique_id="38"><rich_text>#使用list tuple dict set
list
#Python内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。
#classmates = ['Michael', 'Bob', 'Tracy'] #比如，列出班里所有同学的名字,就可以用一个list表示：
classmates #列表为['Michael', 'Bob', 'Tracy']
len(classmates) #得到列表长度为3
classmates[0] #得到第1个索引的元素,索引从0开始
classmates[1] #得到第2个索引的元素,索引从0开始
classmates[3] #IndexError 索引越界
classmates[-1] #得到倒数第一的元素
classmates[-2] #得到倒数第二的元素
classmates.append('hero') #list是一个可变的有序表，往list中追加元素hero到末尾
classmates.insert(1, 'Jack') #把元素插入到指定的位置，比如索引号为1的位置
classmates #列表为['Michael', 'jack','Bob', 'Tracy','hero']
classmates.pop() #删除列表末尾的元素
classmates.pop(1) #删除索引1的元素
classmates.remove('Bob') #删除Bob匹配值的元素不存在触发ValueError错误
classmates[1] = 'Sarah' #改变索引1的元素
L = ['Apple', 123, True] #列表可以是不同的数据类型
s = ['python', 'java', ['asp', 'php'], 'scheme'] #列表可以包括另一个列表
classmates = ['Michael', 'Bob', 'Tracy']
s = [123,L,classmates] #列表包含另外2个列表
s[2][0] #要拿到'asp',因此s可以看成是一个二维数组
L = [] #一个空列表，通常用来初始化一个列表
t =[1,2,3,4,5]
for n in t:  #遍历列表
    print n
for item in range(len(t)): #遍历列表
    print t[item]  
t[1:4] #从索引1开始到索引3不包括索引4,可以简单理解列表元素内容为t[n,m]为t[n,m-1]
t[2:] #不知道结尾默认到最后一个元素
t[-3:] #从倒数第三个元素到最后一个元素,后面的元素要小不能t[-1:-3]这样得到是[]
t1 = [1,2,3,4,5]
t2 = [6,7,8]
t1.extend(t2) #列表连接
t2 = t1 + t2 #列表连接
t2 += t1 #列表连接】
t3 = [0,0]
t3 = t3 *2 #元素复制一倍.[0,0,0,0]
t1.index(5) #得到索引4
bool = 4 in t1 #bool是True,元素4在列表中
t1 = [9,8,7,5,0,2,1]
t1.sort() #排序默认是升序，reverse=True
t1.sort(reverse=True) #降序排列，False没必要写 默认就是False
t1.reverse() #列表反转
tuple
#另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改
classmates = ('Michael', 'Bob', 'Tracy') #用圆括号来表示是元组
len(classmates)#得到元组长度为3
t = () #定义一个空元组
t = (1,) #定义只有一个元素的元组，注意：一个元素也要加一个逗号
t = ('a', 'b', ['A', 'B']) #一个元组包含一个列表
t[2][1] ='x' #元组不可变,但是我们改变的是元组里列表
t[2][0] ='y'
t #('a', 'b', ['y', 'x']) #我们没有改变元组而是改变了元组里一个是列表元素
t =(1,3,5,7,9)
for n in t: #遍历元组
    print n
for item in range(len(t)):
    print t[item]  #遍历元组
t1 = (1,2,3)
a,b,c = t1
print a,b,c  #元组的解包
t =(1,3,5,7,9)
dict
#Python内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。
#举个例子，假设要根据同学的名字查找对应的成绩，如果用list实现，需要两个list：
#使用dict和set
names = ['Michael', 'Bob', 'Tracy']
scores = [95, 75, 85]
d = {'Michael': 95, 'Bob': 75, 'Tracy': 85} #字典用{}
d['Michael']
d['Adam'] = 67 #修改键值
'Thomas' in d #判断键是否在字典里
d.get('Thomas') #判断键在字典里，没有返回None
d.get('Thomas', -1) #没有返回-1
d.pop('Bob') #删除一个键和键值
set
#set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key，set和dict的唯一区别仅在于没有存储对应的value
#要创建一个set，需要提供一个list作为输入集合：
s = set([1, 2, 3]) #注意，传入的参数[1, 2, 3]是一个list，而显示的set([1, 2, 3])只是告诉你这个set内部有1，2，3这3个元素，显示的[]不表示这是一个list
s = set([1, 1, 2, 2, 3, 3]) #重复元素在set中自动被过滤
s.add(4) #通过add(key)方法可以添加元素到set中，可以重复添加，但不会有效果
s.remove(4) #方法可以删除元素
s1 = set([1, 2, 3])
s2 = set([2, 3, 4])
s1 &amp; s2 #set([2, 3])
s1 | s2 #set([1, 2, 3, 4])
</rich_text></node><node name="django" prog_lang="python" readonly="False" tags="" unique_id="6"><rich_text>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ #添加清华源
conda config --set show_channel_urls yes #显示URL
conda env list #列举虚拟环境
conda remove -n hero --all #删除虚拟环境hero并删除所有第三方包
conda create -n myenv pip django MySQL-python#创建虚拟环境myenv并按照pip和django开发包
activate myenv #激活虚拟环境
myenv--&gt;&gt;conda install scrapy #在虚拟环境中安装第三方包scrapy
myenv--&gt;&gt;deactivate myenv #离开虚拟环境myenv
</rich_text></node><node name="scrapy" prog_lang="python" readonly="False" tags="" unique_id="13"><rich_text>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scrapy #用清华源来安装scrapy
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ #添加清华源
conda install scrapy #用anaconda版本python安装scrapy
scrapy startproject tutorial #创建项目tutorial
scrapy genspider -t basic domz dmoz.org #创建一个爬虫得到spiders/domz.py。

</rich_text></node></node><node name="demo" prog_lang="python" readonly="False" tags="" unique_id="22"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test.py
# With Pycharm 2016
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/24

import os
import requests
import re

class Bttiantang(object):

    def __init__(self,url):
        self.url = url
        self.tool = Tool()

    def getmovie(self):
        try:
            r = requests.get(self.url)
            r.encoding = 'utf-8'
            return self.tool.GetMovieList(r.text,True)
        except Exception as what:
            print what
            return


class Tool(object):
    def GetMovieList(self,res,debug=False):
        movie=[]
        HTMLstract = re.compile(r'#download(.*?)text-align:center;',re.DOTALL).findall(res)
        if HTMLstract:
            abstract = re.compile(r'&lt;i&gt;|&lt;/i&gt;|&lt;script.*?&lt;/script&gt;|&lt;em.*?&lt;/em&gt;', re.DOTALL).sub('', HTMLstract[0])
        else:
            return

        movie_title = re.compile(r'&lt;h2[^&gt;]*&gt;(.*?)&lt;/h2&gt;',re.DOTALL).findall(abstract)[0]
        movie.append(movie_title)
        #匹配是一个http链接并且以.jpg结尾
        movie_pic = re.compile(r'[a-zA-z]+://[^\s]*\.jpg',re.DOTALL).findall(abstract)[0]
        movie.append(movie_pic)
        info = re.compile(r'&lt;li&gt;(.*?)&lt;/li&gt;', re.DOTALL).findall(abstract)
        # tag = re.compile(u'[\u4e00-\u9fa5]+:', re.DOTALL).findall(info)
        if info:
            for movie_info in info:
                movie_info_list = re.compile(r'&lt;a.*?&gt;(.*?)&lt;/a&gt;', re.DOTALL).findall(movie_info)
                movie_info = ' '.join(movie_info_list)
                if movie_info: #把字符串不等于空加入列表
                    movie.append(movie_info)
        else:
            print &quot;出错!&quot;
        if debug:
            for x in movie:
                print x




if __name__ == '__main__':
    bttiantang = Bttiantang('http://www.bttiantang.com/subject/28050.html')
    text = bttiantang.getmovie()



#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test.py
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/26

from bs4 import BeautifulSoup
import requests

class Bt(object):

    def __init__(self,url):
        self.url = url
        self.base = 'http://www.bttiantang.com'

    def fetch_page(self,debug=False):
        url =[]
        html = requests.get(self.url).text #获得html代码
        soup = BeautifulSoup(html,'lxml')
        links =soup.select('div.litpic a')
        for link in links:
            url.append(self.base + link.get('href'))
        if debug:
            for x in url:
                print x
        return url

    def fetch_item(self):
        urls = self.fetch_page()
        for url in urls:
            html = requests.get(url)#获得html代码
            html.encoding = 'utf-8'
            soup = BeautifulSoup(html.text,'lxml')
            title = list(soup.select('div.moviedteail_tt')[0].stripped_strings)
            name1= soup.find_all(&quot;li&quot;, text=u'又名:')
            print name1

            # info =soup.select('.moviedteail_list li')
            #
            # # for x in info:
            # #     for y in x.stripped_strings:
            # #         print y
            # print '###################################################################################################'
            # print url
            # for x in info:
            #     print list(x.stripped_strings)


            # print info


            # print url,title,info
            # data ={
            #     'title':soup.select('div.moviedteail_tt')[0].text()
            # }



if __name__ == '__main__':
    bt = Bt('http://www.bttiantang.com/movie.php?/order/update/200/')
    bt.fetch_item()
    
    
    
    
    
    
    
    
    
    
#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test.py
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/27

from bs4 import BeautifulSoup
import re
import requests


if __name__ == '__main__':
    # a={}
    # torrent=[]
    # soup = BeautifulSoup(html,'lxml')
    # name = list(soup.find('h2').stripped_strings)
    # pic = soup.find('img')['src']
    # info = soup.find('ul',class_='moviedteail_list').find_all('li')
    # zhongzilinks =soup.select('div.tinfo a')   #&lt;div class=&quot;tinfo&quot;
    #
    #
    #
    #
    #
    # a['name'] =name
    # a['pic'] =pic
    # for x in info:
    #     key = list(x.stripped_strings)[0]
    #     values = list(x.stripped_strings)[1:]
    #     a[key]= values
    # print a
    html =requests.get('http://www.bttiantang.com/subject/28000.html')
    html.encoding='utf-8'
    soup = BeautifulSoup(html.text,'lxml')
    name = list(soup.select('h2')[0].stripped_strings)
    pic = soup.select('img')[0].get('src')
    rates = list(soup.select('p.rt')[0].stripped_strings) #&lt;div class=&quot;tinfo&quot;
    rate =''.join(rates[:-1])  #不要分这个字
    # category = soup.find(text=re.compile(u'又名:'))
    # category = soup.find_all('a',href=re.compile(r&quot;/category.php.*?&quot;))
    category = soup.select('.moviedteail_list li')
    for x in category:
        if x.find(text=(u'又名:')):
            youming= list(x.find('a').stripped_strings)
        if x.find(text=(u'标签:')):
            biaoqian= list(x.find('a').stripped_strings)
        if x.find(text=(u'地区:')):
            area= list(x.find('a').stripped_strings)
        if x.find(text=(u'年份:')):
            year = x.find('a').get_text()
        if x.find(text=(u'导演:')):
            daoyan= list(x.find('a').stripped_strings)
        if x.find(text=(u'编剧:')):
                if x.find('a'):
                    bianju= list(x.find('a').stripped_strings)
                else:
                    bianju = None
        if x.find(text=(u'主演:')):
            zhuyan= list(x.find('a').stripped_strings)
        if x.find(text=(u'imdb:')):
            douban= x.find('a').get_text()
        if x.find(text=(u'详情:')):
            xiangqing = x.find('a').get_text()










    data={
        'name':name,
        'pic':pic,
        'rate':rate,
        'youming':youming,
        'biaoqian':biaoqian,
        'area':area,
        'year':year,
         'daoyan':daoyan,
        'bianju':bianju,
        'zhuyan': zhuyan,
        'douban':douban,
        'xiangqing':xiangqing
    }
    print data





#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test.py
# With Pycharm 2016
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/28

#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test.py
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/27

from bs4 import BeautifulSoup
import requests


if __name__ == '__main__':
    url='http://www.bttiantang.com/subject/24725.html'
    html =requests.get(url)
    html.encoding='utf-8'
    soup = BeautifulSoup(html.text,'lxml')
    name = list(soup.select('h2')[0].stripped_strings)
    pic = soup.select('img')[0].get('src')
    rates = list(soup.select('p.rt')[0].stripped_strings) #&lt;div class=&quot;tinfo&quot;
    rate =''.join(rates[:-1])  #评分但是不要分这个字
    douban = soup.find('a',rel='nofollow').get('href')
    def fun_movie_info(text):
        info=[]
        category = soup.find(text=text)#查找文本
        if category: #没有的话直接None
            if category.next_siblings: #下面的所有节点不为空
                for x in category.next_siblings:
                    info.append(x.get_text())
            else:
                return None
        else:
            return None
        return info

    data={
        'url':url,
        'name':name,
        'pic':pic,
        'rate':rate,
        'youming':fun_movie_info(u'又名:'),
        'tag':fun_movie_info(u'标签:'),
        'area':fun_movie_info(u'地区:'),
        'year':fun_movie_info(u'年份:'),
         'director':fun_movie_info(u'导演:'),
        'writer':fun_movie_info(u'编剧:'),
        'cast': fun_movie_info(u'主演:'),
        'imdb':fun_movie_info(u'imdb:'),
        'douban':douban
    }
    print data
    
    
    
    
#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test.py
# With Pycharm 2016
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/28

#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test.py
# With Pycharm 4.5.4
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/27

from bs4 import BeautifulSoup
import requests


if __name__ == '__main__':
    url='http://www.bttiantang.com/subject/27970.html'
    html =requests.get(url)
    html.encoding='utf-8'
    soup = BeautifulSoup(html.text,'lxml')
    name = list(soup.select('h2')[0].stripped_strings)
    pic = soup.select('img')[0].get('src')
    rates = list(soup.select('p.rt')[0].stripped_strings) #&lt;div class=&quot;tinfo&quot;
    rate =''.join(rates[:-1])  #评分但是不要分这个字
    douban = soup.find('a',rel='nofollow').get('href')
    
    def get_torrent():
        torrent_hash = []
        torrent_info = soup.select('div.tinfo')
        if torrent_info:
        # print torrent_info
            for x in torrent_info:
                a = x.find('span',class_='video').get_text('|')
                b = '|' + x.a['href'].split('uhash=')[1]
                torrent_hash.append( a + b)
        else:
            return None
        return torrent_hash


    def fun_movie_info(text):
        info=[]
        category = soup.find(text=text)#查找文本
        if category: #没有的话直接None
            if category.next_siblings: #下面的所有节点不为空
                for x in category.next_siblings:
                    info.append(x.get_text())
            else:
                return None
        else:
            return None
        return info


    data={
        'url':url,
        'name':name,
        'pic':pic,
        'rate':rate,
        'youming':fun_movie_info(u'又名:'),
        'tag':fun_movie_info(u'标签:'),
        'area':fun_movie_info(u'地区:'),
        'year':fun_movie_info(u'年份:'),
         'director':fun_movie_info(u'导演:'),
        'writer':fun_movie_info(u'编剧:'),
        'cast': fun_movie_info(u'主演:'),
        'imdb':fun_movie_info(u'imdb:'),
        'douban':douban,
        'torrents':get_torrent()
    }
    print data




#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:test1.py
# With Pycharm 2016
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/29

from bs4 import BeautifulSoup
import requests
import threading
import time

class bttiantang(threading.Thread):

    def __init__(self,url_page):
        super(bttiantang, self).__init__()
        self.url_page = url_page

    def run(self):
        print self.url_page
        urls = self.get_movie_url()
        for url in urls:
            print threading.current_thread().getName(),time.time()
            self.get_movie_item(url,debug=True)


    def get_soup(self,url):
        try:
            html = requests.get(url)
            html.encoding = 'utf-8'
            soup = BeautifulSoup(html.text, 'lxml')
        except Exception,e:
            print e
        return soup



    def get_movie_url(self):
        movies = []
        try:
            soup = self.get_soup(self.url_page)
            movie_urls = soup.select('div.litpic a')
            for x in movie_urls:
                movies.append('http://www.bttiantang.com'+ x['href'])
        except Exception,e:
            print e
        return movies



    def fun_movie_info(self,url,text):
        info = []
        soup = self.get_soup(url)
        category = soup.find(text=text)  # 查找文本
        if category:  # 没有的话直接None
            if category.next_siblings:  # 下面的所有节点不为空
                for x in category.next_siblings:
                    info.append(x.get_text())
            else:
                return None
        else:
            return None
        return info

    def get_torrent(self,url):
        torrent_hash = []  #第一个信息是种子名字，第二个是种子大小，第三个是种子hash 中间用'|' 分割
        soup = self.get_soup(url)
        torrent_info = soup.select('div.tinfo')
        if torrent_info:
            for x in torrent_info:
                a = x.find('span',class_='video').get_text('|')
                b = '|' + x.a['href'].split('uhash=')[1]
                torrent_hash.append( a + b)
        else:
            return None
        return torrent_hash

    def get_movie_item(self,url,debug=False):
        soup = self.get_soup(url)
        name = list(soup.select('h2')[0].stripped_strings)
        pic = soup.select('img')[0].get('src')
        rates = list(soup.select('p.rt')[0].stripped_strings)
        rate = ''.join(rates[:-1])  # 评分但是不要分这个字
        # douban = soup.find('a', rel='nofollow').get('href')
        youming = self.fun_movie_info(url, u'又名:')
        tag = self.fun_movie_info(url, u'标签:')
        area = self.fun_movie_info(url, u'地区:'),
        year = self.fun_movie_info(url, u'年份:'),
        director = self.fun_movie_info(url, u'导演:'),
        writer = self.fun_movie_info(url, u'编剧:'),
        cast = self.fun_movie_info(url, u'主演:')
        imdb = self.fun_movie_info(url, u'imdb:')
        torrent = self.get_torrent(url)
        data = {
            'url': url,
            'name': name,
            'pic': pic,
            'rate': rate,
            'youming': youming,
            'tag': tag,
            'area': area ,
            'year': year,
            'director': director,
            'writer': writer,
            'cast': cast,
            'imdb': imdb,
            # 'douban': imdb,
            'torrents': torrent
        }

        if debug:
            print data
        return data


if __name__ == '__main__':
    threads = []
    for i in range(1,6):
        t = bttiantang('http://www.bttiantang.com/movie.php?/order/update/%s' %str(i))
        threads.append(t)
    for x in threads:
        x.start()
    for y in threads:
        y.join()



##############################################2016-05-31##############################

# Program Name:test.py
# With Pycharm 2016
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/31

import os
import requests
from bs4 import BeautifulSoup
import lxml
import time

def getSoup(url):
    ok = False  # 这是一个标志，表示网页访问成功
    for _ in range(10):
        try:
            res = requests.get(url)
            res.encoding='utf-8'
            soup = BeautifulSoup(res.text, 'lxml')
            ok = True
            break
        except Exception,e:
            print e
            time.sleep(1)
            continue
    if not ok:  # 如果循环10次都失败则写入错误url到error文件
        with open('error', 'a') as f:
            f.write(url +'\n')
        return
    return soup

def get_movie_url(soup):
    movies = []
    movie_urls = soup.select('p.tt.cl')
    for x in movie_urls:
        movies.append('http://www.bttiantang.com'+ x.a['href']+'#'+x.span.get_text())
    return movies

def getALLMoviePage(soup):
    num = soup.find(class_='pageinfo').strong.get_text()
    return num


def fun_movie_info(soup, text):
    info = []
    category = soup.find(text=text)  # 查找文本
    if category:  # 没有的话直接None
        if category.next_sibling:  # 下面的所有节点不为空
            for x in category.next_siblings:
                info.append(x.get_text())
        else:
            return
    else:
        return
    return info


def get_torrent(soup):
    torrent_hash = []  # 第一个信息是种子名字，第二个是种子大小，第三个是种子hash 中间用'|' 分割
    torrent_info = soup.select('div.tinfo')
    if torrent_info:
        for x in torrent_info:
            a = x.find('span', class_='video').get_text('|')
            b = '|' + x.a['href'].split('uhash=')[1]
            torrent_hash.append(a + b)
    else:
        return
    return torrent_hash

def get_movie_name(soup):
    name = list(soup.select('h2')[0].stripped_strings)
    return name

def getPic(soup):
    pic = soup.select('img')[0].get('src')
    return pic


def getrate(soup):
    rates= list(soup.select('p.rt')[0].stripped_strings)
    rate = ''.join(rates[:-1])
    return rate





if __name__ == '__main__':
    soup1 = getSoup('http://www.bttiantang.com/')
    num = getALLMoviePage(soup1)
    print num
    soup2 = getSoup('http://www.bttiantang.com/?PageNo=1')
    a = get_movie_url(soup2)
    print a
    soup3 = getSoup('http://www.bttiantang.com/subject/28115.html')
    b =fun_movie_info(soup3, u'又名:')
    print b
    c = get_torrent(soup3 )
    print c
    d = get_movie_name(soup3)
    print d
    f = getPic(soup3)
    print f
    g = getrate(soup3)
    print g
    
    
    
##############################################2016-06-03########################################

#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:getSoup.py
# With Pycharm 2016
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/6/3
##一个getsoup包
import os
import requests
from bs4 import  BeautifulSoup
import time
import lxml


def getSoup(url):
    ok = False  # 这是一个标志，表示网页访问成功
    for _ in range(10):
        try:
            res = requests.get(url)
            res.encoding='utf-8'
            soup = BeautifulSoup(res.text, 'lxml')
            ok = True
            break
        except Exception, e:
            print e
            time.sleep(1)
            continue
    if not ok:
        with open('error', 'a') as f:
            f.write(url + '\n')
        return
    return soup


if __name__ == '__main__':
    soup = getSoup('http://www.bttiantang.com')
    print soup
    
##############################################2016-06-03########################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
#
# Program Name:get_movie.py
# With Pycharm 2016
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/5/31

from bs4 import BeautifulSoup
import requests
import threading
import time
import Queue
from mod.getSoup import getSoup


class bt_movie(threading.Thread):
    def __init__(self,queue):
        super(bt_movie, self).__init__()
        self.queue = queue

    def run(self):
        self.fetchAllUrl()


    def getMoivePage(self):
        soup = getSoup('http://www.bttiantang.com')
        num  = soup.find(class_='pageinfo').strong.get_text()
        return num

    def getMovieUrl(self,soup):
        movies = []
        movie_urls = soup.select('p.tt.cl')
        for x in movie_urls:
            movies.append('http://www.bttiantang.com' + x.a['href'] + '#' + x.span.get_text())
        return movies

    def fetchAllUrl(self):
        nums = int (self.getMoivePage())
        for num in xrange(1,nums+1):
            pageUrl = 'http://www.bttiantang.com/?PageNo=%d'% num
            soup = getSoup(pageUrl)
            movie_urls = soup.select('p.tt.cl')
            for url in movie_urls:
                movie_url = 'http://www.bttiantang.com' + url.a['href'] + '#' + url.span.get_text()
                self.queue.put(movie_url)


class movieItim(threading.Thread):
    def __init__(self,queue):
        super(movieItim, self).__init__()
        self.queue = queue

    def run(self):
        self.thread_fetch()



    def getTorrent(self,soup):
        torrent_hash = []  # 第一个信息是种子名字，第二个是种子大小，第三个是种子hash 中间用'|' 分割
        torrent_info = soup.select('div.tinfo')
        if torrent_info:
            for x in torrent_info:
                try:
                    a = x.find('span', class_='video').get_text('|')
                    b = '|' + x.a['href'].split('uhash=')[1]
                    torrent_hash.append(a + b)
                except Exception,e:
                    print soup
                    print e
        else:
            return
        return torrent_hash

    def getTitle(self,soup):
        name = list(soup.select('h2')[0].stripped_strings)
        return name

    def getPic(self,soup):
        pic = soup.select('img')[0].get('src')
        return pic

    def getRate(self,soup):
        rates = list(soup.select('p.rt')[0].stripped_strings)
        rate = ''.join(rates[:-1])
        return rate


    def thread_fetch(self):  # 抓取线程函数
        while True:
            movie_url = self.queue.get()  # 从队列里获取网址
            movieUrl = movie_url.split('#')
            data =self.fetch(movieUrl[0],movieUrl[1])
            print data
            self.queue.task_done()  # 如果完成这通知队列此任务完成

    def fetch(self,url,pubtime):
        soup = getSoup(url)
        if pubtime:
            pubtime = pubtime
        else:
            pubtime = 1990/01/01

        title = self.getTitle(soup)
        pic =  self.getPic(soup)
        rate = self.getRate(soup)
        youming = self.getMovieInfo(soup, u'又名:')
        tag = self.getMovieInfo(soup, u'标签:')
        area = self.getMovieInfo(soup, u'地区:'),
        year = self.getMovieInfo(soup, u'年份:'),
        director = self.getMovieInfo(soup, u'导演:'),
        writer = self.getMovieInfo(soup, u'编剧:'),
        cast = self.getMovieInfo(soup, u'主演:')
        imdb = self.getMovieInfo(soup, u'imdb:')
        torrent = self.getTorrent(soup)
        data ={
            'url':url,
            'pubtime':pubtime,
            'titile':title,
            'pic':pic,
            'rate':rate,
            'youming': youming,
            'tag': tag,
            'area': area,
            'year': year,
            'director': director,
            'writer': writer,
            'cast': cast,
            'imdb': imdb,
            'torrents': torrent
        }
        return data

    def getMovieInfo(self,soup,text):
        info = []
        category = soup.find(text=text)  # 查找文本
        if category:  # 没有的话直接None
            if category.next_siblings:  # 下面的所有节点不为空
                for x in category.next_siblings:
                    info.append(x.get_text())
            else:
                return None
        else:
            return None
        return info

def main():
    queue= Queue.Queue()
    processed = []
    for i in range(5):
        processed.append(bt_movie(queue))
        processed.append(movieItim(queue))

    # start processes
    for i in range(len(processed)):
        processed[i].start()

    # join processes
    for i in range(len(processed)):
        processed[i].join()


if __name__ == '__main__':
    main()





import win32com.client
speaker = win32com.client.Dispatch(&quot;SAPI.SpVoice&quot;)
speaker.Speak(&quot;Hello, it works!&quot;)





















</rich_text></node><node name="代码注释" prog_lang="python" readonly="False" tags="" unique_id="23"><rich_text>##############################################down.py##############################################
import os,sys
import urllib2
from time import time,sleep

path = os.path.dirname(os.path.realpath(sys.argv[0]))

#proxies = {'http':'http://proxyaddress:port'}
#proxy_support = urllib2.ProxyHandler(proxies)
#opener = urllib2.build_opener(proxy_support, urllib2.HTTPHandler)
#urllib2.install_opener(opener)

#functions
def report(blocknum, bs, size, t):   #是一个下载报告函数
    if t == 0:
        t = 1 
    if size == -1:
        print '%10s' % (str(blocknum*bs)) + ' downloaded | Speed =' + '%5.2f' % (bs/t/1024) + 'KB/s'
    else:
        percent = int(blocknum*bs*100/size)
        print '%10s' % (str(blocknum*bs)) + '/' + str(size) + 'downloaded | ' + str(percent) + '% Speed =' + '%5.2f'%(bs/t/1024) + 'KB/s'
    
def httpfetch(url, headers={}, reporthook=report, postData=None, report=True):
    ok = False    #这是一个标志，表示网页访问成功
    for _ in range(10): #循环10次，访问网页如果出错,记录在error这个文件里
        try:
            reqObj = urllib2.Request(url, postData, headers) #访问url,postdata是post参数是一个字典,headers是网页头部文件
            fp = urllib2.urlopen(reqObj) #访问网页
            headers = fp.info() #得到网页头文件的信息
            ok = True #访问成功标志
            break #循环退出
        except:
            sleep(1) #如果访问失败则休息1秒
            continue            #继续循环

    if not ok:    #如果循环10次都失败则写入错误url到error文件
        open(path+'/errors','a').write(url+'\n')
        return ''

    rawdata = ''
    bs = 1024*8
    size = -1
    read = 0
    blocknum = 0
    
    if reporthook and report:
        if &quot;content-length&quot; in headers:  #长度在headers里 则打印下载报告函数
            size = int(headers[&quot;Content-Length&quot;])
        reporthook(blocknum, bs, size, 1)
        
    t0 = time()   #获得时间
    while 1:
        block = ''
        try:
            block = fp.read(bs)
        except:
            open(path+'/errors','a').write(url+'\n')
            return ''
        if block == &quot;&quot;:
            break
        rawdata += block
        read += len(block)
        blocknum += 1
        if reporthook and report:
            reporthook(blocknum, bs, size, time()-t0)
        t0 = time()
            
    # raise exception if actual size does not match content-length header
    if size &gt;= 0 and read &lt; size:
        raise ContentTooShortError(&quot;retrieval incomplete: got only %i out &quot;
                                    &quot;of %i bytes&quot; % (read, size), result)

    return rawdata   #这是返回数据
    
if __name__ == '__main__':    
    url = 'http://www.verycd.com/entries/750943/'

    #test it
    data = httpfetch(url)  #访问网页
    open('down','w').write(data) #写入数据

##############################################fetchvc.py##############################################
import urllib
import re
import sqlite3
import time
import os,sys

from threading import Thread   #多线程模块
from Queue import Queue   #队列模块

from download import httpfetch  #引入网页下载模块

path = os.path.dirname(os.path.realpath(sys.argv[0]))   #程序所在的完整路径目录，os.path.realpath(sys.argv[0])是获得完整路径，前面dirname是获得目录
conn = sqlite3.connect(path+'/verycd.sqlite3.db')  #连接一个sqlite数据库，没有则会创建这个文件
conn.text_factory = str #模式，因为要插入中文，2种方法一种树模式改为str，一直是把插入的数据转换为unicode(str.decode('utf8')) 
#插入中文到sqlite3的相关说明网址 http://my.oschina.net/letiantian/blog/217770
q = Queue()  #队列
MAXC = 8   #最大线程

def thread_fetch():   #抓取线程函数
	conn = sqlite3.connect(path+'/verycd.sqlite3.db')  #连接数据库
	conn.text_factory = str  #模式改变
	while True:
		topic = q.get()  #从队列里获取网址
		fetch(topic,conn) #这是真正的板砖的函数，传入2个参数一个是网址,一个是数据库链接
		q.task_done()  #如果完成这通知队列此任务完成

def search(keyword,full=True): #搜索函数,keyword是关键字,full是查询所有
	'''search verycd, fetch search results'''
	url = 'http://www.verycd.com/search/folders/'+keyword #关键字搜索
	print 'fetching search results ...'
	res = httpfetch(url)  #打开关键字网址
	topics = re.compile(r'/topics/(\d+)',re.DOTALL).findall(res) #通过正则找到所有资源
	topics = set(topics) #去除重复的连接
	links = []  #连接列表
	if full:
		links = re.compile(r'/search/folders/(.*?\?start=\d+)',re.DOTALL).findall(res)
		print links   
	print topics
	if topics:
		for topic in topics:
			q.put(topic)
	if full and links:
		for key in links:
			search(key,full=False)
		

def hot():
	''' read verycd hot res and keep update very day '''
	url = 'http://www.verycd.com/'
	print 'fetching homepage ...'
	home = httpfetch(url)
	hotzone = re.compile(r'热门资源.*?&lt;/dl&gt;',re.DOTALL).search(home).group()
	hot = re.compile(r'&lt;a href=&quot;/topics/(\d+)/&quot;[^&gt;]*&gt;(《.*?》)[^&lt;]*&lt;/a&gt;',re.DOTALL).findall(hotzone)
	html = '&lt;h2 style=&quot;color:red&quot;&gt;每日热门资源&lt;/h2&gt;\n'
	for topic in hot:
		print 'fetching hot topic',topic[0],'...'
		q.put(topic[0])
		html += '&amp;nbsp;&lt;a target=&quot;_parent&quot; href=&quot;/?id=%s&quot;&gt;%s&lt;/a&gt;&amp;nbsp;\n' % topic
	open(path+'/static/hot.html','w').write(html)

def feed():
	''' read verycd feed and keep update very 30 min '''
	url = 'http://www.verycd.com/sto/feed'
	print 'fetching feed ...'
	feeds = httpfetch(url)
	ids = re.compile(r'/topics/(\d+)',re.DOTALL).findall(feeds)#根据feed抓取资源
	ids = set(ids) #去重
	print ids
	now = time.mktime(time.gmtime())
	for id in ids:
		q.put(id) #写入队列
		#updtime = fetch(id)
		#updtime = time.mktime(time.strptime(updtime,'%Y/%m/%d %H:%M:%S'))-8*3600 #gmt+8-&gt;gmt
		#diff = now - updtime
		#print '%10s secs since update' % (diff)
		#if diff &gt; 1900: # only need recent 30min updates
		#	break

def update(num=10): #抓取前10也内容
	urlbase = 'http://www.verycd.com/sto/~all/page'
	for i in range(1,num+1): #记住循环这里是num不包括num+1
		print 'fetching list',i,'...'		
		url = urlbase+str(i) #拼接字符串形成url
		res = httpfetch(url)
		res2 = re.compile(r'&quot;topic-list&quot;(.*?)&quot;pnav&quot;',re.DOTALL).findall(res) #正则匹配
		if res2:
			res2 = res2[0]  
		else:
			continue
		topics = re.compile(r'/topics/(\d+)',re.DOTALL).findall(res2) #继续匹配,先抓大在抓小原则
		topics = set(topics) #去重
		print topics	
		for topic in topics:
			q.put(topic) #写入队列
		

def fetchall(ran='1-max',debug=False): #重点来了，全抓
	urlbase = 'http://www.verycd.com/archives/' #基地址
	if ran == '1-max':  #如果参数是1-max
		m1 = 1 #range循环的下限
		res = urllib.urlopen(urlbase).read()
		m2 = int(re.compile(r'archives/(\d+)').search(res).group(1)) #m2是循环的上限
	else:
		m = ran.split('-')  #如果是1-10 那么m1是1,m2是10
		m1 = int(m[0])
		m2 = int(m[1])
	print 'fetching list from',m1,'to',m2,'...'
	for i in range(m1,m2+1):  #开始循环
		url = urlbase + '%05d'%i + '.html' #拼接字符串形成网页
		print 'fetching from',url,'...'
		res = httpfetch(url) #访问网页
		ids = re.compile(r'topics/(\d+)/',re.DOTALL).findall(res) #正则匹配页面
		print ids 
		for id in ids:
			q.put(id) #写入队列
	

def fetch(id,conn=conn,debug=False): #第一个参数是每个资源id,参数2是sqlite3的conn,第三个参数是是否打印数据用来调试
	print 'fetching topic',id,'...'
	urlbase = 'http://www.verycd.com/topics/'
	url = urlbase + str(id)

	res = ''
	for _ in range(3):   #访问3次,以防网页打不开
		try:
			res = httpfetch(url,report=True)
			break
		except:
			continue #出错继续

	abstract = re.compile(r'&lt;h1&gt;.*?visit',re.DOTALL).findall(res) #在得到的html代码中筛选,先打后小的方式过滤一次代码
	if not abstract:   
		print res
		if res == '' or '很抱歉' in res:
			print 'resource does not exist'
			return
		else:
			print 'fetching',id,'again...'
			return fetch(id,conn)  #递归在抓取
	abstract = abstract[0]
    

	title = re.compile(r'&lt;h1&gt;(.*?)&lt;/h1&gt;',re.DOTALL).findall(abstract)[0] #从过滤的代码中查找标题
	status = re.compile(r'&quot;requestWords&quot;&gt;(.*?)&lt;',re.DOTALL).search(abstract).group(1)  #这个应该是状态
	brief = re.compile(r'&quot;font-weight:normal&quot;&gt;&lt;span&gt;(.*?)&lt;/td&gt;',re.DOTALL).search(abstract).group(1)
	brief = re.compile(r'&lt;.*?&gt;',re.DOTALL).sub('',brief).strip()
	pubtime = re.compile(r'&quot;date-time&quot;&gt;(.*?)&lt;/span&gt;.*?&quot;date-time&quot;&gt;(.*?)&lt;/span&gt;',re.DOTALL).findall(abstract)[0] #发布时间
	category1 = re.compile(r'分类.*?&lt;td&gt;(.*?)&amp;nbsp;&amp;nbsp;(.*?)&amp;nbsp;&amp;nbsp;',re.DOTALL).findall(abstract)[0] #分类
	category = ['','']
	category[0] = re.compile(r'&lt;.*?&gt;',re.DOTALL).sub('',category1[0]).strip()
	category[1] = re.compile(r'&lt;.*?&gt;',re.DOTALL).sub('',category1[1]).strip()  #分类

	res2 = re.compile(r'iptcomED2K&quot;&gt;&lt;!--eMule.*?&lt;!--eMule end--&gt;',re.DOTALL).findall(res)[0] #这个是获取ed2k链接的

	ed2k = re.compile(r'ed2k=&quot;([^&quot;]*)&quot; subtitle_[^=]*=&quot;([^&quot;]*)&quot;&gt;([^&lt;]*)&lt;/a&gt;',re.DOTALL).findall(res2)
	ed2k.extend( re.compile(r'ed2k=&quot;([^&quot;]*)&quot;&gt;([^&lt;]*)&lt;/a&gt;',re.DOTALL).findall(res2) )

	content = re.compile(r'&lt;!--eMule end--&gt;(.*?)&lt;!--Wrap-tail end--&gt;',re.DOTALL).findall(res)

	if content:
		content = content[0]
		content = re.compile(r'&lt;br /&gt;',re.DOTALL).sub('\n',content)
		content = re.compile(r'&lt;.*?&gt;',re.DOTALL).sub('',content)
		content = re.compile(r'&amp;.*?;',re.DOTALL).sub(' ',content)
		content = re.compile(r'\n\s+',re.DOTALL).sub('\n',content)
		content = content.strip()
	else:
		content=''

	if debug:
		print title
		print status
		print brief
		print pubtime[0],pubtime[1]
		print category[0],category[1]
		for x in ed2k:
			print x
		print content

	ed2kstr = ''
	for x in ed2k:
		ed2kstr += '`'.join(x)+'`'  #把抓到的ed2k数组转换成字符串用字符'`'分割

	if not dbfind(id,conn):   #假如在数据库中没找到这个资源id那么写入,否则update
		dbinsert(id,title,status,brief,pubtime,category,ed2kstr,content,conn)
	else:
		dbupdate(id,title,status,brief,pubtime,category,ed2kstr,content,conn)

	return pubtime[1]

def dbcreate():  #建表
	c = conn.cursor()
	c.execute('''create table verycd(
		verycdid integer primary key,
		title text,
		status text,
		brief text,
		pubtime text,
		updtime text,
		category1 text,
		category2 text,
		ed2k text,
		content text
	)''')
	conn.commit()
	c.close()

def dbinsert(id,title,status,brief,pubtime,category,ed2k,content,conn): #写入表
	c = conn.cursor()
	c.execute('insert into verycd values(?,?,?,?,?,?,?,?,?,?)',\
		(id,title,status,brief,pubtime[0],pubtime[1],category[0],category[1],\
		ed2k,content))
	conn.commit()
	c.close()

def dbupdate(id,title,status,brief,pubtime,category,ed2k,content,conn): #更新
	c = conn.cursor()
	c.execute('update verycd set verycdid=?,title=?,status=?,brief=?,pubtime=?,\
		updtime=?,category1=?,category2=?,ed2k=?,content=? where verycdid=?',\
		(id,title,status,brief,pubtime[0],pubtime[1],category[0],category[1],\
		ed2k,content,id))
	conn.commit()
	c.close()

def dbfind(id,conn): #查询
	c = conn.cursor()
	c.execute('select 1 from verycd where verycdid=?',(id,))
	c.close()
	for x in c:
		if 1 in x:
			return True
		else:
			return False

def dblist(): #罗列
	c = conn.cursor()
	c.execute('select * from verycd')
	for x in c:
		for y in x:
			print y

def usage(): #用法
	print '''Usage:
  python fetchvc.py createdb
  python fetchvc.py fetchall
  python fetchvc.py fetch 1-1611 #fetch archive list
  python fetchvc.py fetch 5633~5684 #fetch topics
  python fetchvc.py fetch 5633 #fetch a topic
  python fetchvc.py fetch q=keyword
  python fetchvc.py list #list the database
  python fetchvc.py feed #run every 30 min to keep up-to-date
  python fetchvc.py hot
  python fetchvc.py update #update first 20 pages, run on a daily basis'''

if __name__=='__main__':
	#initialize thread pool
	for i in range(MAXC): #用8个线程启动，从队列里获取数据抓取
		t = Thread(target=thread_fetch)
		t.setDaemon(True)
		t.start()  

	if len(sys.argv) == 1:
		usage()
	elif len(sys.argv) == 2:
		if sys.argv[1] == 'createdb':
			dbcreate()
		elif sys.argv[1] == 'fetchall':
			fetchall()
		elif sys.argv[1] == 'update':
			update(20)
		elif sys.argv[1] == 'update1':
			update(1)
		elif sys.argv[1] == 'feed':
			feed()
		elif sys.argv[1] == 'hot':
			hot()
		elif sys.argv[1] == 'list':
			dblist()
	elif len(sys.argv) == 3:
		if sys.argv[1] != 'fetch':
			usage()
		elif '-' in sys.argv[2]:
			fetchall(sys.argv[2])
		elif '~' in sys.argv[2]:
			m = sys.argv[2].split('~')
			for i in range(int(m[0]),int(m[1])+1):
				q.put(i)
		elif sys.argv[2].startswith(&quot;q=&quot;):
			search(sys.argv[2][2:])
		else:
			fetch(int(sys.argv[2]),debug=True)

	# wait all threads done
	q.join()

################################python2.7+qt5################################
import sys
from PyQt5 import QtCore, QtWidgets


class ProgressBar(QtWidgets.QWidget):
    def __init__(self, parent=None):
        QtWidgets.QWidget.__init__(self, parent)
        self.setGeometry(300, 300, 250, 150)
        self.setWindowTitle('ProgressBar')

        self.pbar = QtWidgets.QProgressBar(self)
        self.pbar.setGeometry(30, 40, 200, 25)

        self.button = QtWidgets.QPushButton('Start', self)
        self.button.setFocusPolicy(QtCore.Qt.NoFocus)
        self.move(40, 80)

        #self.connect(self.button, QtCore.SIGNAL('clicked()'), self.onStart)
        self.button .clicked.connect(self.onStart)  # 这里是qt5的用法 用按钮直接connect方法
        self.timer = QtCore.QBasicTimer()
        self.step = 0

    def timerEvent(self, event):
        if self.step &gt;= 100:
            self.timer.stop()
            return
        self.step += 1
        self.pbar.setValue(self.step)

    def onStart(self):
        if self.timer.isActive():
            self.timer.stop()
            self.button.setText('Start')
        else:
            self.timer.start(100, self)
            self.button.setText('Stop')




if __name__=='__main__':
    app = QtWidgets.QApplication(sys.argv) #初始化一个app
    mainWindow = ProgressBar() #初始化类
    mainWindow.show() #显示框口
    sys.exit(app.exec_()) #进入消息循环
</rich_text></node><node name="自己的程序" prog_lang="python" readonly="False" tags="" unique_id="39"><rich_text>
</rich_text><node name="SqliteHelper" prog_lang="python" readonly="False" tags="" unique_id="27"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2018/6/9
import sqlite3
'''
封装sqlite3,创建表增删改用DbExecute,查询DbQuery,可以直接传语句，也可以用?占位,后面传入元组
DbExecute和DbQuery 区别是commit提交
'''

class DBDriver:

    def __init__(self, dbfile):
        try:
            self.conn = sqlite3.connect(dbfile)
            self.cur = self.conn.cursor()
        except Exception as err:
            print('Connect db Error: ' + str(err))

    def DbExecute(self, sql,*params):  #*params是一个元组
        try:
            self.cur.execute(sql,*params)  # 执行sql语句
            self.conn.commit()
        except Exception as err:
            print('Execute sql scraipt Error: '+ str(err) )

    def DbQuery(self, sql,*params):
        try:
            self.cur.execute(sql,*params)  # 执行sql语句
            return self.cur.fetchall()
        except Exception as err:
            print('Select table Error: ' + str(err))

    def close(self):
        self.cur.close()
        self.conn.close()


if __name__ == &quot;__main__&quot;:
    Test = DBDriver('train.db')
    tbSql1 = &quot;create table tumblr(id integer primary key,tumblrName varchar(128),total varchar(128))&quot;
    Test.DbExecute(tbSql1)
    tbsql2 = &quot;insert into tumblr values (1,'godbells','1000') &quot;
    Test.DbExecute(tbsql2)
    tbsql3= &quot;insert into tumblr values (?,?,?) &quot;
    tbsql3p= (2,'godbells','2000')
    Test.DbExecute(tbsql3,tbsql3p)
    tbsql4 = &quot; select * from tumblr where id=1 &quot;
    a = Test.DbQuery(tbsql4)
    print (a)
    tbsql5 = &quot; select * from tumblr where id= ?&quot;
    tbsql5p = (2,)
    b = Test.DbQuery(tbsql5,tbsql5p)
    print (b)
    tbsql6 = &quot; select * from tumblr&quot;
    c = Test.DbQuery(tbsql6)
    print (c)






</rich_text></node><node name="MysqlHelper" prog_lang="python" readonly="False" tags="" unique_id="42"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2018
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/9/4
import pymysql

'''
mysql的占位符和sqlite不一样，sqlite是?,mysql是%s
DbExec方法是带回滚的执行
'''

class DBHelper():

    def __init__(self,host,user,passwd,db,port=3306):
        self.host = host
        self.port = port
        self.user = user
        self.passwd = passwd
        self.db = db
        self.charset = &quot;utf8&quot;
        try:
            self.conn = pymysql.connect(host=self.host,
                                        db=self.db,
                                        user=self.user,
                                        passwd=self.passwd,
                                        port=self.port,
                                        charset=self.charset,
                                        connect_timeout=60)
            self.cur = self.conn.cursor()
        except:
            print('connect error!')

    def DbExecute(self, sql,*params):  #*params是一个元组
        try:
            self.cur.execute(sql,*params)  # 执行sql语句
            self.conn.commit()
        except Exception as err:
            print('Execute sql scraipt Error: '+ str(err) )

    def DbQuery(self, sql, *params):
        try:
            self.cur.execute(sql, *params)  # 执行sql语句
            return self.cur.fetchall()
        except Exception as err:
            print('Select table Error: ' + str(err) )

    def DbExec(self, sql,*params): #这个是带回滚的
        try:
            self.cur.execute(sql,*params)  # 执行失败,回滚数据
            self.conn.commit()
        except Exception as err:
            self.conn.rollback()
            print('Execute sql scraipt Error,data rollback : '+ str(err) )


    def close(self):
        self.cur.close()
        self.conn.close()










if __name__ == &quot;__main__&quot;:
    mysqldb = DBHelper('127.0.0.1','root','78619808','test')
    sql = &quot; select * from crawlblog limit %s &quot;
    params =(5,)
    c = mysqldb.DbQuery(sql,params)
    print(c)















</rich_text></node><node name="SendMail" prog_lang="python" readonly="False" tags="" unique_id="40"><rich_text>########################################################py2.7 sendmail########################################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
import os
'''
send_mail v1版本 
方法send发邮件 传入文件名或者文件夹,如果出现文件和文件夹混合的情况将判断为是否为文件添加进列表
'''

class Send_Mail(object):

    def __init__(self, host, user, passwd):
        self._user = user
        self._account = user.split('@')[0]
        self._me = self._account + &quot;&lt;&quot; + self._user + &quot;&gt;&quot;
        server = smtplib.SMTP()
        server.connect(host)
        server.login(self._account, passwd)
        self._server = server


    def send(self, to_list, sub, content, attfile = [],subtype='html'):
        msg = MIMEMultipart()
        msg['Subject'] = sub
        msg['From'] = self._me
        msg['To'] = &quot;;&quot;.join(to_list)
        msg.attach(MIMEText(content, _subtype=subtype, _charset='utf-8'))
        files = self.att_add_file(attfile)
        for plugin in files:
            att = MIMEApplication(open(plugin, 'rb').read())
            att_name = os.path.basename(plugin)
            att.add_header('content-disposition', 'attachment', filename=att_name)
            msg.attach(att)
        try:
            self._server.sendmail(self._me, to_list, msg.as_string())
            return True
        except Exception, e:
            print str(e)
            return False

    def att_add_file(self,path=[]):
        real_file = []
        for file in path:
            if os.path.isdir(file):
                for root, dirs, files in os.walk(file):
                    for name in files:
                        real_file.append(os.path.join(root, name))
            if os.path.isfile(file):
                real_file.append(file)
        return real_file

if __name__ == '__main__':
    mailto_list = ['XXXX@qq.com','XXXX@qq.com']
    mail = Send_Mail('smtp.163.com', 'XXXXX@163.com', 'ZZZZZ')
    mail.send(mailto_list,'资料文档10','详见清单',[r'F:\ros',r'F:\123',r'E:\project\bt\log_test.py',r'E:\project\bt\test_log.py',r'E:\project\bt\test1.py'])
    

########################################################py2.7 send_mail demo########################################################            
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText


class Send_Mail(object):

    def __init__(self, host, user, passwd):
        self._user = user
        self._account = user.split('@')[0]
        self._me = self._account + &quot;&lt;&quot; + self._user + &quot;&gt;&quot;
        server = smtplib.SMTP()
        server.connect(host)
        server.login(self._account, passwd)
        self._server = server

    def send(self,to_list,sub,content,subtype='html'):
        msg = MIMEMultipart()
        msg['Subject'] = sub
        msg['From'] = self._me
        msg['To'] = &quot;;&quot;.join(to_list)
        msg.attach(MIMEText(content, _subtype=subtype, _charset='utf-8'))
        try:
            self._server.sendmail(self._me, to_list, msg.as_string())
            return True
        except Exception, e:
            print str(e)
            return False
        finally:
            self._server.quit()



if __name__ == '__main__':
    mail = Send_Mail('smtp.163.com', 'kylinshine@163.com', 'XXXXXXXX')
    mail.send(['78619808@qq.com'],u'2017年流向费用',u'附件是流向内容')

</rich_text></node><node name="CompressZip" prog_lang="python" readonly="False" tags="" unique_id="41"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808


import os, os.path
import zipfile

class Compress_Zip(object):

    def __init__(self, files,zipfilename,debug = False): #debug如果为True则输出压缩信息
        self._files = files
        self._zipfilename = zipfilename
        self._debug = debug                                #属性写在前面.方法写在后面可能因为方法在调用写在后面的属性报错
        self.zip_dir(self._files,self._zipfilename )


    def zip_dir(self, dirname, zipfilename):
        filelist = self.add_file(dirname)
        try:
            zf = zipfile.ZipFile(zipfilename, &quot;w&quot;, zipfile.zlib.DEFLATED)
            for tar in filelist:
                if self._debug:
                    zf.write(tar)
                    print u'%s 已被压缩!' % tar
                else:
                    zf.write(tar)
        except Exception, e:
            print str(e)
            return False
        finally:
            print '压缩完成,存放位置在 ' + self._zipfilename
            zf.close()

    def add_file(self,path=[]):               #对传入的列表,会自动扫描目录下的文件，如果是空的文件夹会忽略
        real_file = []
        for file in path:
            if os.path.isdir(file):
                for root, dirs, files in os.walk(file):
                    for name in files:
                        real_file.append(os.path.join(root, name))
            if os.path.isfile(file):
                real_file.append(file)
        return real_file


if __name__ == '__main__':
    zip = Compress_Zip([u'F:\新建文件夹'],r'f:\123\qq.zip',debug = True)</rich_text></node><node name="nod32_update" prog_lang="python" readonly="False" tags="" unique_id="43"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*
import requests
import re
import time
def httpfetch(url, headers={}):
    ok = False    #这是一个标志，表示网页访问成功
    for _ in range(10): #循环10次，访问网页如果出错,记录在error这个文件里
        try:
            reqObj = requests.get(url, headers=headers) #访问url,postdata是post参数是一个字典,headers是网页头部文件
            html = reqObj.text
            ok = True #访问成功标志
            break #循环退出
        except:
            time.sleep(1) #如果访问失败则休息1秒
            continue            #继续循环

    if not ok:    #如果循环10次都失败则写入错误url到error文件
        open('errors','a').write(url+'\n')
        return ''
    print html
    return html





if __name__ == '__main__':
    url = 'http://isnet.grifon.info/nod/v4/'
    html = httpfetch(url)
    
    
#########################################################################
&quot;&quot;&quot;
encoding: utf-8
language: python 2.7.3
author:   Shijie(Jeremiah Stone)
date:     Mar. 25th,2012
version   4.2.0
purpose:  Spider for the Pirate Bay
modified: Mar. 31th,2012
changelog:added micro-multithread to accelerate each single page catching

&quot;&quot;&quot;
#==========================================================================================
import urllib,urllib2
import re,os,sys
import codecs
import socket
import datetime,time
import threading
from bs4 import BeautifulSoup 
from django.utils.encoding import smart_str, smart_unicode
#==========================================================================================
#GLOBAL VARIABLES
FLAGTITLE=r&quot;Download music, movies, games, software! The Pirate Bay - The galaxy's most resilient BitTorrent site&quot;
FLAGTPB=r&quot;Not Found | The Pirate Bay - The world's most resilient BitTorrent site&quot;
Table=[100, 101, 102, 103, 104, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 299, 300, 301, 302, 303, 304, 305, 306, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 499, 500, 501, 502, 503, 504, 505, 506, 599, 600, 601, 602, 603, 604, 605, 699]
Chars=[r'/','\\',r'\\',r'\\\\',r':',r'?',r'&quot;',r'&lt;',r'&gt;',r'|',r'*']
Ger2Eng={'Porno':'Porn','Programma\'s':'Applications','Andere':'Other','Anders':'Other','Film':'Movie','TV-programma\'s':'TV shows','Hoezen':'Covers','video\'s':'videos','Strips':'Comics','Foto\'s':'Pictures','Gesproken boeken':'Audio books','Muziek':'Music'}
tpbURL=['thepiratebay.se','malaysiabay.org','unblockedpiratebay.com/?loadurl=','labaia.ws']

totDict={} 
url=''
socket.setdefaulttimeout(6000)

switch=False
update=False
copy=True
mode=0
cnt=0
#==========================================================================================
#Regular Expression
re_name=re.compile(r'&lt;optgroup label=\&quot;.*\&quot;&gt;')
re_title=re.compile(r'/torrent/[0-9]+/(.*)')
re_type=re.compile(r'option value=\&quot;&gt;.*&lt;/option&gt;')
re_info=re.compile(r'&lt;pre&gt;.*')
re_size=re.compile(r'&lt;dd&gt;.*\.*&amp;nbsp;.iB&amp;nbsp;\(.*&amp;nbsp;Bytes\)')
re_file=re.compile(r'return false;\&quot;&gt;(.*)&lt;/a&gt;&lt;/dd&gt;')
re_by=re.compile(r'\/user\/(.*)\/\&quot;')
re_anonymous=re.compile(r'&lt;i&gt;(Anonymous)&lt;/i&gt;')
re_uploaded=re.compile(r'&lt;dd&gt;[0-9]{4}-..-.. ..:..:.. ...')
re_comments=re.compile(r'&lt;a href=\&quot;/user/(.*)/\&quot; title=&quot;Browse .*\&quot;&gt;.*&lt;/a&gt;.*\n&lt;/p&gt;')
re_infohash=re.compile(r'&lt;dt&gt;Info Hash:&lt;/dt&gt;&lt;dd&gt;&amp;nbsp;&lt;/dd&gt;\n.*&lt;/dl&gt;')
re_magnet=re.compile(r'\&quot;(magnet:?.*)\&quot; title=\&quot;Get this torrent\&quot;&gt;')
re_mag=re.compile(r'\&quot;(magnet:?.*)\&quot; title=\&quot;Download this torrent using magnet\&quot;&gt;')
re_magex=re.compile(r'(magnet:?.*)&amp;dn=')
re_url=re.compile(r'&lt;a href=\&quot;(.*)\&quot; class=\&quot;detLink\&quot;')
re_genre=re.compile(r'&lt;a href=\&quot;/.+/.+\&quot; title=\&quot;.*\&quot;&gt;(.*)&lt;(/)a&gt;&lt;br /&gt;\n.*\(&lt;a href=\&quot;/.+/.+\&quot; title=\&quot;.*\&quot;&gt;(.*)&lt;/a&gt;\)') 
re_torrent=re.compile(r'&lt;a href=\&quot;//(torrents\.[^/]*/.*)\&quot; title=\&quot;Torrent File\&quot;&gt;Get Torrent File&lt;/a&gt;')
re_part=re.compile(r'[^/]*(/.*)')
#============================================================================================
class item:
    def __init__(self):
        self.title=''
        self.file=''
        self.size=''
        self.id=''
        self.url=''
        self.comments=''
        self.type=''
        self.by=''
        self.magnet=''
        self.info=''
        self.torrent=''
        self.path=''
#============================================================================================
def fetch_dir(pageURL):
    webpage=urllib.urlopen(pageURL) 
    soup=BeautifulSoup(webpage.read())

    #Retrieve category
    catList=[]   
    for tag in soup.find_all('optgroup'):
        name=tag.get('label').replace('/','_')
        catList.append(name)
        
        #Retrieve Sub-category
        urlDict={}
        for sub in tag.find_all('option'):
            code=sub.contents[0].replace('/','_')             
            urlDict[sub.get('value')]=code
        totDict[name]=urlDict
        
    #Make directory
    output=open(r'index.txt','w')    
    for i in totDict:
        output.write(i+'\n')
        if not os.path.exists(i):
            os.mkdir(i)
        for j in totDict[i]:
            output.write(r'    '+''.join(totDict[i][j]+' '+j)+'\n')
            path=os.curdir+r'/'+i+r'/'+''.join(totDict[i][j])             
            if not os.path.exists(path):
                os.mkdir(path)
    output.close()
#======================================================================================        
def fetch_page(pageURL,which,spirit=None):    
    try: 
        webpage=urllib2.urlopen(pageURL,None,600)
        if webpage == None:
            print pageURL,'...reconnecting...'  
            return -1
    except:
    #except(urllib2.HTTPError,urllib2.URLError,IOError,Exception):
        print pageURL,'...reconnecting...' 
        return -1
    
    stream=webpage.read()
    soup=BeautifulSoup(stream)
    
    #print stream    
    #print soup.original_encoding

    #SWITCH BETWEEN PROXY AND ORIGINAL SITE
    pid=''.join(re.findall(re.compile(r&quot;/torrent/([0-9]*)&quot;),pageURL))

    if which == 3:
        SIGNATURE=u''    
        for sig in soup.title.contents:
            SIGNATURE=SIGNATURE+smart_unicode(sig)
        if(switch):        
            if SIGNATURE== FLAGTITLE:
                print pid,'....none....'  
                return 0
        else:
            if SIGNATURE == FLAGTPB:
                print pid,'....none....'  
                return 0
    enableWrite=False
    
    #See if in search mode
    if spirit== None:        
        temp=item()
        temp.id = ''.join(re.findall(re.compile(r&quot;/torrent/([0-9]*)&quot;),pageURL))
                           
        categoryList=soup.find_all('a',title='More from this category')
        
        nameList= soup.find_all('div',id='title')        
        magnetList= re.findall(re_magnet,stream)
        if len(magnetList) &gt; 0:
            mag_info=urllib.unquote(str(magnetList[0]))        
            temp.magnet=urllib.unquote(mag_info)        
        if len(categoryList)&gt;0:
            temp.type=  smart_unicode(''.join(categoryList[0].contents)).strip().replace(' &gt; ',r'/')            
        if len(nameList) &gt;0 :
            temp.title=smart_unicode(filter_char(nameList[0].contents[0].strip())).replace('\t','')          
        spirit=temp        
                
        enableWrite=True

    #Path 
    path=os.curdir+'/'+spirit.type.replace('(iPad/iPhone)','(iPad_iPhone)')
    target=path+'/'+spirit.title+'.index'   
    
    if update == True:
        if os.path.exists(target):
            print spirit.id,'....skip....',target
            return 1
        elif enableWrite==True:
            save_all(spirit,which)
            
        
    output=codecs.open(target,'w','utf-8')  
    output.write( &quot;%-15s%s\r\n&quot;%(u'[Title]',spirit.title))
    
    #File
    number=re.findall(re_file,stream)
    spirit.file=''.join(number)
     
    output.write(&quot;%-15s%s\r\n&quot;%(u'[Id ]',spirit.id))
    output.write(&quot;%-15s%s\r\n&quot;%(u'[Files]',spirit.file))     
                
    #Size
    size= re.findall(re_size,stream)
    spirit.size=''.join(size)[4:].replace(r'&amp;nbsp;',' ')
    output.write(&quot;%-15s&quot;%(u'[Size]'))
    output.write(spirit.size)
    output.write('\r\n')
    
    #User
    user = re.findall(re_anonymous,stream)
    if len(user) &gt; 0:
        output.write(&quot;%-15s%s\r\n&quot;%('[User] ',smart_unicode(user[0])))
    else:
        user=re.findall(re_by,stream)
        if len(user) &gt; 0:
           output.write(&quot;%-15s%s\r\n&quot;%('[User] ',smart_unicode(user[0])))          
    
    #Type
    output.write(&quot;%-15s&quot;%'[Type]')
    output.write(smart_unicode(spirit.type))
    output.write('\r\n')
    
    #Time
    upload= re.findall(re_uploaded,stream)            
    output.write(&quot;%-15s%s\r\n&quot;%('[Upload] ',''.join(upload)[4:]))
    
    #Hash    
    hashinfo= re.findall(re_infohash,stream)    
    output.write(&quot;%-15s%s\r\n&quot;%('[Hash Info] ',''.join(hashinfo)[35:].replace('&lt;/dl&gt;','').strip()))
    
    #Torrent
    download= re.findall(re_torrent,stream)    
    if download != None and len(download)&gt;0:
        torrent=smart_unicode(download[0])
        torid=''.join(re.findall(re_part,torrent))
        output.write(&quot;%-15s&quot;%('[Torrent]'))
        output.write(torid)
        output.write(&quot;\r\n&quot;)     
        src=r'http://'+torrent
        tar=path+r'/torrents/'+spirit.title+r'.torrent'
        #print src
        if copy == True:            
            if not os.path.exists(path+r'/torrents'):
                os.mkdir(path+r'/torrents')
            if not os.path.exists(tar) or not update:
                try:
                    urllib.urlretrieve(src, urllib.unquote(tar))
                except IOError:
                    pass
    
    output.write(&quot;%-15s&quot;%(u'[Magnet]'))
    output.write(smart_unicode(spirit.magnet))
    output.write('\r\n')
    
    #Description
    info=soup.find_all('pre')
    output.write(u'[Description]'+'\r\n')
    for inf in info:
        for t in inf.contents:            
            output.write(smart_unicode(t).replace(r'&lt;a href=&quot;','').replace(r'&lt;/a&gt;','').replace(r'rel=&quot;nofollow&quot;&gt;','').replace(r'&amp;quot;','') ) 
              
    #Comments
    comment_usr= re.findall(re_comments,stream)    
    
    k=0
    output.write(&quot;\r\n%-15s\r\n&quot;%(u'[Comments]'))
    for cmt in soup.find_all('div','comment'):  
        #output.write(unicode(comment_usr[k],errors='ignore')+': ')
        output.write(smart_unicode(comment_usr[k])+': ' )                          
        k=k+1  
        for c in cmt.contents:
            output.write(smart_unicode(c).replace(r'&lt;br/&gt;','').replace(r'&lt;br /&gt;','').replace('\n','').replace('\r','').replace('&lt;/a&gt;','').replace(r'&lt;a href=&quot;',''))             
        output.write('\r\n')
        
    output.close()   
    
    print spirit.id,'....done....',target 
    return 1
#==========================================================================================
def save_all(spirit,which):
    path = os.curdir+r'/'+spirit.type+r'/'+spirit.title
    filename = path + '.index' 
    mag_info=''.join(re.findall(re_magex,spirit.magnet))
    index_path=os.curdir+'/index/'+spirit.type.replace('iPad/iPhone','iPad_iPhone').replace('/','.')+'.index'
    index_content= spirit.id+'|'+spirit.title+'|'+spirit.type.replace('/','|')+'|'+mag_info    
    
    if not os.path.exists(filename):
        #print '@',index_path,index_content
        save_index(index_path,index_content)
        if  which == 1 :
            save_index(os.curdir+'/index/recent.index',index_content)
        elif which == 2:
            save_index(os.curdir+'/index/top/'+spirit.type.replace('/','.')+'.index',index_content)    

def save_index(path,content):     
    out=codecs.open(path,'a','utf8')
    out.write(content)
    out.write('\n')
    out.close()

def filter_char(string):
    string=string.replace('Con.','Concert.')
    for ch in Chars:
        string=string.replace(ch,'_')
    return string

def convert(string):
    for word in Ger2Eng:
        string=string.replace(word,Ger2Eng[word])
    return string
#==========================================================================================            
def fetch_url(pageURL,which):
    cout=open(os.curdir+'/torrent_records.index','a')
    cout.write(pageURL+'\r\n')
    cout.close()
    try: 
        webpage=urllib2.urlopen(pageURL,None,600)
        if webpage == None:
            print pageURL,'...reconnecting...'
            return None
    except:
    #except(urllib2.HTTPError,urllib2.URLError,IOError,Exception):
        print pageURL,'...reconnecting...' 
        return None
    
    stream=webpage.read()
    sp=BeautifulSoup(stream)

    urlList=[]
    magList=[]
    titleList=[]

    genList=re.findall(re_genre,stream)
    
    rawList=sp.find_all('a','detLink')
    for u in rawList:
        urlList.append(u.get('href'))
        titleList.append(smart_unicode(filter_char(u.contents[0].strip())).replace('\t',''))
        #titleList.append( smart_unicode(''.join(u.contents)))
         
    rawList=sp.find_all('a',title='Download this torrent using magnet')
    for u in rawList:
        magList.append(u.get('href'))
    global cnt
    cnt=0    
    if len(magList)!= len(urlList):
        print len(magList),len(urlList)
        for i in range(len(urlList)):
            u=''.join(re.findall(re.compile(r&quot;/(torrent/[0-9]*/)&quot;),urlList[i]))
            new_thread(baseURL+u,which)
            #fetch_page(baseURL+u,which)
        cur=time.time()
        while cnt&lt;len(urlList):
            if time.time()-cur&gt;30:
                break
            pass
        print baseURL+u,'@',cnt
        return -1
        
    itemList=[]
    for i in range(len(urlList)):
        it=item()
        it.title=filter_char(titleList[i]).strip().replace('\t','')
        it.type=convert(''.join(genList[i]))
        it.url=''.join(re.findall(re.compile(r&quot;/(torrent/[0-9]*/)&quot;),urlList[i]))        
        it.id=''.join(re.findall(re.compile(r&quot;torrent/([0-9]*)&quot;),it.url))       
        it.magnet=urllib.unquote(magList[i])         
        itemList.append(it)
    
    return itemList
#==========================================================================================
class catcher(threading.Thread):
    def __init__(self,addr,which,it):
        threading.Thread.__init__(self)
        self.addr=addr
        self.which=which
        self.it=it
    def run(self):
        global cnt
        fetch_page(self.addr,self.which,self.it)
        cnt=cnt+1         
def new_thread(pageURL,which,it=None):
    th=catcher(pageURL,which,it)
    th.setDaemon(True)
    th.start()
    th.join(0.2)
#===============================================================================
def fetch(pageURL, which):
    print pageURL,'...loading...'
    itemList=fetch_url(pageURL,which)    
    if itemList==-1:
        return
    while itemList==None or itemList==[]:
        itemList=fetch_url(pageURL,which)     
    
    if len(itemList) &lt;=0: return
    global cnt
    cnt=0
    totcnt=0
    
    for i in range(len(itemList)): 
        new_url=baseURL+itemList[i].url
        path =os.curdir+r'/'+itemList[i].type+r'/'+itemList[i].title
        filename = path + '.index' 
        
        save_all(itemList[i],which)
        if update == False or not os.path.exists(filename):
            new_thread(new_url,which,itemList[i])
            totcnt=totcnt+1
            continue
        print itemList[i].id,'....skip....',filename
    curtime=time.time()
    while cnt&lt;totcnt:
        if (time.time()-curtime) &gt;30:
            break
        pass
    print pageURL,'@',cnt
        
def fetch_recent():
    for i in range(0,101):
        pageURL= baseURL+'recent/'+str(i)      
        fetch(pageURL,1)

def fetch_top(which=0):
    if which == 0:
        for u in Table:
            pageURL= baseURL+'top/'+str(u)              
            fetch(pageURL,2)
            if u%10==0:
                pageURL=baseURL+'top/48h'+str(u)
                fetch(pageURL,2)
        pageURL=baseURL+'top/0'
        fetch(pageURL,2)
        pageURL=baseURL+'top/all'
        fetch(pageURL,2)
        pageURL=baseURL+'top/48hall'
        fetch(pageURL,2)
    else:
        pageURL= baseURL+'top/'+str(which)          
        fetch(pageURL,2)             

def fetch_all(n,s=0,e=100,x=0,y=16):    
    for i in range(s,e):
        for j in range(x, y):            
            pageURL= baseURL+'browse/'+str(n) +'/'+ str(i) + '/' + str(j) + '/'            
            fetch(pageURL,0)
            
def fetch_range(s,e):
    curcnt=0
    global cnt
    cnt=0
    for i in range(s,e+1):
        pageURL=baseURL+r'torrent/'+str(i)
        new_thread(pageURL,3)
        curcnt=curcnt+1
        curtime=time.time()
        if curcnt&gt;10:            
            while cnt&lt;10:               
                if time.time()-curtime&gt;10:                     
                    break
                pass
            cnt=0
            curcnt=0
            
        #while True:
        #if fetch_page(pageURL,3) != -1: break        
#========================================================================================== 
def header():
    head='[resource id]:\n'
    sss=''.join(str(Table))
    mark=0
    for i in Table:
        head=head+str(i)+' '
        mark=mark+1
        if mark%10==0: head=head+'\n'
    print head 
#==========================================================================================
if __name__ =='__main__':
 
    k=0
    for u in tpbURL:
        print k,'('+u+')'
        k=k+1
    k=int(raw_input('choose a site: '))
    url=tpbURL[k]
   
    if k== len(tpbURL)-1:
        switch=True
    else:
        switch=False

    baseURL=r'http://'+url+r'/'     
    
    if int(raw_input(r'0(all) 1(update): '))==1:  update=True
    else:  update=False 
    
    if not os.path.exists(os.curdir+'/index.txt'):
        fetch_dir(baseURL+r'browse')
        
    mode=int(raw_input(r'0(normal) 1(recent) 2(top100) 3(search):'))

    begin_time = time.time()   
    
    if mode == 0:
        header()
        n=int(raw_input(r'code: '))
        auto=int(raw_input(r'0(all) 1(single) 2(partial): '))
        if auto == 0:  
            fetch_all(n)
        elif auto==1:
            c=int(raw_input('page: '))
            x=int(raw_input('min:  '))
            y=int(raw_input('max:  '))
            fetch_all(n,c,c+1,x,y)
        elif auto == 2:
            s=int(raw_input('min:  '))
            e=int(raw_input('max:  '))
            fetch_all(n,s,e)
    elif mode==1:
        fetch_recent()
    elif mode==2:
        auto=int(raw_input(r'0(auto) 1(single): '))
        if auto == 0 :
            fetch_top()
        else:
            header()
            print '48h100 48h200 48h300 48h400 \n48h500 48h600 48hall 0 all'
            n=str(raw_input(r'code: '))
            fetch_top(n)
    elif mode==3:
        s=int(raw_input('min: '))
        e=int(raw_input('max: '))
        fetch_range(s,e)

    print 'total time: ',time.time()-begin_time,'seconds'
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    #-*_coding:utf8-*-
import requests
import re
import sys
reload(sys)
sys.setdefaultencoding(&quot;utf-8&quot;)

class spider(object):
    def __init__(self):
        print u'开始爬取内容。。。'

#getsource用来获取网页源代码
    def getsource(self,url):
        html = requests.get(url)
        return html.text

#changepage用来生产不同页数的链接
    def changepage(self,url,total_page):
        now_page = int(re.search('pageNum=(\d+)',url,re.S).group(1))
        page_group = []
        for i in range(now_page,total_page+1):
            link = re.sub('pageNum=\d+','pageNum=%s'%i,url,re.S)
            page_group.append(link)
        return page_group
#geteveryclass用来抓取每个课程块的信息
    def geteveryclass(self,source):
        everyclass = re.findall('(&lt;li deg=&quot;&quot;.*?&lt;/li&gt;)',source,re.S)
        return everyclass
#getinfo用来从每个课程块中提取出我们需要的信息
    def getinfo(self,eachclass):
        info = {}
        info['title'] = re.search('target=&quot;_blank&quot;&gt;(.*?)&lt;/a&gt;',eachclass,re.S).group(1)
        info['content'] = re.search('&lt;/h2&gt;&lt;p&gt;(.*?)&lt;/p&gt;',eachclass,re.S).group(1)
        timeandlevel = re.findall('&lt;em&gt;(.*?)&lt;/em&gt;',eachclass,re.S)
        info['classtime'] = timeandlevel[0]
        info['classlevel'] = timeandlevel[1]
        info['learnnum'] = re.search('&quot;learn-number&quot;&gt;(.*?)&lt;/em&gt;',eachclass,re.S).group(1)
        return info
#saveinfo用来保存结果到info.txt文件中
    def saveinfo(self,classinfo):
        f = open('info.txt','a')
        for each in classinfo:
            f.writelines('title:' + each['title'] + '\n')
            f.writelines('content:' + each['content'] + '\n')
            f.writelines('classtime:' + each['classtime'] + '\n')
            f.writelines('classlevel:' + each['classlevel'] + '\n')
            f.writelines('learnnum:' + each['learnnum'] +'\n\n')
        f.close()

if __name__ == '__main__':

    classinfo = []
    url = 'http://www.jikexueyuan.com/course/?pageNum=1'
    jikespider = spider()
    all_links = jikespider.changepage(url,20)
    for link in all_links:
        print u'正在处理页面：' + link
        html = jikespider.getsource(link)
        everyclass = jikespider.geteveryclass(html)
        for each in everyclass:
            info = jikespider.getinfo(each)
            classinfo.append(info)
    jikespider.saveinfo(classinfo)

#!/usr/bin/env python
# coding=utf-8
import os
import sys
import time
import urllib
import urllib2
import threading


class HttpGetThread(threading.Thread):
    def __init__(self, name, url, filename, range=0):
        threading.Thread.__init__(self)
        self.url = url
        self.filename = filename
        self.range = range
        self.totalLength = range[1] - range[0] + 1
        try:
            self.downloaded = os.path.getsize(self.filename)
        except OSError:
            self.downloaded = 0
        self.percent = self.downloaded / float(self.totalLength) * 100
        self.headerrange = (self.range[0] + self.downloaded, self.range[1])
        self.bufferSize = 8192

    def run(self):
        try:
            self.downloaded = os.path.getsize(self.filename)
        except OSError:
            self.downloaded = 0
        self.percent = self.downloaded / float(self.totalLength) * 100
        # self.headerrange = (self.range[0]+self.downloaded, self.range[1])
        self.bufferSize = 8192
        # request = urllib2.Request(self.url)
        # request.add_header('Range', 'bytes=%d-%d' %self.headerrange)
        downloadAll = False
        retries = 1
        while not downloadAll:
            if retries &gt; 10:
                break
            try:
                self.headerrange = (self.range[0] + self.downloaded, self.range[1])
                request = urllib2.Request(self.url)
                request.add_header('Range', 'bytes=%d-%d' % self.headerrange)
                conn = urllib2.urlopen(request)
                startTime = time.time()
                data = conn.read(self.bufferSize)
                while data:
                    f = open(self.filename, 'ab')
                    f.write(data)
                    f.close()
                    self.time = int(time.time() - startTime)
                    self.downloaded += len(data)
                    self.percent = self.downloaded / float(self.totalLength) * 100
                    data = conn.read(self.bufferSize)
                downloadAll = True
            except Exception, err:
                retries += 1
                time.sleep(1)
                continue


def Split(size, blocks):
    ranges = []
    blocksize = size / blocks
    for i in xrange(blocks - 1):
        ranges.append((i * blocksize, i * blocksize + blocksize - 1))
    ranges.append((blocksize * (blocks - 1), size - 1))

    return ranges


def GetHttpFileSize(url):
    length = 0
    try:
        conn = urllib.urlopen(url)
        headers = conn.info().headers
        for header in headers:
            if header.find('Length') != -1:
                length = header.split(':')[-1].strip()
                length = int(length)
    except Exception, err:
        pass

    return length


def hasLive(ts):
    for t in ts:
        if t.isAlive():
            return True
    return False


def MyHttpGet(url, output=None, connections=4):
    &quot;&quot;&quot;
    arguments:
        url, in GBK encoding
        output, default encoding, do no convertion
        connections, integer
    &quot;&quot;&quot;
    length = GetHttpFileSize(url)
    print length
    mb = length / 1024 / 1024.0
    if length == 0:
        raise URLUnreachable
    blocks = connections
    if output:
        filename = output
    else:
        output = url.split('/')[-1]
    ranges = Split(length, blocks)
    names = [&quot;%s_%d&quot; % (output, i) for i in xrange(blocks)]

    ts = []
    for i in xrange(blocks):
        t = HttpGetThread(i, url, names[i], ranges[i])
        t.setDaemon(True)
        t.start()
        ts.append(t)

    live = hasLive(ts)
    startSize = sum([t.downloaded for t in ts])
    startTime = time.time()
    etime = 0
    while live:
        try:
            etime = time.time() - startTime
            d = sum([t.downloaded for t in ts]) / float(length) * 100
            downloadedThistime = sum([t.downloaded for t in ts]) - startSize
            try:
                rate = downloadedThistime / float(etime) / 1024
            except:
                rate = 100.0
            progressStr = u'\rFilesize: %d(%.2fM) Downloaded: %.2f%% Avg rate: %.1fKB/s' % (length, mb, d, rate)
            sys.stdout.write(progressStr)
            sys.stdout.flush()
            # sys.stdout.write('\b'*(len(progressStr)+1))
            live = hasLive(ts)
            time.sleep(0.2)
        except KeyboardInterrupt:
            print
            print &quot;Exit...&quot;
            for n in names:
                try:
                    os.remove(n)
                except:
                    pass
            sys.exit(1)

    print
    # print u'used time： %d:%d, pingjunsudu：%.2fKB/s' %(int(etime)/60, int(etime)%60,rate)

    f = open(filename, 'wb')
    for n in names:
        f.write(open(n, 'rb').read())
        try:
            os.remove(n)
        except:
            pass
    f.close()


if __name__ == '__main__':
    MyHttpGet('http://ftp.ivanovo.ac.ru/updates/eset/offline_update_eav.zip', 'eav.zip', 10)
    
    
    


# -*- coding: utf-8 -*-
# Author: ToughGuy

import threading
import urllib2
import sys

max_thread = 10
# 初始化锁
lock = threading.RLock()


class Downloader(threading.Thread):
    def __init__(self, url, start_size, end_size, fobj, buffer):
        self.url = url
        self.buffer = buffer
        self.start_size = start_size
        self.end_size = end_size
        self.fobj = fobj
        threading.Thread.__init__(self)

    def run(self):
        &quot;&quot;&quot;
            马甲而已
        &quot;&quot;&quot;
        with lock:
            print 'starting: %s' % self.getName()
        self._download()

    def _download(self):
        &quot;&quot;&quot;
            我才是搬砖的
        &quot;&quot;&quot;
        req = urllib2.Request(self.url)
        # 添加HTTP Header(RANGE)设置下载数据的范围
        req.headers['Range'] = 'bytes=%s-%s' % (self.start_size, self.end_size)
        f = urllib2.urlopen(req)
        # 初始化当前线程文件对象偏移量
        offset = self.start_size
        while 1:
            block = f.read(self.buffer)
            # 当前线程数据获取完毕后则退出
            if not block:
                with lock:
                    print '%s done.' % self.getName()
                break
            # 写如数据的时候当然要锁住线程
            # 使用 with lock 替代传统的 lock.acquire().....lock.release()
            # 需要python &gt;= 2.5
            with lock:
                sys.stdout.write('%s saveing block...' % self.getName())
                # 设置文件对象偏移地址
                self.fobj.seek(offset)
                # 写入获取到的数据
                self.fobj.write(block)
                offset = offset + len(block)
                sys.stdout.write('done.\n')


def main(url, thread=3, save_file='', buffer=1024):
    # 最大线程数量不能超过max_thread
    thread = thread if thread &lt;= max_thread else max_thread
    # 获取文件的大小
    req = urllib2.urlopen(url)
    size = int(req.info().getheaders('Content-Length')[0])
    # 初始化文件对象
    fobj = open(save_file, 'wb')
    # 根据线程数量计算 每个线程负责的http Range 大小
    avg_size, pad_size = divmod(size, thread)
    plist = []
    for i in xrange(thread):
        start_size = i * avg_size
        end_size = start_size + avg_size - 1
        if i == thread - 1:
            # 最后一个线程加上pad_size
            end_size = end_size + pad_size + 1
        t = Downloader(url, start_size, end_size, fobj, buffer)
        plist.append(t)

    # 开始搬砖
    for t in plist:
        t.start()

    # 等待所有线程结束
    for t in plist:
        t.join()

    # 结束当然记得关闭文件对象
    fobj.close()
    print 'Download completed!'


if __name__ == '__main__':
    url = 'http://ftp.ivanovo.ac.ru/updates/eset/offline_update_eav.zip'
    main(url=url, thread=10, save_file='offline_update_eav.zip', buffer=4096)
    

########################################################有点问题的下载类########################################################
#!/usr/bin/env python
# coding=utf-8

import requests
import threading

class downloader:
    def __init__(self):
        self.url='http://sw.bos.baidu.com/sw-search-sp/software/e6207d37846ba/QQ_8.7.19113.0_setup.exe'
        self.num=8
        self.name=self.url.split('/')[-1]
        r = requests.head(self.url)
        self.total = int(r.headers['Content-Length'])
        print 'total is %s' % (self.total)
    def get_range(self):
        ranges=[]
        offset = int(self.total/self.num)
        for i in  range(self.num):
            if i==self.num-1:
                ranges.append((i*offset,''))   #这里的下载分段线程偏移写的出现问题
            else:
                ranges.append((i*offset,(i+1)*offset))
        return ranges
    def download(self,start,end):
        headers={'Range':'Bytes=%s-%s' % (start,end),'Accept-Encoding':'*'}
        res = requests.get(self.url,headers=headers)
        print '%s:%s download success'%(start,end)
        self.fd.seek(start)
        self.fd.write(res.content)
    def run(self):
        self.fd =  open(self.name,'w')
        thread_list = []
        n = 0
        for ran in self.get_range():
            start,end = ran
            print 'thread %d start:%s,end:%s'%(n,start,end)
            n+=1
            thread = threading.Thread(target=self.download,args=(start,end))
            thread.start()
            thread_list.append(thread)
        for i in thread_list:
            i.join()
        print 'download %s load success'%(self.name)
        self.fd.close()

if __name__=='__main__':
    down = downloader()
    down.run()
    
    
    
    
    
 ########################################################多线程下载文件修改的##################################   
# -*- coding: utf-8 -*-
# Author: ToughGuy

import threading
import urllib2
import sys

max_thread = 10
# 初始化锁
lock = threading.RLock()


class Downloader(threading.Thread):
    def __init__(self, url, start_size, end_size, fobj, buffer):
        self.url = url
        self.buffer = buffer
        self.start_size = start_size
        self.end_size = end_size
        self.fobj = fobj
        threading.Thread.__init__(self)

    def run(self):
        &quot;&quot;&quot;
            马甲而已
        &quot;&quot;&quot;

        self._download()

    def _download(self):
        &quot;&quot;&quot;
            我才是搬砖的
        &quot;&quot;&quot;
        req = urllib2.Request(self.url)
        # 添加HTTP Header(RANGE)设置下载数据的范围
        req.headers['Range'] = 'bytes=%s-%s' % (self.start_size, self.end_size)
        f = urllib2.urlopen(req)
        # 初始化当前线程文件对象偏移量
        offset = self.start_size
        while 1:
            block = f.read(self.buffer)
            # 当前线程数据获取完毕后则退出
            if not block:
                break  #未读到数据，证明下载完了
            # 写如数据的时候当然要锁住线程
            # 使用 with lock 替代传统的 lock.acquire().....lock.release()
            # 需要python &gt;= 2.5
            with lock:
                #sys.stdout.write('%s saveing block...' % self.getName())
                # 设置文件对象偏移地址
                self.fobj.seek(offset)
                # 写入获取到的数据
                self.fobj.write(block)
                offset = offset + len(block)


def main(url, thread=3, save_file='', buffer=1024):
    # 最大线程数量不能超过max_thread
    thread = thread if thread &lt;= max_thread else max_thread
    # 获取文件的大小
    req = urllib2.urlopen(url)
    size = int(req.info().getheaders('Content-Length')[0])
    # 初始化文件对象
    fobj = open(save_file, 'wb')
    # 根据线程数量计算 每个线程负责的http Range 大小
    avg_size, pad_size = divmod(size, thread)
    plist = []
    for i in xrange(thread):
        start_size = i * avg_size
        end_size = start_size + avg_size - 1
        if i == thread - 1:
            # 最后一个线程加上pad_size
            end_size = end_size + pad_size + 1
        t = Downloader(url, start_size, end_size, fobj, buffer)
        plist.append(t)

    # 开始搬砖
    for t in plist:
        t.start()

    # 等待所有线程结束
    for t in plist:
        t.join()

    # 结束当然记得关闭文件对象
    fobj.close()
    print u'地址{0}下载完毕，保存为{1}'.format(url,save_file)


if __name__ == '__main__':
    n = 1
    url =[ 'http://sw.bos.baidu.com/sw-search-sp/software/e6207d37846ba/QQ_8.7.19113.0_setup.exe','http://dldir1.qq.com/qqfile/qq/tm/2013Preview2/10913/TM2013Preview2.exe']
    for x in url:
        name = '{}.exe'.format(n)
        main(url=x, thread=10, save_file=name, buffer=4096)
        n = n + 1


</rich_text></node><node name="Get163_Music" prog_lang="python" readonly="False" tags="" unique_id="44"><rich_text>#!/usr/bin/python
#coding:utf-8

import re
import requests
import json
import urllib2
import os
import sys

minimumsize = 1

print &quot;ID: &quot; + sys.argv[1] + &quot;\n&quot;;
url = &quot;http://music.163.com/playlist?id=&quot; + sys.argv[1]
r = requests.get(url)
contents = r.text
res = r'&lt;ul class=&quot;f-hide&quot;&gt;(.*?)&lt;/ul&gt;'
mm = re.findall(res, contents, re.S | re.M)
res = r'&lt;li&gt;&lt;a .*?&gt;(.*?)&lt;/a&gt;&lt;/li&gt;'
mm = re.findall(res, contents, re.S | re.M)

for value in mm:

    url = 'http://sug.music.baidu.com/info/suggestion'
    payload = {'word': value, 'version': '2', 'from': '0'}
    print &quot;Song Name: &quot; + value

    r = requests.get(url, params=payload)
    contents = r.text
    d = json.loads(contents, encoding=&quot;utf-8&quot;)
    if('data' not in d):
    	print &quot;do not have flac\n&quot;
        continue
    if('song' not in d[&quot;data&quot;]):
    	print &quot;do not have flac\n&quot;
    	continue
    songid = d[&quot;data&quot;][&quot;song&quot;][0][&quot;songid&quot;]
    print &quot;Song ID: &quot; + songid 

    url = &quot;http://music.baidu.com/data/music/fmlink&quot;
    payload = {'songIds': songid, 'type': 'mp3'}
    r = requests.get(url, params=payload)
    contents = r.text
    d = json.loads(contents, encoding=&quot;utf-8&quot;)
    if d is not None and 'data' not in d or d['data'] == '':
        continue
    songlink = d[&quot;data&quot;][&quot;songList&quot;][0][&quot;songLink&quot;]
    if(len(songlink) &lt; 10):
        print &quot;do not have flac\n&quot;
        continue
	print &quot;Song Source: &quot; + songlink + &quot;\n&quot;;

    songdir = &quot;songs&quot;
    if not os.path.exists(songdir):
        os.makedirs(songdir)

    songname = d[&quot;data&quot;][&quot;songList&quot;][0][&quot;songName&quot;]
    artistName = d[&quot;data&quot;][&quot;songList&quot;][0][&quot;artistName&quot;]
    filename = &quot;./&quot; + songdir + &quot;/&quot; + songname + &quot;-&quot; + artistName + &quot;.flac&quot;

    f = urllib2.urlopen(songlink)
    headers = requests.head(songlink).headers
    size = int(headers['Content-Length']) / (1024 ** 2)

    if not os.path.isfile(filename) or os.path.getsize(filename) &lt; minimumsize:
        print &quot;%s is downloading now ......\n&quot; % songname
        with open(filename, &quot;wb&quot;) as code:
                code.write(f.read())
    else:
        print &quot;%s is already downloaded. Finding next song...\n\n&quot; % songname
print &quot;\n================================================================\n&quot;
print &quot;Download finish!&quot;</rich_text></node><node name="HttpFetch" prog_lang="python" readonly="False" tags="" unique_id="7"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:tee.py
# IDE:Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2016/11/13

import os,sys
import urllib2
import random
from time import time,sleep



def RandomUA(): #随机浏览器头
    UA_list=['Mozilla/5.0 (Windows NT 6.2; rv:16.0) Gecko/20100101 Firefox/16.0',\
            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11',\
            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',\
            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)',\
            'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)',\
            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)',\
            'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)',\
            'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',\
            'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16']
    UA = random.choice(UA_list)
    header = {'User-Agent':UA}
    return header

def report(blocknum, bs, size, t):
    if t == 0:
        t = 1
    if size == -1:
        print '%10s' % (str(blocknum*bs)) + ' downloaded | Speed =' + '%5.2f' % (bs/t/1024) + 'KB/s'
    else:
        percent = int(blocknum*bs*100/size)
        print '%10s' % (str(blocknum*bs)) + '/' + str(size) + 'downloaded | ' + str(percent) + '% Speed =' + '%5.2f'%(bs/t/1024) + 'KB/s'

def httpfetch(url,reporthook=report, postData=None, report=True):
    ok = False
    path = os.path.dirname(os.path.realpath(sys.argv[0])) #得到脚本执行的路径
    for _ in range(10):
        try:
            ua = RandomUA()  #伪装浏览器信息
            reqObj = urllib2.Request(url, postData, headers=ua)
            fp = urllib2.urlopen(reqObj)
            headers = fp.info()
            ok = True
            break
        except:
            sleep(1)
            continue

    if not ok:
        open(path+'/errors','a').write(url+'\n')
        return ''

    rawdata = ''
    bs = 1024*8
    size = -1
    read = 0
    blocknum = 0

    if reporthook and report:
        if &quot;content-length&quot; in headers:
            size = int(headers[&quot;Content-Length&quot;])
        reporthook(blocknum, bs, size, 1)

    t0 = time()
    while 1:
        block = ''
        try:
            block = fp.read(bs)
        except:
            open(path+'/errors','a').write(url+'\n')
            return ''
        if block == &quot;&quot;:
            break
        rawdata += block
        read += len(block)
        blocknum += 1
        if reporthook and report:
            reporthook(blocknum, bs, size, time()-t0)
        t0 = time()


    if size &gt;= 0 and read &lt; size:
        print 'down error'

    return rawdata


if __name__ == '__main__':
    url = 'https://zhidao.baidu.com/question/570528117.html'
    data = httpfetch(url)
    open('qq','w').write(data)</rich_text></node><node name="nod32_download" prog_lang="python" readonly="False" tags="" unique_id="25"><rich_text>########################################################FolderUse.py########################################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 4.5
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2017/3/26

import os
import shutil

def CleanFolder(path,bakpath):
    baseFolder= os.path.dirname(path) #父目录
    bakFolder = '%s%s'%(baseFolder,bakpath) #备份目录
    if os.path.exists(path):#如果存在目录删除目录，如果存在目录并且存在旧目录删除旧目录
        if os.path.exists(bakFolder):
            try:
                shutil.rmtree(bakFolder)
            except:
                erros = 'delete %s fail'% bakFolder
                open(baseFolder+'/errors','a').write(erros+'\n')
        shutil.copytree(path,bakFolder)
    else:
         print u'文件目录不存在，不用备份!'



def DeleteFiles(path,fileList): #删除除了列表文件外目录下的其他文件

    FullPathList = []
    DestPathList = []

    if os.path.exists(path):
        for parent,dirnames,filenames in os.walk(path):
            for x in fileList:
                DestPath = os.path.join(path,x)
                DestPathList.append(DestPath)

            for filename in filenames:
                FullPath = os.path.join(parent,filename)
                FullPathList.append(FullPath)

            for xlist in FullPathList:
                if xlist not in DestPathList:  #判断这个文件不在排除外删除
                    os.remove(xlist)
    else:
        print u'文件夹目录不存在!'
        
        
########################################################zipfile.py########################################################        
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 4.5
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2017/3/26

import zipfile
import os

class ZFile(object):
    def __init__(self, filename, mode='r', basedir=''):
        self.filename = filename
        self.mode = mode
        if self.mode in ('w', 'a'):
            self.zfile = zipfile.ZipFile(filename, self.mode, compression=zipfile.ZIP_DEFLATED)
        else:
            self.zfile = zipfile.ZipFile(filename, self.mode)
        self.basedir = basedir
        if not self.basedir:
            self.basedir = os.path.dirname(filename)

    def addfile(self, path, arcname=None):
        path = path.replace('//', '/')
        if not arcname:
            if path.startswith(self.basedir):
                arcname = path[len(self.basedir):]
            else:
                arcname = ''
        self.zfile.write(path, arcname)

    def addfiles(self, paths):
        for path in paths:
            if isinstance(path, tuple):
                self.addfile(*path)
            else:
                self.addfile(path)

    def close(self):
        self.zfile.close()
        try:
            os.remove(self.filename)   #删除待解压的文件
        except:
            pass

    def extract_to(self, path):
        for p in self.zfile.namelist():
            self.extract(p, path)

    def extract(self, filename, path):
        if not filename.endswith('/'):
            f = os.path.join(path, filename)
            dir = os.path.dirname(f)
            if not os.path.exists(dir):
                os.makedirs(dir)
            file(f, 'wb').write(self.zfile.read(filename))


        
######################################################## ThreadDownFile.py########################################################       

#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 4.5
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2017/3/26

import threading
import urllib2
import os
import time

class HttpGetThread(threading.Thread):
    def __init__(self, url, filename, range=0): #传入下载地址和保存的文件名，还有一个需要下载字节的列表
        super(HttpGetThread,self).__init__() #初始化父类
        self.url = url   #下载的地址
        self.filename = filename #保存的文件名
        self.range = range #是一个列表传入下载的起始字节和结束字节
        self.totalLength = range[1] - range[0] + 1 #需要下载的大小
        try:
            self.downloaded = os.path.getsize(self.filename)  #得到文件的大小
        except Exception:
            self.downloaded = 0 #没有就是还没开始下载
        self.percent = self.downloaded / float(self.totalLength) * 100 #下载的文件百分比
        self.headerrange = (self.range[0] + self.downloaded, self.range[1]) #从列表取出要分块文件的大小range[0]是开始rang[1]是结束
        self.bufferSize = 8192 #缓冲字节 每8192字节写入一次

    def run(self):
        &quot;&quot;&quot;
        马甲函数
        &quot;&quot;&quot;
        self.downfile()


    def downfile(self):
        &quot;&quot;&quot;
        下载类方法
        &quot;&quot;&quot;
        try:
            self.downloaded = os.path.getsize(self.filename)  #得到分块文件的大小
        except Exception:
            self.downloaded = 0 #异常则下载为0
        self.percent = self.downloaded / float(self.totalLength) * 100 #下载的百分比,totallength是要下载的分块大小
        # self.headerrange = (self.range[0]+self.downloaded, self.range[1])
        #self.bufferSize = 8192
        # request = urllib2.Request(self.url)
        # request.add_header('Range', 'bytes=%d-%d' %self.headerrange)
        downloadAll = False  #下载成功标识
        retries = 1  #重试变量
        while not downloadAll: #如果下载未成功并写重试次数大于10退出
            if retries &gt; 10:
                break
            try:
                request = urllib2.Request(self.url)  #下载地址
                request.add_header('Range', 'bytes=%d-%d' % self.headerrange)  #下载字节区间
                conn = urllib2.urlopen(request) #请求下载链接
                startTime = time.time() #取时间
                data = conn.read(self.bufferSize) #每次读缓冲字节然后写入文件
                while data:
                    f = open(self.filename, 'ab')
                    f.write(data) #写入数据
                    f.close()
                    self.time = int(time.time() - startTime)
                    self.downloaded += len(data)
                    self.percent = self.downloaded / float(self.totalLength) * 100
                    data = conn.read(self.bufferSize)
                downloadAll = True  #下载完成了
            except Exception, err: #异常重试
                retries += 1  #重试次数加1
                time.sleep(1)
                continue
                
                
######################################################## main.py########################################################                    
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 4.5
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2017/3/26

import sys
import urllib
import ThreadDownFile
import time
import os
import ZFile
import FolderUse

class DownFile:
    def __init__(self,url,path,savaName=None, connections=5):
        self.url = url    #下载的地址
        self.path = path   #解压zip的文件地址
        self.connections = connections #线程数,默认8个线程
        if savaName:
            self.savaName = savaName #下载文件保存的名字
        else:
             self.savaName = url.split('/')[-1]  #如果未传入文件名默认以下载地址的文件名保存

    def GetFileSize(self):  #返回下载文件的字节数
        filesize=0
        try:
            conn = urllib.urlopen(self.url)
            headers = conn.info().headers
            for header in headers:
                if header.find('Length') != -1:
                    length = header.split(':')[-1].strip()
                    filesize= int(length) #转换为int类型
        except Exception, err:
            pass
        return filesize

    def mergedFile(self,file,files): #合并后的文件，需求合并的列表
        f = open(file, 'wb')
        for n in files:
            f.write(open(n, 'rb').read())
            try:
                os.remove(n)
            except:
                pass
        f.close()

    def cleanfold(self,path):
        try:
            FolderUse.CleanFolder(path,'bak')  #备份path文件夹到bak文件夹
            FolderUse.DeleteFiles(path,['web.config','index.html','index.htm']) #清除目录下文件

        except:
            pass




    def extract(self,zfile,path):  #解压zip文件到位置
        z = ZFile.ZFile(zfile)
        z.extract_to(path)
        z.close()
        print u'\rextract %s to %s sucess!'% (zfile,path)


    def Split(self,size, blocks): #size是大小,blocks是分多少块
        ranges = []
        blocksize = size / blocks #eg：144786123分成10份
        for i in xrange(blocks - 1): #10份xrange(10)是从0～9 这里减少1是因为最后一块写入剩下的字节
            ranges.append((i * blocksize, i * blocksize + blocksize - 1)) #这里减1是0到14478611 其实是14478612 产生的第一个数组是[0,14478611]
        ranges.append((blocksize * (blocks - 1), size - 1)) #这是最后一块
        return ranges   #文件大小分成一个输出

    def hasLive(self,ts):
        for t in ts:
            if t.isAlive():
                return True
        return False

    def down(self):
        filesize = self.GetFileSize()
        if filesize == 0:
            print '网络发生问题！'  #出错退出
            sys.exit(1)
        mb = filesize / 1024 / 1024.0 #换算成的多少兆
        ranges = self.Split(filesize,self.connections) #按照文件大小和线程数分割成列表
        print u'\rFilesize: %d(%.2fM)' % (filesize, mb)
        names = [r&quot;%s.pat.%d&quot; % (self.savaName, i+1) for i in xrange(self.connections)]  #一个由线程和保存文件名组成的名字列表
        ts = []
        for i in xrange(self.connections): #按传入的先生数创建下载线程
            t = ThreadDownFile.HttpGetThread( self.url, names[i], ranges[i])
            t.setDaemon(True)
            t.start()
            ts.append(t)
        live = self.hasLive(ts) #检测所有的线程都是激活状态
        startSize = sum([t.downloaded for t in ts])
        startTime = time.time()
        usetime = 0
        while live:
            try:
                usetime = time.time() - startTime
                d = sum([t.downloaded for t in ts]) / float(filesize) * 100
                downloadedThistime = sum([t.downloaded for t in ts]) - startSize
                try:
                    rate = downloadedThistime / float(usetime) / 1024
                except:
                    rate = 100.0
                progressStr = u'\rDownloaded: %.2f%% Avg rate: %.1fKB/s'%(d, rate)
                sys.stdout.write(progressStr)
                sys.stdout.flush()
                # sys.stdout.write('\b'*(len(progressStr)+1))
                live = self.hasLive(ts)
                time.sleep(0.2)
            except KeyboardInterrupt:
                print
                print &quot;Exit...&quot;
                for n in names:
                    try:
                        os.remove(n)
                    except:
                        pass
                sys.exit(1)

        print u'\rused time:%d:%d, Avg:%.2fKB/s' %(int(usetime)/60, int(usetime)%60,rate)
        self.mergedFile(self.savaName,names)  #合并下载的分片文件
        time.sleep(0.5)#休息0.5秒
        self.cleanfold(self.path) #备份文件夹到bak,删除文件夹下文件
        time.sleep(0.5)#休息0.5秒
        self.extract(self.savaName,self.path) #解压到指定目录







if __name__ == '__main__':
    try:
        t = DownFile('http://ftp.ivanovo.ac.ru/updates/eset/offline_update_eav.zip','e:\\eva',connections=10)
        t.down()
    except:
        sys.exit(1)


















</rich_text></node><node name="wwwyoujizzcom" prog_lang="python" readonly="False" tags="" unique_id="10"><rich_text>########################################################DBhelper.py########################################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 4.5
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2017/3/31



def dbcreate(conn):
	c = conn.cursor()
	c.execute('''create table wwwyoujizzcom(
		id integer,
		title text,
		images text,
		movie text)''')
	conn.commit()
	c.close()

def dbinsert(id,title,images,movie,conn):
	c = conn.cursor()
	c.execute('insert into wwwyoujizzcom values(?,?,?,?)',\
		(id,title,images,movie))
	conn.commit()
	c.close()


def dbupdate(id,title,images,movie,conn):
	c = conn.cursor()
	c.execute('update wwwyoujizzcom set id=?,title=?,images=?,movie=? where id=?',\
		(id,title,images,movie,id))
	conn.commit()


def dbfind(id,conn):
    c = conn.cursor()
    c.execute('select 1 from wwwyoujizzcom where id=?',(id,))
    for x in c:
        if 1 in x:
            return True
        else:
            return False



def dblist(conn):
	c = conn.cursor()
	c.execute('select * from wwwyoujizzcom')
	for x in c:
		for y in x:
			print y



########################################################Download.py########################################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 4.5
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2017/3/30


import os,sys
import urllib2
import random
from time import time,sleep
import re



def RandomUA(): #随机浏览器头
    UA_list=['Mozilla/5.0 (Windows NT 6.2; rv:16.0) Gecko/20100101 Firefox/16.0',\
            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11',\
            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',\
            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)',\
            'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)',\
            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)',\
            'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)',\
            'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',\
            'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16']
    UA = random.choice(UA_list)
    header = {'User-Agent':UA}
    return header

def report(blocknum, bs, size, t):
    if t == 0:
        t = 1
    if size == -1:
        print '%10s' % (str(blocknum*bs)) + ' downloaded | Speed =' + '%5.2f' % (bs/t/1024) + 'KB/s'
    else:
        percent = int(blocknum*bs*100/size)
        print '%10s' % (str(blocknum*bs)) + '/' + str(size) + 'downloaded | ' + str(percent) + '% Speed =' + '%5.2f'%(bs/t/1024) + 'KB/s'

def httpfetch(url,reporthook=report, postData=None, report=True):
    ok = False
    path = os.path.dirname(os.path.realpath(sys.argv[0])) #得到脚本执行的路径
    for _ in range(10):
        try:
            ua = RandomUA()  #伪装浏览器信息
            reqObj = urllib2.Request(url, postData, headers=ua)
            fp = urllib2.urlopen(reqObj)
            headers = fp.info()
            ok = True
            break
        except:
            sleep(1)
            continue

    if not ok:
        open(path+'/errors','a').write(url+'\n')
        return ''

    rawdata = ''
    bs = 1024*8
    size = -1
    read = 0
    blocknum = 0

    if reporthook and report:
        if &quot;content-length&quot; in headers:
            size = int(headers[&quot;Content-Length&quot;])
        reporthook(blocknum, bs, size, 1)

    t0 = time()
    while 1:
        block = ''
        try:
            block = fp.read(bs)
        except:
            open(path+'/errors','a').write(url+'\n')
            return ''
        if block == &quot;&quot;:
            break
        rawdata += block
        read += len(block)
        blocknum += 1
        if reporthook and report:
            reporthook(blocknum, bs, size, time()-t0)
        t0 = time()


    if size &gt;= 0 and read &lt; size:
        print 'down error'

    return rawdata


########################################################main.py########################################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 4.5
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2017/3/31
import download
import sqlite3
import re
import Queue
import os
import sys
import threading
import DBhelper
import time

def fetchall(url,pages=[]):
    urlbase =url.split('/videos')[0] #取url前面的网址为何链接拼成完整的地址
    print 'fetching from',url,'...'
    res = download.httpfetch(url)
    pattern_next = re.compile(r'link rel=&quot;next&quot; href=&quot;(/videos\?p=\d+)&quot;')
    nextpage = pattern_next.findall(res)
    if nextpage:
        pages.append(url)
        nexturl= urlbase + nextpage[0]  #补全url地址
        fetchall(nexturl)   #递归
    else:
        pages.append(url) #最后一个
    return pages

def fetchids(url,queue):
    urlbase =url.split('/videos')[0]
    print 'fetching from',url,'...'
    res = download.httpfetch(url)
    pattern_ids = re.compile(r'/videos/(\d+)')
    _ids = pattern_ids.findall(res)
    if pattern_ids:
        ids = set(_ids)
        for id in ids:
            queue.put(id)




def fetch(id,conn):
    print 'fetching id',id,'...'
    urlbase = 'http://www.wwwyoujizzcom.info/videos/'
    url = urlbase + str(id)
    res = ''
    for _ in range(3):
        try:
            print url
            res = download.httpfetch(url)
            break
        except:
            continue
    title = re.compile(r'&lt;title&gt;(.*?)&lt;/title&gt;',re.DOTALL).findall(res)
    resource = re.compile(r'{&quot;360p&quot;:&quot;(.*?)&quot;',re.DOTALL).findall(res)
    images = re.compile(r'&lt;a href=&quot;#&quot;&gt;&lt;img src=&quot;(.*?)&quot; alt=&quot;&quot;&gt;&lt;/a&gt;',re.DOTALL).findall(res)
    if title:
        title = title[0]
    else:
        title = ''
    if images:
            imagestr = '#'.join(images)
    else:
        imagestr = ''
    if resource:
        movie=resource[0]
    else:
        movie= ''

    if not DBhelper.dbfind(id,conn):
            DBhelper.dbinsert(id,title,imagestr,movie,conn)
            time.sleep(0.5)
    else:
            pass


def thread_fetch(queue,conn):
    # path = os.path.dirname(os.path.realpath(sys.argv[0]))
    # conn = sqlite3.connect(path+'/verycd.sqlite3.db')
    # conn.text_factory = str
    while True:
        topic = queue.get()
        fetch(topic,conn)
        queue.task_done()


if __name__ == '__main__':
    stime=time.time()
    path = os.path.dirname(os.path.realpath(sys.argv[0]))
    conn = sqlite3.connect(path+'/db.db',check_same_thread=False)
    conn.text_factory = str
    DBhelper.dbcreate(conn)
    q = Queue.Queue()
    for i in range(10):
        t = threading.Thread(target=thread_fetch,args=(q,conn))
        t.setDaemon(True)
        t.start()
    p = fetchall('http://www.wwwyoujizzcom.info/videos?p=1')
    for x in p:
        fetchids(x,q)
    q.join()
    estime=time.time()
    print u'总共花费%s'%(stime-estime)


########################################################image_get.py########################################################

#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 4.5
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2017/3/31
import download
import sqlite3
import re
import Queue
import os
import sys
import threading
import DBhelper
import time
import urllib2
def fetchall(url,pages=[]):
    urlbase =url.split('/videos')[0] #取url前面的网址为何链接拼成完整的地址
    print 'fetching from',url,'...'
    res = download.httpfetch(url)
    pattern_next = re.compile(r'link rel=&quot;next&quot; href=&quot;(/videos\?p=\d+)&quot;')
    nextpage = pattern_next.findall(res)
    if nextpage:
        pages.append(url)
        nexturl= urlbase + nextpage[0]  #补全url地址
        fetchall(nexturl)   #递归
    else:
        pages.append(url) #最后一个
    return pages

def fetchids(url,queue):
    urlbase =url.split('/videos')[0]
    print 'fetching from',url,'...'
    res = download.httpfetch(url)
    pattern_ids = re.compile(r'/videos/(\d+)')
    _ids = pattern_ids.findall(res)
    if pattern_ids:
        ids = set(_ids)
        for id in ids:
            queue.put(id)




def fetch(id,conn):
    print 'fetching id',id,'...'
    urlbase = 'http://www.wwwyoujizzcom.info/videos/'
    url = urlbase + str(id)
    res = ''
    for _ in range(3):
        try:
            print url
            res = download.httpfetch(url)
            break
        except:
            continue
    title = re.compile(r'&lt;title&gt;(.*?)&lt;/title&gt;',re.DOTALL).findall(res)
    resource = re.compile(r'{&quot;360p&quot;:&quot;(.*?)&quot;',re.DOTALL).findall(res)
    images = re.compile(r'&lt;a href=&quot;#&quot;&gt;&lt;img src=&quot;(.*?)&quot; alt=&quot;&quot;&gt;&lt;/a&gt;',re.DOTALL).findall(res)
    if title:
        title = title[0]
    else:
        title = ''
    if images:
            imagestr = '#'.join(images)
    else:
        imagestr = ''
    if resource:
        movie=resource[0]
    else:
        movie= ''

    if not DBhelper.dbfind(id,conn):
            DBhelper.dbinsert(id,title,imagestr,movie,conn)
            time.sleep(0.5)
    else:
            pass


def thread_fetch(queue,conn):
    # path = os.path.dirname(os.path.realpath(sys.argv[0]))
    # conn = sqlite3.connect(path+'/verycd.sqlite3.db')
    # conn.text_factory = str
    while True:
        topic = queue.get()
        fetch(topic,conn)
        queue.task_done()
		
		
def thread_fetch_image(queue):
    # path = os.path.dirname(os.path.realpath(sys.argv[0]))
    # conn = sqlite3.connect(path+'/verycd.sqlite3.db')
    # conn.text_factory = str
    while True:
        topic = queue.get()
        fetch_image(topic)
        queue.task_done()

def fetch_image(id):
    if not os.path.exists(id):
        os.mkdir(id)
    print 'fetching id',id,'...'
    urlbase = 'http://www.wwwyoujizzcom.info/videos/'
    url = urlbase + str(id)
    res = ''
    for _ in range(3):
        try:
            print url
            res = download.httpfetch(url)
            break
        except:
            continue
    resource = re.compile(r'{&quot;360p&quot;:&quot;(.*?)&quot;',re.DOTALL).findall(res)
    images = re.compile(r'&lt;a href=&quot;#&quot;&gt;&lt;img src=&quot;(.*?)&quot; alt=&quot;&quot;&gt;&lt;/a&gt;',re.DOTALL).findall(res)

    if images:
        for image in images:
            name = os.path.join(id,image.split('/')[-1])
            if os.path.exists(name): #存在下过了
                print u'%s存在略过不下载'
                continue
            else:
                downfile(image,name) #不存在下载
			    
            
    # if resource:
    #     movie=resource[0]
    #     moviename=os.path.join(id,id)
    #     downfile(movie,moviename)





def downfile(url,savename):
    ua = download.RandomUA()
    reqObj = urllib2.Request(url, headers=ua)
    data = urllib2.urlopen(reqObj).read()
    with open(savename, &quot;wb&quot;) as code:
        code.write(data)


if __name__ == '__main__':
    q = Queue.Queue()
    for i in range(10):
        t = threading.Thread(target=thread_fetch_image,args=(q,))
        t.setDaemon(True)
        t.start()
    p = fetchall('http://www.wwwyoujizzcom.info/videos?p=1')
    for x in p:
        fetchids(x,q)
    q.join()
    estime=time.time()
 







</rich_text></node><node name="find_movie" prog_lang="python" readonly="False" tags="" unique_id="20"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:compress_movie.py
# IDE:Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2017/5/22
'''
传入path是一个列表，代表需要扫描的路径,type是扫描类型,size是一个列表，代表扫描的文件在列表2个区间内
1024*1024代表1M,默认扫描5M~5g的视频
7z压缩并上传到ftp 然后删除临时压缩文件
'''
import os
import subprocess
import ftpclient
import time

class find_compress_movie(object):

    def __init__(self,path=[],_type=[],size=[100*1024*1024,5*1024*1024*1024]):
        self.paths = path
        self._type = _type
        self.size_s = int(size[0])
        self.size_l = int(size[1])

    def get_movie(self,files=[]):
        for path in self.paths:
            if os.path.isdir(path):
                for root, dirs, names in os.walk(path):
                    for name in names:
                        if os.path.splitext(name)[1].lower() in self._type:
                            if self.size_s &lt; os.path.getsize(os.path.join(root,name)) &lt;self.size_l :
                                #print os.path.join(root, name)
                                files.append(os.path.join(root, name))
        return files

    def upload_file(self,localname,remotename):
        host = '192.168.0.3'
        user = 'admin'
        password= '78619808'
        remote_dir = '/NAS/mycloud'
        Ftp = ftpclient.FtpClient(host,user,password,remote_dir)
        Ftp.login()
        Ftp.upload_file(localname,remotename)

    def compress(self,outpath,password):
        F = self.get_movie()
        if not os.path.exists(outpath):
            os.mkdir(outpath)
        try:
            for file in F:
                if os.path.isfile(file):
                    t = time.strftime('%y%m%d%H%M%S',time.localtime(time.time()))
                    name = t + '.7z'
                    filename = os.path.join(outpath,name)
                    fd = subprocess.Popen(&quot;7za a -p%s %s %s -mx=9 -m0=LZMA2 -mmt=on&quot; %(password,filename,file),stdout=subprocess.PIPE)
                    print fd.stdout.read()
                    time.sleep(0.5)
                    for _ in range(3):
                        try:
                            self.upload_file(filename,name)
                            time.sleep(0.5)
                            os.remove(filename)
                            break
                        except:
                            time.sleep(1)
                            continue
        except:
            print 'error'


if __name__ == '__main__':
    files = find_compress_movie(['e:\\','f:\\','g:\\'],['.mp4','.avi','.wmv','.mkv','.vob','.ts'],[200*1024*1024,5*1024*1024*1024])
    files.compress('e:\\tmp','kylinshine_78619808')
    
    
    
##############################################################################################################################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:ftpclient.py
# IDE:Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# last edit 2017/5/22
'''
FTP使用类
'''

from ftplib import FTP
import os
import socket

class FtpClient:
    def __init__(self, host, user, passwd, remotedir, port=21):
        self.hostaddr = host
        self.username = user
        self.password = passwd
        self.remotedir  = remotedir
        self.port     = port
        self.ftp      = FTP()
        self.file_list = []

    def __del__(self):
        self.ftp.close()

    def login(self):
        ftp = self.ftp
        try:
            timeout = 60
            socket.setdefaulttimeout(timeout)
            ftp.set_pasv(True)
            ftp.connect(self.hostaddr, self.port)
            print 'Connect Success %s' %(self.hostaddr)
            ftp.login(self.username, self.password)
            print 'Login Success %s' %(self.hostaddr)
            print ftp.getwelcome()
        except Exception:
           print &quot;Connect Error or Login Error&quot;
        try:
            ftp.cwd(self.remotedir)
            print 'Change Directory sucess'
        except(Exception):
           print 'Change Directory Error'

    def is_same_size(self, localfile, remotefile):
        try:
            remotefile_size = self.ftp.size(remotefile)
        except:
            remotefile_size = -1
        try:
            localfile_size = os.path.getsize(localfile)
        except:
            localfile_size = -1
        print 'lo:%d  re:%d' %(localfile_size, remotefile_size)
        if remotefile_size == localfile_size:
            return 1
        else:
            return 0

    def download_file(self, localfile, remotefile):
        if self.is_same_size(localfile, remotefile):
            return
        else:
            pass
        file_handler = open(localfile, 'wb')
        self.ftp.retrbinary('RETR %s'%(remotefile), file_handler.write)
        file_handler.close()

    def download_files(self, localdir='./', remotedir='./'):
        try:
            self.ftp.cwd(remotedir)
        except:
            return
        if not os.path.isdir(localdir):
            os.makedirs(localdir)
        self.file_list = []
        self.ftp.dir(self.get_file_list)
        remotenames = self.file_list
        for item in remotenames:
            filetype = item[0]
            filename = item[1]
            local = os.path.join(localdir, filename)
            if filetype == 'd':
                self.download_files(local, filename)
            elif filetype == '-':
                self.download_file(local, filename)
        self.ftp.cwd('..')

    def upload_file(self, localfile, remotefile):
        if not os.path.isfile(localfile):
            return
        if self.is_same_size(localfile, remotefile):
            return
        try:
            file_handler = open(localfile, 'rb')
            self.ftp.storbinary('STOR %s' %remotefile, file_handler)
            file_handler.close()
            print 'upload sucess'
        except:
            print 'upload error'


    def upload_files(self, localdir='./', remotedir = './'):
        if not os.path.isdir(localdir):
            return
        localnames = os.listdir(localdir)
        self.ftp.cwd(remotedir)
        for item in localnames:
            src = os.path.join(localdir, item)
            if os.path.isdir(src):
                try:
                    self.ftp.mkd(item)
                except:
                    print 'Directory Exists %s' %item
                self.upload_files(src, item)
            else:
                self.upload_file(src, item)
        self.ftp.cwd('..')
    def mkdir(self, remotedir='./'):
        try:
            self.ftp.mkd(remotedir)
        except:
            print 'Directory Exists %s' %remotedir
    def get_file_list(self, line):
        ret_arr = []
        file_arr = self.get_filename(line)
        if file_arr[1] not in ['.', '..']:
            self.file_list.append(file_arr)
    def get_filename(self, line):
        pos = line.rfind(':')
        while(line[pos] != ' '):
            pos += 1
        while(line[pos] == ' '):
            pos += 1
        file_arr = [line[0], line[pos:]]
        return file_arr


if __name__ == '__main__':
    Ftp = FtpClient('192.168.0.3','admin','78619808','/NAS/mycloud')
    Ftp.login()
    Ftp.upload_file('d:\\debian-8.8.0-amd64-DVD-1.iso','debian-8.8.0.iso')
    
################################################################################################################
import os
import shutil
'''
找到电影并且移动到指定目录
'''
def find_movie(path,movie_type,files=[]):
    if os.path.isdir(path):
                for root, dirs, names in os.walk(path):
                    for name in names:
                        if os.path.splitext(name)[1].lower() in movie_type:
                            if 20*1024*1024&lt; os.path.getsize(os.path.join(root,name)) &lt; 1024*1024*1024*8 :
                                print root,name
                                print os.path.join(root, name)
                                files.append(os.path.join(root, name))
    return files




if __name__ == '__main__':
    f = find_movie('g:\\',['.mp4','.avi','.wmv','.mkv','.vob','.ts','.rm','.rmvb','.mpg'])

    for x in f:
        newfile = os.path.join('g:\\movie',os.path.basename(x))
        shutil.move(x,newfile)
</rich_text></node><node name="ReblogTumblr" prog_lang="python" readonly="False" tags="" unique_id="52"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/2/27
import pymysql
import pytumblr
import time

class ReblogTumblrBlog():

    def __init__(self,blogname,delay):   #初始化发布的blog名字,以及延时，多久发布一个文章
        self.blogname = blogname
        self.delay = int(delay)

    def setMysql(self,host, user, passwd, db,sql,port=3306,charset='utf8',limit=3000):  #设置mysql连接信息,以及sql语句,limit暂时没什么用
        self.host = host
        self.user = user
        self.passwd = passwd
        self.db = db
        self.limit= limit
        self.port = port
        self.sql = sql
        self.charset = charset

    def getCursor(self):   #获取游标
        try:
            conn = pymysql.connect(host=self.host,port=self.port,user=self.user,passwd=self.passwd,db=self.db,charset=self.charset)
            cursor = conn.cursor()
        except:
            print (&quot;connect mysql error&quot;)
        return cursor

    def getData(self):       #获取数据
        try:
            cursor = self.getCursor()
            cursor.execute(self.sql)
            data = cursor.fetchall()
        except:
            print (&quot;getData error&quot;)
            return None
        return data

    def setTumblrOauth(self,consumer_key,consumer_secret,oauth_token,oauth_secret):  #设置tumblr的api信息
        self.consumer_key = consumer_key
        self.consumer_secret = consumer_secret
        self.oauth_token = oauth_token
        self.oauth_secret = oauth_secret


    def getTumblrApid(self):  #获得客户端
        try:
            client = pytumblr.TumblrRestClient(self.consumer_key ,self.consumer_secret,self.oauth_token,self.oauth_secret)
        except:
            print (&quot;Get tumblr api error&quot;)
            return None
        return client


    def Display(self):   #测试用,获取blog信息
        client = self.getTumblrApid()
        if client:
            print (client.info())
        else:
            print (&quot;Can't Get Tumblr info&quot;)

    def reBlog(self,comment):   #转发文章
        client = self.getTumblrApid()
        data = self.getData()
        print (data)
        if client and data:
            for id,reblogkey in data:
                posts = client.reblog(blogname=self.blogname,id=id,reblog_key=reblogkey,comment=comment)
                print (posts,self.blogname,id)
                time.sleep(self.delay)
        print (&quot;reblog complete!&quot;)






if __name__==&quot;__main__&quot;:
    blog = ReblogTumblrBlog('godbells',30)
    blog.setMysql('127.0.0.1','root','78619808','tumblr','select id,reblogkey from reblog LIMIT 3000')
    blog.setTumblrOauth('9VhS4ytWYGNFnyYSItPKdZNxxlDkTh8hDI6IEBxCGmZCTuvrad',
                         'nWu7pm8HjNlpn0VPzNkcZKfLB15Y4VInUfrrBT5gpY5oj2q7Th',
                         'lgnKPjMutSSTun3Tw6h9HsBRr02Rqw0Qj2uwlInlCqxYv5RsGx',
                         'vdpYKS1s5epZNXmvhGcUUIfOL21U0ITrcE1jEppjimqA4N7MLT')
    blog.reBlog('&lt;/p&gt;还在为翻墙找不着梯子烦恼吗，还在为梯子速度不佳看视频卡顿烦恼吗，老哥梯子服务超过200人，速度快质量佳,买梯子就买老哥梯子！&lt;/font&gt;&lt;/font&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;&lt;a href=&quot;http://www.lgssr.org&quot;&gt;&lt;font style=&quot;vertical-align: inherit;&quot;&gt;点我注册&lt;/font&gt;&lt;/font&gt;')













</rich_text></node><node name="qt" prog_lang="python" readonly="False" tags="" unique_id="55"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/3/15
import sys
from PyQt5 import QtCore, QtWidgets
import requests
import base64
import qrcode

class HelloPyQt(QtWidgets.QWidget):
    def __init__(self, parent = None):
        super(HelloPyQt, self).__init__(parent)  #调用父类的初始化函数
        self.setWindowTitle('老哥梯子 www.lgssr.org')
        self.setFixedSize(400, 300)
        vbox = QtWidgets.QVBoxLayout()
        hbox = QtWidgets.QHBoxLayout()
        self.inputbox = QtWidgets.QLineEdit()
        vbox.addWidget(self.inputbox)
        self.tranbtn = QtWidgets.QPushButton('获取节点')
        self.erweima = QtWidgets.QPushButton('二维码')
        self.outputbox = QtWidgets.QTextEdit()
        hbox.addWidget(self.tranbtn)
        hbox.addWidget(self.erweima)
        vbox.addLayout(hbox)
        self.outputbox = QtWidgets.QTextEdit()
        vbox.addWidget(self.outputbox)
        self.setLayout(vbox)
        self.inputbox.setText(&quot;http://www.lgssr.org/link/siHDpZ1QIHPRDb2M?mu=0&quot;)
        self.outputbox.setText('\t\t说    明\n把订阅地址填上点击获取\n读取完ssr地址右键select all(全选),右键copy\n运行小飞机右键剪切版批量倒入ssr://链接')
        self.tranbtn.clicked.connect(self.btnPress_Clicked)
        self.erweima.clicked.connect(self.btnerweima_Clicked)


        # # self.setWindowTitle(&quot;PyQt Test&quot;)  #设置标题
        # self.setFixedSize(400,300)    #设置窗口大小
        # self.textHello = QtWidgets.QTextEdit(&quot;This is a test program written in python with PyQt lib!&quot;) #添加一个文本框
        # self.textworld = QtWidgets.QTextEdit(&quot;This is second text!&quot;)
        # self.btnPress = QtWidgets.QPushButton(u&quot;按我&quot;) #添加一个按钮
        # self.btnAbout = QtWidgets.QPushButton(u&quot;关于&quot;) #添加一个按钮
        # v_layout = QtWidgets.QVBoxLayout()  #添加一个垂直布局
        # h_layout = QtWidgets.QHBoxLayout()  #添加一个水平布局
        # v_layout.addWidget(self.textHello) #把文本框放进垂直布局
        # v_layout.addWidget(self.textworld) #把按钮放进垂直布局
        # h_layout.addWidget(self.btnPress) #把按钮放进水平布局
        # h_layout.addWidget(self.btnAbout) #把按钮放进水平布局
        # v_layout.addLayout(h_layout) #把水平布局放进垂直布局
        # self.setLayout(v_layout) #利用布局来水平布局
        # self.connect(self.btnPress,QtCore.SIGNAL('clicked()'),self.btnPress_Clicked) #信号连接槽函数，挡在按钮上按时触发后面的函数
        # self.connect(self.btnAbout,QtCore.SIGNAL('clicked()'),self.btnAbout_Clicked) #信号连接槽函数，挡在按钮上按时触发后面的函数

    def btnPress_Clicked(self):
        url = self.inputbox.text().strip()
        if not url:
            QtWidgets.QMessageBox.warning(self, 'warning', '请输入订阅地址')
            return
        try:
            html = requests.get(url=url,timeout=5).text
            html += &quot;=&quot; * ((4 - len(html) % 4) % 4)
            ssrs = base64.b64decode(html).decode('utf-8')
            self.outputbox.setText(ssrs)
        except:
            self.outputbox.setText('出错')

        # self.textHello.setText(&quot;Hello PyQt!\nThe button has been pressed.&quot;)
        # self.textworld.setText(&quot;Hello World!\nThe text is change.&quot;)

    def btnerweima_Clicked(self):
        url = self.inputbox.text().strip()
        if not url:
            QtWidgets.QMessageBox.warning(self, 'warning', '请输入订阅地址')
            return
        try:
            html = requests.get(url=url, timeout=5).text
            html += &quot;=&quot; * ((4 - len(html) % 4) % 4)
            ssrs = base64.b64decode(html).decode('utf-8')
            self.outputbox.setText(ssrs)
        except:
            self.outputbox.setText('出错')

if __name__=='__main__':
    app = QtWidgets.QApplication(sys.argv) #初始化一个app
    mainWindow = HelloPyQt() #初始化类
    mainWindow.show() #显示框口
    sys.exit(app.exec_()) #进入消息循环</rich_text></node><node name="getSsr" prog_lang="python" readonly="False" tags="" unique_id="56"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2018/5/25
# ssr下载程序
# zbarimg是zbar二维码解码文件
import requests
import re
import os
import subprocess


class GetSsr(object):
    def __init__(self, url, folder):
        self.url = url
        self.folder = folder
        self.downSsrImg()

    def parseallqrcode(self):
        filelist = os.listdir(self.folder)
        if os.path.exists('ssr.txt'):
            os.remove('ssr.txt')
        if filelist:
            for filename in filelist:
                cmd = [&quot;bin/zbarimg.exe&quot;, &quot;-D&quot;, self.folder + filename]
                output = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()[0].rstrip()
                ssr = output[8:].decode('utf-8')
                # ssrlist.append(ssr)
                os.remove(self.folder + filename)
                with open('ssr.txt', 'a+') as f:
                    f.write(ssr + '\n')  # 加\n换行显示

    def downSsrImg(self):
        imgs = self.getPicUrl()
        for img in imgs:
            pic = self.url + img
            for _ in range(3):  # 重试下载次数为3次
                try:
                    self.downPic(url=pic, folder=self.folder)
                    break
                except:
                    continue

    def getPicUrl(self):
        html = self.urlOpen(self.url).text
        imgPic = re.findall('&lt;h4&gt;&lt;a href=&quot;(.*?)&quot;', html)
        if imgPic:  # 判断列表不为空
            return imgPic
        else:
            return None

    def downPic(self, url, folder):
        req = self.urlOpen(url)
        if not os.path.exists(folder):  # 判断文件夹是否已经存在
            os.makedirs(folder)  # 创建文件夹
        imgName = folder + url.split('/')[-1]  # 下载保存的目录
        if os.path.exists(imgName):  # 判断是否存在图片
            os.remove(imgName)  # 存在则删除
        if req:  # 图片有效则下载
            with open(imgName, 'wb') as file:
                file.write(req.content)

    def urlOpen(self, url):
        headers = {
            'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'}
        try:
            req = requests.get(url, headers=headers)
            if req.status_code == 200:  # 这里判断url有效
                return req
        except:
            return None


if __name__ == '__main__':
    ssr = GetSsr('https://my.ishadowx.net/', './imgs/')
    ssr.parseallqrcode()
</rich_text></node><node name="getJsqpro" prog_lang="python" readonly="False" tags="" unique_id="57"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2018/5/26
import requests
import base64
import time

class getJsqpro(object):
    def __init__(self,url):
        self.url = url
        self.getSsr()

    def httpfetch(self,url):
        headers = {
            'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'}
        ok = False  # 这是一个标志，表示网页访问成功
        for _ in range(5):  # 循环10次，访问网页如果出错,记录在error这个文件里
            try:
                reqObj = requests.get(url, headers=headers)  # 访问url,postdata是post参数是一个字典,headers是网页头部文件
                html = reqObj.text
                ok = True  # 访问成功标志
                break  # 循环退出
            except:
                time.sleep(1)  # 如果访问失败则休息1秒
                continue  # 继续循环

        if not ok:  # 如果循环10次都失败则写入错误url到error文件
            open('errors', 'a').write(url + '\n')
            return ''
        return html

    def getSsr(self):
        req = self.httpfetch(self.url)
        text = self.deByBase64(req)
        ssrs = text.split('\n')[:-1]
        for ssr in ssrs:
            acc = ssr[6:]
            ssrAcc = self.deByBase64(acc)
            print (ssrAcc)



    def deByBase64(self,base64Text):
        if isinstance(base64Text, str):
            bytes_utf_8 = base64Text.encode(encoding=&quot;utf-8&quot;) #变成二进制
        if isinstance(base64Text, bytes):
            bytes_utf_8 = base64Text  # 二进制就不改了
        missing_padding = 4 - len(bytes_utf_8) % 4
        if missing_padding:
            bytes_utf_8 += b'=' * missing_padding
        text = base64.b64decode(bytes_utf_8).decode(encoding=&quot;utf-8&quot;) #返回值变成str
        return text









if __name__ == '__main__':
    acc = getJsqpro('https://jsqpro.com/link/7G9kqlFEgOIp5dok?mu=0')
</rich_text></node><node name="autoSsr" prog_lang="python" readonly="False" tags="" unique_id="58"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2018/5/27

from bs4 import BeautifulSoup
import json
import requests
import time
import re
import base64
import os
import subprocess


class AutoGetSsr(object):

    def __init__(self, url):
        self.url = url
        self.main()

    def main(self):
        req = self.httpFetch(self.url, {
            'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'})
        items = self.getSSr(req)
        self.witeSsrConfig(items)
        time.sleep(1)
        self.killSsr()
        time.sleep(1)
        self.startSSr()

    def killSsr(self):
        cmd = 'taskkill /f /im ShadowsocksR-dotnet4.0.exe'
        output = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()[0]
        cmdline = output.decode('gbk')
        print(cmdline)

    def startSSr(self):
        cmd = 'start ShadowsocksR-dotnet4.0.exe'
        os.system(cmd)

    def httpFetch(self, url, headers={}):
        ok = False  # 这是一个标志，表示网页访问成功
        for _ in range(3):  # 循环3次，访问网页如果出错,记录在error这个文件里
            try:
                reqObj = requests.get(url, headers=headers)  # 访问url,postdata是post参数是一个字典,headers是网页头部文件
                html = reqObj.text
                ok = True  # 访问成功标志
                break  # 循环退出
            except:
                time.sleep(1)  # 如果访问失败则休息1秒
                continue  # 继续循环

        if not ok:  # 如果循环10次都失败则写入错误url到error文件
            open('errors', 'a').write(url + '\n')
            return ''
        return html

    def getSSr(self, html):
        soup = BeautifulSoup(html, &quot;html.parser&quot;)
        ssrs = soup.find_all(&quot;div&quot;, class_=&quot;portfolio-item&quot;)
        ssrList = []
        if ssrs:
            for ssr in ssrs:
                ssrServer = ssr.find(id=re.compile(&quot;ip\w+&quot;)).string.lstrip()
                ssrPort = ssr.find(id=re.compile(&quot;port\w+&quot;)).get_text(&quot;&quot;, strip=True)
                ssrPass = ssr.find(id=re.compile(&quot;pw\w+&quot;)).get_text(&quot;&quot;, strip=True)
                methodStr = ssr.find(text=re.compile(&quot;Method&quot;)).string.lstrip()  # 带有Method:,需要去掉
                ssrMethod = methodStr.split(':')[-1]
                ObfsStr = ssr.find(text=re.compile(&quot;auth&quot;))
                if ObfsStr:
                    ssrObfs = ObfsStr.split(' ')[-1]
                else:
                    ssrObfs = 'plain'
                remarksBase = base64.b64encode(ssrServer.encode('utf-8'))
                remarksBase64 = remarksBase.decode('utf-8').replace('=', '')  # 去掉base64加密后的=
                acc = {
                    &quot;remarks&quot;: ssrServer,  # 备注
                    &quot;id&quot;: &quot;200BBA5D829A352903E74571BEE3AB0C&quot;,
                    &quot;server&quot;: ssrServer,
                    &quot;server_port&quot;: ssrPort,
                    # &quot;server_udp_port&quot;: 0,
                    &quot;password&quot;: ssrPass,
                    &quot;method&quot;: ssrMethod,
                    &quot;protocol&quot;: &quot;origin&quot;,
                    &quot;protocolparam&quot;: &quot;&quot;,
                    &quot;obfs&quot;: ssrObfs,
                    &quot;obfsparam&quot;: &quot;&quot;,
                    &quot;remarks_base64&quot;: remarksBase64,  # 备注base64
                    &quot;group&quot;: '',  # 组
                    &quot;enable&quot;: True,
                    &quot;udp_over_tcp&quot;: False
                }
                ssrList.append(acc)
            return ssrList  # 返回一个ssr列表
        else:
            return None

    def readJson(self):
        with open('config', mode='r+', encoding='utf-8') as f:
            data = json.load(f)
        return data

    def witeSsrConfig(self, ssrList):
        with open('config', mode='r+', encoding='utf-8') as f:
            data = json.load(f)
        ssrConfig = data['configs']
        ssrConfig.extend(ssrList)
        if os.path.exists(&quot;gui-config.json&quot;):
            os.remove(&quot;gui-config.json&quot;)
        with open(&quot;gui-config.json&quot;, mode=&quot;w+&quot;) as f:
            json.dump(data, f)


if __name__ == &quot;__main__&quot;:
    a = AutoGetSsr('https://my.ishadowx.net/')
</rich_text></node><node name="utils" prog_lang="python" readonly="False" tags="" unique_id="59"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/5/28
import socket
import time

'''
通过域名获取ip
'''


def getIp(url, debug=False):
    ok = False
    for _ in range(5):
        try:
            ip = socket.gethostbyname(url)
            ok = True
            break
        except Exception as e:
            if debug:  # debug模式打印出错内容
                print(e)
            time.sleep(1)  # 如果访问失败则休息1秒
            continue  # 继续循环
    if not ok:
        return None
    return ip


if __name__ == &quot;__main__&quot;:
    ip = getIp('www.google.com', debug=True)
    if ip:
        print(ip)
        

#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/5/28
import requests
import time
import random

'''
get和post访问网页,参数列表有随机浏览器头,代理,提交数据,二进制获取数据,debug调试模式
'''


def httpGet(url, headers={}, proxies={}, payload={}, binary=False,
            debug=False):  # debug是调试模式,binary是否返回二进制数据,params是提交内容
    ok = False  # 这是一个标志，表示网页访问成功
    for _ in range(5):  # 循环5次
        try:
            req = requests.get(url, headers=headers, proxies=proxies, params=payload)  # 访问url
            if binary:  # 返回二进制数据
                reqobj = req.content
            else:  # 返回文本数据
                reqobj = req.text
            ok = True  # 访问成功标志
            break  # 循环退出
        except Exception as e:
            time.sleep(1)  # 如果访问失败则休息1秒
            if debug:
                print(e)
            continue  # 继续循环

    if not ok:  # 访问失败返回空
        return None
    if debug:
        print('status:{}'.format(req.status_code))
        print('headers:{}'.format(req.headers))
        print(reqobj)
    return reqobj


def httpPost(url, headers={}, proxies={}, payload={}, binary=False, debug=False):  # debug是调试模式,binary是否返回二进制数据
    ok = False  # 这是一个标志，表示网页访问成功
    for _ in range(5):  # 循环5次
        try:
            req = requests.post(url, headers=headers, proxies=proxies,
                                data=payload)  # 访问url,postdata是post参数是一个字典,headers是网页头部文件
            if binary:  # 返回二进制数据
                reqobj = req.content
            else:  # 返回文本数据
                reqobj = req.text
            ok = True  # 访问成功标志
            break  # 循环退出
        except Exception as e:
            time.sleep(1)  # 如果访问失败则休息1秒
            if debug:
                print(e)
            continue  # 继续循环

    if not ok:  # 如果循环10次都失败则写入错误url到error文件
        return None
    if debug:
        print('status:{}'.format(req.status_code))
        print('headers:{}'.format(req.headers))
        print(reqobj)
    return reqobj


def RandomUA():  # 随机浏览器头
    UA_list = ['Mozilla/5.0 (Windows NT 6.2; rv:16.0) Gecko/20100101 Firefox/16.0', \
               'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11', \
               'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER', \
               'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)', \
               'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)', \
               'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)', \
               'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)', \
               'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0', \
               'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16']
    UA = random.choice(UA_list)
    header = {'User-Agent': UA}
    return header


if __name__ == &quot;__main__&quot;:
    payload = {'key1': 'value1', 'key2': 'value2'}  # 提交数据
    proxies = {&quot;http&quot;: &quot;http://127.0.0.1:1080&quot;, &quot;https&quot;: &quot;http://127.0.0.1:1080&quot;}  # 代理
    httpGet('http://httpbin.org/get', headers=RandomUA(), debug=True)  # 正常访问不用代理
    httpGet('http://httpbin.org/get', headers=RandomUA(), proxies=proxies, debug=True)  # 用代理访问
    httpGet('http://httpbin.org/get', headers=RandomUA(), proxies=proxies, payload=payload, debug=True)  # 用代理提交参数
    httpPost(&quot;http://httpbin.org/post&quot;, headers=RandomUA(), payload=payload, debug=True)  # 正常访问提交参数
    httpPost(&quot;http://httpbin.org/post&quot;, headers=RandomUA(), proxies=proxies, payload=payload, debug=True)  # 代理访问提交参数
    
    
    
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/5/29
import base64

'''
解密密文,缺失=号的自动补全再解密,返回str字符串
加密文本,可选择是否压缩=号,返回str字符串
'''


def decrypt(strText):
    if isinstance(strText, str):
        text = strText.encode()  # 默认就是utf-8
    elif isinstance(strText, bytes):
        text = strText  # 二进制不用编码了
    else:
        print('type error')
    missing_padding = len(text) % 4  # 这里判断是否要补全=号
    if missing_padding != 0:
        text += b'=' * (4 - missing_padding)  # 补全=号
    try:
        deStr = base64.urlsafe_b64decode(text).decode()  # 返回uft-8
    except Exception as e:
        print(e)
        return None
    return deStr  # 返回解密字符串


def encrypt(strText, compress=True):  # compress是压缩=号
    if isinstance(strText, str):
        text = strText.encode()  # 默认就是utf-8
    elif isinstance(strText, bytes):
        text = strText  # 二进制不用编码了
    else:
        print('type error')
    try:
        compressStr = base64.urlsafe_b64encode(text).decode()  # 返回uft-8
    except Exception as e:
        print(e)
        return None
    if compress:  #默认是压缩
        enstr = compressStr.split('=')[0]
    else:
        enstr = compressStr
    return enstr


if __name__ == &quot;__main__&quot;:
    print(decrypt('5Y-y6Imz5paH'))
    print(encrypt('史艳文'))

#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/5/29
from utils.DecryptBase64 import encrypt

'''
根据ssr配置生成base64加密字符串
encrypt加密函数就是上面自定义的encrypt
'''


def SsrStr(serverip, port, password, protocol, method, obfs, remarks='', group='', obfsparam='', protoparam=''):
    sPassword = encrypt(password)
    sRemarks = encrypt(remarks)
    sGroup = encrypt(group)
    sObfsparam = encrypt(obfsparam)
    sProtoparam = encrypt(protoparam)
    ssr = '{}:{}:{}:{}:{}:{}/?obfsparam={}&amp;protoparam={}&amp;remarks={}&amp;group={}' \
        .format(serverip, port, protocol, method, obfs, sPassword, sObfsparam, sProtoparam, sRemarks, sGroup) #拼接字符串
    ssrStr = 'ssr://' + encrypt(ssr, True)
    return ssrStr


if __name__ == &quot;__main__&quot;:
    ssr = SsrStr('103.15.216.133', '443', 'hero78619808', 'auth_aes128_sha1', 'aes-256-ctr', 'tls1.2_ticket_auth','史艳文')
    print (ssr)
    
import os
'''
传入文件路径和文件类型,过滤文件的大小默认是从1M到10G过滤
'''


def FindFiles(dirPath=[],fileType=[],fileSize=[1024*1024,1024*1024*1024*10]):
    files = []
    fileSizeMin = int(fileSize[0])
    fileSizeMax = int(fileSize[1])
    for path in dirPath:
        if os.path.isdir(path):
            for root, dirs, names in os.walk(path):
                for name in names:
                    if os.path.splitext(name)[1].lower() in fileType:
                        if fileSizeMin &lt;= os.path.getsize(os.path.join(root, name)) &lt;= fileSizeMax:
                            files.append(os.path.join(root, name))
    return files

if __name__ == '__main__':
    f = FindFiles(['a:/','d:/','e:/'],['.mp4', '.avi', '.wmv', '.mkv', '.vob', '.ts', '.rm', '.rmvb', '.mpg'])
    print(f)
    
import hashlib
import os
'''
计算文件的md5
'''
def GetFileMd5(filePath):
    if os.path.isfile(filePath):
        f = open(filePath, 'rb')
        hashObj = hashlib.md5()
        while True:
            d = f.read(4096)
            if not d:
                break
            hashObj.update(d)
        hashCode = hashObj.hexdigest()
        f.close()
        fileMd5 = str(hashCode).lower()
        return fileMd5

if __name__ == '__main__':
    fileMd5 = GetFileMd5('d:/fc2ppv_676186.mp4')
    print (fileMd5)
    
import subprocess
'''
python调用wget下载文件,成功返回0,失败返回-1,传入的proxy代理字符串如果下载链接是http那么http_proxy=http://127.0.0.1:1090
如果下载链接是https那么应该是https_proxy=http://127.0.0.1:1090
'''
def DownLoadFile(url,proxy=None):
    if proxy:
        cmd = 'wget -t 3 --no-check-certificate -c -e &quot;{}&quot; {}'.format(proxy, url)
    else:
        cmd = 'wget -t 3 --no-check-certificate -c {}'.format(url)
    p = subprocess.Popen(cmd, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    while p.poll() is None:
        line = p.stdout.readline()
        line = line.strip()
        if line:
            print('正在下载:{}'.format(line))
    if p.returncode == 0:
        return 0
    else:
        return -1


if __name__ == '__main__':
    a = DownLoadFile('http://mirrors.aliyun.com/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1804.iso','http_proxy=http://127.0.0.1:1090')
    print (a)

#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2018
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/8/31
import os
'''
传入文件路径和文件类型,过滤文件的大小默认是从1M到10G过滤
'''
class FindFiles():
    def __init__(self,path=[],type=[],fileSize=[1024*1024,1024*1024*1024*10]):
        self.path = path  #需要查询的路径列表
        self.type = type  #查询的
        self.fileSizeMin = fileSize[0]
        self.fileSizeMax = fileSize[1]

    def setType(self,str):
        if str == 'video':
            self.type = ['.mp4','.rmvb','.rm','.mpg','.avi','.mpg','.mkv','.vob','.3gp']
        if str == 'photo':
            


    def GetFile(self):
        files = []
        for path in self.path:
            if os.path.isdir(path):
                for root, dirs, names in os.walk(path):
                    for name in names:
                        if os.path.splitext(name)[1].lower() in self.type:
                            if self.fileSizeMin &lt;= os.path.getsize(os.path.join(root, name)) &lt;= self.fileSizeMax:
                                fileName = os.path.join(root, name)
                                files.append(fileName)
        return files

    def GetFileSize(self):
        files = []
        for path in self.path:
            if os.path.isdir(path):
                for root, dirs, names in os.walk(path):
                    for name in names:
                        if os.path.splitext(name)[1].lower() in self.type:
                            if self.fileSizeMin &lt;= os.path.getsize(os.path.join(root, name)) &lt;= self.fileSizeMax:
                                fileName = os.path.join(root, name)
                                fileSize = os.path.getsize(fileName)
                                files.append((fileName,fileSize))
        return files






if __name__ == '__main__':
    f = FindFiles(['d:/'],['.mp4'])
    n = f.GetFileSize()
    print (n)

</rich_text></node><node name="SsrList" prog_lang="python" readonly="False" tags="" unique_id="60"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm 2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin on 2018/5/27

from bs4 import BeautifulSoup
import re
from utils.Fetch import httpGet, RandomUA
from utils.GetIp import getIp
from utils.EncryptSsr import SsrStr


class AutoGetSsr(object):

    def __init__(self, url):
        self.url = url
        self.ssrDo()

    def ssrDo(self):
        ssrs = self.getSsr()
        ssrText = '\n'.join(ssrs)
        with open('ssrs.txt', 'w+') as f:
            f.write(ssrText)

    def getSsr(self):
        html = httpGet(self.url, headers=RandomUA())
        soup = BeautifulSoup(html, &quot;html.parser&quot;)
        ssrs = soup.find_all(&quot;div&quot;, class_=&quot;portfolio-item&quot;)
        ssrList = []
        if ssrs:
            for ssr in ssrs:
                ssrServer = ssr.find(id=re.compile(&quot;ip\w+&quot;)).string.lstrip()
                serverip = getIp(ssrServer)
                port = ssr.find(id=re.compile(&quot;port\w+&quot;)).get_text(&quot;&quot;, strip=True)
                password = ssr.find(id=re.compile(&quot;pw\w+&quot;)).get_text(&quot;&quot;, strip=True)
                _method = ssr.find(text=re.compile(&quot;Method&quot;)).string.lstrip()  # 带有Method:,需要去掉
                method = _method.split(':')[-1]
                _obfs = ssr.find(text=re.compile(&quot;auth&quot;))
                protocol = 'origin'  # 没有这个默认origin
                if _obfs:
                    obfs = _obfs.split(' ')[-1]
                else:
                    obfs = 'plain'

                acc = SsrStr(serverip, port, password, protocol, method, obfs)
                ssrList.append(acc)
            return ssrList  # 返回一个ssr列表
        else:
            return None


if __name__ == &quot;__main__&quot;:
    a = AutoGetSsr('https://my.ishadowx.net/')
</rich_text></node><node name="ThreadDown" prog_lang="python" readonly="False" tags="" unique_id="63"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2017
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/7/11
import requests
import threading

class downloader:
    def __init__(self):
        self.url = &quot;http://ftp.ivanovo.ac.ru/updates/eset/offline_update_ess.zip&quot;
        self.num = 5
        self.name = &quot;offline_update_ess.zip&quot;
        r = requests.head(self.url)
        # 获取文件大小
        self.total = int(r.headers['Content-Length'])
        print (self.total)

    # 获取每个线程下载的区间
    def get_range(self):
        ranges = []
        offset = int(self.total/self.num)
        for i in range(self.num):
            if i == self.num-1:
                ranges.append((i*offset,''))
            else:
                ranges.append((i*offset,(i+1)*offset))
        return ranges  # [(0,100),(100,200),(200,&quot;&quot;)]

    # 通过传入开始和结束位置来下载文件
    def download(self,start,end):
        headers = {'Range':'Bytes=%s-%s'%(start,end),'Accept-Encoding':'*'}
        res = requests.get(self.url,headers=headers)
        # 将文件指针移动到传入区间开始的位置
        self.fd.seek(start)
        self.fd.write(res.content)
        print(&quot;%s-%s write success&quot; % (start, end))

    def run(self):
        self.fd = open(self.name,&quot;wb&quot;)
        thread_list = []
        for ran in self.get_range():
            # 获取每个线程下载的数据块
            start,end = ran
            thread = threading.Thread(target=self.download,args=(start,end))
            thread.start()
            thread_list.append(thread)
        for i in thread_list:
            # 设置等待，避免上一个数据块还没写入，下一数据块对文件seek，会报错
            i.join()
        self.fd.close()

if __name__ == &quot;__main__&quot;:
    downloader().run()
</rich_text></node><node name="demo" prog_lang="python" readonly="False" tags="" unique_id="64"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2018
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/8/31
import threading
import os
import time
import queue

class FindAV(threading.Thread):
    def __init__(self,Q,dirPath=[],fileType=[]):
        super().__init__()
        self.q = q
        self.dirPath=dirPath
        self.fileType=fileType

    def run(self):
        self.FindFiles()

    def FindFiles(self,fileSize=[1024 * 1024, 1024 * 1024 * 1024 * 10]):
        fileSizeMin = int(fileSize[0])
        fileSizeMax = int(fileSize[1])
        for path in self.dirPath:
            if os.path.isdir(path):
                for root, dirs, names in os.walk(path):
                    for name in names:
                        if os.path.splitext(name)[1].lower() in self.fileType:
                            if fileSizeMin &lt;= os.path.getsize(os.path.join(root, name)) &lt;= fileSizeMax:
                                filename =os.path.join(root, name)
                                print ('我是生产者线程,线程名是{}正在往里塞数据,现在队列长度是{},我塞的数据是{}'.format(self.getName(),self.q.qsize(),filename))
                                self.q.put(filename)


class GetAV(threading.Thread):
    def __init__(self, q):
        super().__init__()
        self.q = q
    def run(self):
        while True:
            data = self.q.get()
            print('我是消费者线程,线程名是{},现在队列长度是{},我读到数据是{}'.format(self.getName(),self.q.qsize(), data))
            time.sleep(20)
            self.q.task_done()

if __name__ == '__main__':
    q =  queue.Queue(10)
    data = FindAV(q,['D:/','e:/','F:/','G:/'],['.mp4', '.avi', '.wmv', '.mkv', '.vob', '.ts', '.rm', '.rmvb', '.mpg'])
    data.start()
    for i in range(8):
        t = GetAV(q)
        t.setDaemon(True)
        t.start()
    q.join()





</rich_text></node><node name="SyncToMysql" prog_lang="python" readonly="False" tags="" unique_id="65"><rich_text>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Program Name:test.py
# IDE:Pycharm2018
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808
# Created by Kylin at 2018/9/5
from utils.SqliteHelper import DBDriver
from utils.MysqlHelper import DBHelper
import os
from utils.config import *
'''
同步本地sqlite的数据到mysql数据库,config里是mysql数据库的配置
查询本地sqlite数据用到的是生成器,遍历生成器便可得到数据,这样比查出数据返回列表，遍历列表要快
查询本地sqlite的字段要与占位符,以及和mysql的写入语句字段 占位符相匹配(可以一样可以不一样 但要匹配）
'''

class SyncData():
    def __init__(self,dbPath):
        self.mysqlDB = DBHelper(MYSQL_HOST, MYSQL_USER, MYSQL_PASSWD, MYSQL_DBNAME)
        self.dbPath = dbPath   # 本地sqlite文件所在
        self.sqliteDB = self.GetDb() #如果存在文件,那么返回sqlite的连接对象 进一步操作数据

    def setSqliteScript(self,sqlscript,limit,offset=0): #sql是语句用select * from test limit ?,?,limit是一次读多少条记录,offset是偏移量
        self.sqliteScript = sqlscript
        self.limit = int(limit)
        self.offset = int(offset)

    def setMysqlScript(self, sqlscript):  #写入字段的个数要和查询字段个数匹配,占位符也要和字段个数匹配
        self.mysqlScript = sqlscript

    def GetDb(self):
        if os.path.isfile(self.dbPath):  # 判断下sqlite的文件是否存在,存在打开
            sqliteDB = DBDriver(self.dbPath)
            return sqliteDB
        else:
            return None


    def GetDbData(self): #生成器,遍历得到数据,每次查询limt条数据,然后偏移limit后再查询，通过循环可以查询到所有数据
        if self.sqliteDB:
            while True:
                sqlParams = (self.offset,self.limit)
                data = self.sqliteDB.DbQuery(self.sqliteScript,sqlParams)
                if data:
                    self.offset += self.limit
                    yield data
                else:
                    break

    def SyncToMysql(self):  #这里是同步到mysql,得到生成器遍历生成器,得到列表,遍历列表写入数据,通过外循环完生成器写完所有数据
        localdatas = self.GetDbData()
        for localdata in localdatas:
            if localdata:
                for data in localdata:
                    self.mysqlDB.DbExec(self.mysqlScript, data)
                    print('写入数据{}成功'.format(data[0]))


if __name__ == '__main__':
    a = SyncData('../db/tumblr.db')
    a.setSqliteScript('select name from tumblrBlog limit ?,?',5)
    a.setMysqlScript('replace into TumblrBlog (name) values (%s)')
    a.SyncToMysql()




</rich_text></node></node></node><node name="soft" prog_lang="custom-colors" readonly="False" tags="" unique_id="26"><rich_text background="#ffffff" foreground="#222222">文件编辑</rich_text><rich_text>: Notepad++
python语言:Anaconda2
</rich_text><rich_text background="#ffffff" foreground="#222222">文件查找：everything</rich_text><rich_text>
</rich_text><rich_text background="#ffffff" foreground="#222222">视频播放：PotPlayer</rich_text><rich_text>
</rich_text><rich_text background="#ffffff" foreground="#222222">网页浏览：Chrome</rich_text><rich_text>
</rich_text><rich_text background="#ffffff" foreground="#222222">翻GFW：shadowsocks</rich_text><rich_text>
下载工具:迅雷 旋风
数据库:sqlserver 2008 mysql
即时通讯:qq
FTP客户端:FileZilla
python IDE:JetBrains PyCharm
压缩软件:winrar
ghost一键恢复：老毛桃一键还原
杀毒软件:eset nod32 推荐4.2版本
java ide:intellij idea </rich_text><rich_text background="#ffffff" foreground="#222222">Eclipse </rich_text><rich_text>MyEclipse
</rich_text><rich_text background="#ffffff" foreground="#222222">垃圾清理</rich_text><rich_text>:CCleaner
办公软件:</rich_text><rich_text background="#ffffff" foreground="#222222">Office 2013</rich_text><rich_text>
局域网通讯:</rich_text><rich_text background="#ffffff" foreground="#222222">IPMsg</rich_text><rich_text>
笔记软件:CherryTree
远程软件:Anydesk</rich_text></node><node name="Linux" prog_lang="sh" readonly="False" tags="Linux" unique_id="8"><rich_text>
</rich_text><node name="Centos" prog_lang="custom-colors" readonly="False" tags="Centos" unique_id="9"><rich_text scale="h2">• </rich_text><rich_text scale="h2" weight="heavy">Centos7 学习之静态IP设置</rich_text><rich_text>
(sudo) </rich_text><rich_text background="#ffffff" foreground="#000000">vi /etc/sysconfig/network-scripts/ifcfg-eth0  </rich_text><rich_text>  #后面的网卡名字可能有所变化，虚拟机vbox就是ifcfg-eng0s3
修改下来内容,vi操作i是insert
</rich_text><rich_text foreground="#000000">BOOTPROTO=static #dhcp改为static   </rich_text><rich_text>
</rich_text><rich_text foreground="#000000">ONBOOT=yes #开机启用本配置  </rich_text><rich_text>
</rich_text><rich_text foreground="#000000">IPADDR=192.168.0.11 #静态IP  </rich_text><rich_text>
</rich_text><rich_text foreground="#000000">GATEWAY=192.168.0.240 #默认网关  </rich_text><rich_text>
</rich_text><rich_text foreground="#000000">NETMASK=255.255.255.0 #子网掩码  </rich_text><rich_text>
</rich_text><rich_text foreground="#000000">DNS1=114.114.114.114 #DNS1 配置  </rich_text><rich_text>
</rich_text><rich_text foreground="#000000">DNS2=8.8.8.8 #DNS2 配置  </rich_text><rich_text>
</rich_text><rich_text background="#ffffff" foreground="#555555">重启下网络服务</rich_text><rich_text>
</rich_text><rich_text background="#ffffff" foreground="#000000">service network restart </rich_text><rich_text>
检验网卡配置
</rich_text><rich_text background="#ffffff" foreground="#000000">ip addr  </rich_text><rich_text>
检验网络是否通畅
ping www.baidu.com -c 4 #ping4次百度,参数-c 是count的意思

</rich_text><rich_text foreground="#000000000000" scale="h1" weight="heavy">yum命令使用实例</rich_text><rich_text>
yum clean all  #清理缓存
yum makecache  #重新生成缓存
yum -y update #更新系统
yum -y install epel-release #安装并启用EPEL源
yum -y install wget  #安装wget
yum -y install screen #安装screen
yum clean all &amp;&amp; yum makecache  &amp;&amp; yum -y update &amp;&amp; yum -y install wget screen #依次更新系统和安装wget screen
uname -a    #查看当前内核,再重启后删除当年内核
rpm -qa | grep kernel  #重启后再查看
yum -y remove kernel-3.10.0-957.el7.x86_64   #卸载旧内核(重启后才能删除)
yum -y install vsftpd #安装ftp服务

</rich_text><rich_text scale="h1" weight="heavy">备份恢复centos系统</rich_text><rich_text>
mkdir /home/bak
tar cvpzf /home/bak/centos7_20190214_install.tgz / --exclude=/proc --exclude=/lost+found --exclude=/home --exclude=/mnt --exclude=/sys  
tar cvpjf /home/bak/install.tgz.bz2 / --exclude=/proc --exclude=/lost+found --exclude=/home --exclude=/mnt --exclude=/sys  #Bzip来压缩你的备份
tar cvpzf /home/bak/centos7_20190214_install.tgz / --exclude=/proc --exclude=/lost+found --exclude=/home --exclude=/mnt --exclude=/sys
tar xvpfz centos7_20190214_install.tgz -C /
restorecon -Rv / #不执行这个无法登陆系统
mkdir proc
mkdir lost+found
mkdir mnt
mkdir sys

</rich_text><rich_text scale="h1" weight="heavy">passwd</rich_text><rich_text foreground="#000000000000" scale="h1" weight="heavy">命令使用实例</rich_text><rich_text>
passwd root

</rich_text><rich_text scale="h1" weight="heavy">centos下安装python3</rich_text><rich_text>
yum -y install wget #安装wget
yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel gcc #安装依赖包
yum -y install libffi-devel #python3.7需要libffi-deve
mkdir /usr/local/python3  #建立python3目录
cd /home #切换到home目录
wget https://www.python.org/ftp/python/3.7.2/Python-3.7.2.tgz #下载python3.7
tar -xzvf Python-3.7.2.tgz #解压
rm -rf Python-3.7.2.tgz #删除压缩包
mv Python-3.7.2 Python3 #重命名文件夹
cd Python3#进入目录
./configure --prefix=/usr/local/python3 #修改配置
make &amp;&amp; make install #编译安装
make clean #如果编译失败清理下
ln -s /usr/local/python3/bin/python3 /usr/bin/python3 #建立python3 软链接
ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 #建立pip3 软链接
cd /home &amp;&amp; rm -rf Python3 #删除Python3目录
tar cvpzf /home/bak/centos7_20190215_py3.tgz / --exclude=/proc --exclude=/lost+found --exclude=/home --exclude=/mnt --exclude=/sys  #备份下python3安装后的环境

</rich_text><rich_text scale="h1" weight="heavy">python3建立虚拟环境</rich_text><rich_text>
pip3 install virtualenv
</rich_text><rich_text foreground="#0000ff">ln</rich_text><rich_text> -s /usr/local/python3/bin/virtualenv /usr/bin/</rich_text><rich_text foreground="#000000">virtualenv</rich_text><rich_text>  #建立软链接
virtualenv ENV
source ./bin/activate
pip install requests pymysql scrapy
deactivate
</rich_text><rich_text scale="h1" weight="heavy">centos下安装nginx和mysql</rich_text><rich_text>
</rich_text><rich_text foreground="#000000">yum </rich_text><rich_text>-y install bzip2 #mysql要用到bzip2
wget -c </rich_text><rich_text link="webs http://soft.vpser.net/lnmp/lnmp1.5.tar.gz">http://soft.vpser.net/lnmp/lnmp1.5.tar.gz</rich_text><rich_text> &amp;&amp; tar zxf lnmp1.5.tar.gz &amp;&amp; cd lnmp1.5 #用lnmp包编译安装nginx和mysql
./install.sh nginx 
./install.sh db
chkconfig --list  
chkconfig --level 345 mysql on #可以自己更改
netstat -na | grep 3306 #查看端口,需要安装net-tools

</rich_text><rich_text scale="h1" weight="heavy">centos下查看服务启动列表</rich_text><rich_text>
</rich_text><rich_text background="#eeeeee" foreground="#000000">systemctl list-unit-files</rich_text><rich_text>
</rich_text><rich_text background="#eeeeee" foreground="#000000">systemctl list-unit-files | grep enable</rich_text><rich_text> #查询自动启动
</rich_text><rich_text background="#eeeeee" foreground="#000000">systemctl list-unit-files | grep zabbix</rich_text><rich_text> #过滤查询服务名

</rich_text><rich_text scale="h1" weight="heavy">centos下mysql相关操作</rich_text><rich_text>
mysql -uroot -p  #用密码登录
use mysql ;#切换数据库
update user set host = '%' where user = 'root'; #最好不要这样做,应该是建立一个数据库而重新建立一个普通用户
update user set password=password('hero-mysql@78619808!@#41717670') where user='root'; #修改密码
flush privileges; #刷新权限
DROP USER 'hero'@'%';  #删除用户
flush privileges;  
create database tumblr;#创建数据库
GRANT ALL PRIVILEGES ON tumblr.* TO 'hero'@'localhost' IDENTIFIED BY 'mysql!@#_hero@4171767078619808' WITH GRANT OPTION; #创建一个普通用户，权限只是本地可以连接
GRANT ALL PRIVILEGES ON tumblr.* TO 'hero'@'%' IDENTIFIED BY '41717670' WITH GRANT OPTION; #%是都可以连接
flush privileges;



</rich_text><rich_text scale="h1" weight="heavy">centos下安装phantomjs</rich_text><rich_text>
yum -y install wget install bzip2.x86_64 </rich_text><rich_text foreground="#000000">fontconfig</rich_text><rich_text> # 安装下载工具和解压工具bzip2,</rich_text><rich_text foreground="#000000">fontconfig</rich_text><rich_text>是phantomjs需求环境
wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 # 下载PhantomJS,不成功的话windows下下载用sftp上传
tar -xjf phantomjs-2.1.1-linux-i686.tar.bz2 #解压文件
mv /home/hero/tmp/phantomjs-2.1.1-linux-x86_64 /usr/local/phantomjs # 剪切重命名
ln -s /usr/local/phantomjs/bin/phantomjs /usr/bin/ # 建立软链接
vi test.js #创建javascript脚本
console.log('hello world!'); #写入测试内容
phantom.exit(); #退出phantom
phantomjs test.js #测试phantomjs 

</rich_text><rich_text scale="h1" weight="heavy">centos下pyspider爬虫</rich_text><rich_text>
pyspider all #  启动所有pyspider组件 包括phantomjs，需要root权限
</rich_text><rich_text background="#ffffff" foreground="#262626">nohup </rich_text><rich_text>pyspider all &amp; #后台启动pyspider关闭终端也可以运行


</rich_text><rich_text scale="h1" weight="heavy">centos下防火墙添加端口</rich_text><rich_text>
firewall-cmd --permanent --zone=public --add-service=http  #添加http服务(对应端口80)
firewall-cmd --permanent --zone=public --add-service=https #添加htts服务(对应端口443)
firewall-cmd --permanent --zone=public --add-port=5000/tcp #添加指定端口5000
firewall-cmd --permanent --zone=public --remove-port=5000/tcp #删除添加的端口
firewall-cmd --reload　#重新加载防火墙
firewall-cmd --state    #显示状态
firewall-cmd --list-services  #查看当前开了服务的端口,如果以端口形式添加的不显示,一个服务对应一个端口,每个服务对应/usr/lib/firewalld/services下面一个xml文件
firewall-cmd --get-services #查看还有哪些服务可以打开
firewall-cmd --list-ports #查看所有开放的端口,这里只是看端口,如果是开放的服务不在这里显示
firewall-cmd --zone=public --list-ports #查看pulic区域开放的端口
vi /etc/firewalld/zones/public.xml #这个是防火墙的配置文件




</rich_text><rich_text scale="h1" weight="heavy">centos下dd备份系统</rich_text><rich_text>
1. 磁盘克隆
也就是把整个硬盘复制一份。当然你首先需要在计算机上在接上一块新硬盘，并让系统识别。例如这块硬盘可能被识别为/dev/sdb,原有硬盘叫/dev/sda. 然后你可以在linux命令行上简单地执行：
dd if=/dev/sda of=/dev/sdb
对就这么简单，此命令完成后，你的第二块硬盘上将有一个和第一块硬盘一模一样的副本，也是可以启动的。因为dd操作就是简单的按字节复制，什么分区表啊，MBR啊统统照搬。
当然你也可以做一些微调，来定制一下克隆操作。例如你可能希望把硬盘上的内容全部备份到一个磁盘文件中，而不是另一块新硬盘，你可以
dd if=/dev/sda of=~/disk1.img
之后，disk1.img就是备份好的磁盘映像文件，你可以复制或转移到其他介质也可以压缩一下：
gzip disk1.img #generates disk1.img.gz
或者在创建磁盘映像文件的同时就执行压缩：
dd if=/dev/sda1 | gzip &gt; boot_efi.img.gz
dd if=/dev/sda | bzip2 &gt; disk.img.bz2
不错吧！ghost的功能都有了。
2.分区克隆
如果我们不想克隆整块磁盘，只想把某些存放数据的分区备份一下，你会需要克隆一个分区。正如预备知识所述，分区在linux下也是一个文件。例如
dd if=/dev/sda1 of=~/part1.img
将把第一块磁盘的第一个分区备份到名为part1.img的文件中。
也可以进行分区复制：
dd if=/dev/hda1 of=/dev/hda3
将把分区一原样复制到分区3.
3. 从镜像文件恢复磁盘或分区
很简单，把上面命令中，of和if指向的文件名互换一下即可实现恢复。例如
dd if=disk1.img of=/dev/sda
把保存在disk1.img中的映像恢复到第一块磁盘
dd if=part1.img of=/dev/hda2
把保存在part1.img中的映像恢复到第一块硬盘的第二个分区
最后，需要注意一点是，无论备份还是恢复，都需要保证 of 指向的文件或磁盘空间要大于 if 指向的内容，因为是按字节复制 如果没有足够的空间去存储内容，备份自然会失败。


</rich_text></node><node name="bash" prog_lang="sh" readonly="False" tags="" unique_id="50"><rich_text>#!/bin/bash

#关闭防火墙
Shut_down_iptables(){
	apt-get -y install iptables iptables-services
	iptables -F;iptables -X
	iptables -I INPUT -p tcp -m tcp --dport 22:65535 -j ACCEPT
	iptables -I INPUT -p udp -m udp --dport 22:65535 -j ACCEPT
	iptables-save &gt; /etc/sysconfig/iptables
	echo 'iptables-restore /etc/sysconfig/iptables' &gt;&gt; /etc/rc.local
}
#连接限制
Unfile_number_limit(){
	echo &quot;root soft nofile 65535
root hard nofile 65535&quot; &gt;&gt; /etc/security/limits.conf
	echo &quot;session required pam_limits.so&quot; &gt;&gt; /etc/pam.d/login
}
#增加缓冲器
Add_swap_partition(){
	Memory_size=`cat /proc/meminfo | grep MemTotal | grep -E -o &quot;[1-9][0-9]{4,}&quot;`
	Swap_size=`expr ${Memory_size} \* 2`
	
	dd if=/dev/zero of=/var/swap bs=1024 count=${Swap_size}
	mkswap /var/swap;swapon /var/swap;free -m
	echo '/var/swap swap swap default 0 0' &gt;&gt; /etc/fstab
}

#设置ssr前端信息
Setting_node_information(){
	clear;echo &quot;设定服务端信息:&quot;
	read -p &quot;(1/3)前端地址:&quot; Front_end_address
		if [[ ${Front_end_address} = '' ]];then
			Front_end_address=`curl -s &quot;https://myip.ipip.net&quot; | grep -E -o &quot;([0-9]{1,3}[\.]){3}[0-9]{1,3}&quot;`
			echo &quot;emm,我们已将前端地址设置为:http://${Front_end_address}&quot;
		fi
	read -p &quot;(2/3)节点ID:&quot; Node_ID
	read -p &quot;(3/3)Mukey:&quot; Mukey
	if [[ ${Mukey} = '' ]];then
		Mukey='mupass';echo &quot;未设置该项,默认Mukey值为:mupass&quot;
	fi
	echo;echo &quot;Great！即将开始安装...&quot;;echo;sleep 2.5
}
install_node_for_debian(){
	pip install cymysql requests -i https://pypi.org/simple/
	wget -O /usr/bin/shadowsocks &quot;https://raw.githubusercontent.com/qinghuas/ss-panel-and-ss-py-mu/master/node/ss&quot;;chmod 777 /usr/bin/shadowsocks
	git clone -b manyuser https://github.com/glzjin/shadowsocks.git &quot;/root/shadowsocks&quot;
	cd shadowsocks;chmod +x *.sh;pip install -r requirements.txt -i https://pypi.org/simple/
	cp apiconfig.py userapiconfig.py;cp config.json user-config.json
	
	sed -i &quot;17c WEBAPI_URL = \'${Front_end_address}\'&quot; /root/shadowsocks/userapiconfig.py
	sed -i &quot;2c NODE_ID = ${Node_ID}&quot; /root/shadowsocks/userapiconfig.py
	sed -i &quot;18c WEBAPI_TOKEN = \'${Mukey}\'&quot; /root/shadowsocks/userapiconfig.py
}

Edit_ss_node_info(){
	echo &quot;旧设置如下:&quot;
	sed -n '2p' /root/shadowsocks/userapiconfig.py
	sed -n '17,18p' /root/shadowsocks/userapiconfig.py
	
	echo;read -p &quot;(1/3)请设置新的前端地址:&quot; Front_end_address
	read -p &quot;(2/3)请设置新的节点ID:&quot; Node_ID
	read -p &quot;(3/3)请设置新的Mukey:&quot; Mukey
	
	if [[ ${Mukey} = '' ]];then
		Mukey='mupass';echo &quot;emm,我们已将Mukey设置为:mupass&quot;
	fi
	
	sed -i &quot;17c WEBAPI_URL = \'${Front_end_address}\'&quot; /root/shadowsocks/userapiconfig.py
	sed -i &quot;2c NODE_ID = ${Node_ID}&quot; /root/shadowsocks/userapiconfig.py
	sed -i &quot;18c WEBAPI_TOKEN = \'${Mukey}\'&quot; /root/shadowsocks/userapiconfig.py
	
	bash /root/shadowsocks/stop.sh
	bash /root/shadowsocks/run.sh
	echo &quot;新设置已生效.&quot;
}
#检查BBR状态
Check_BBR_installation_status(){
	uname -r
	echo -e &quot;\033[31m[↑]查看内核版本,含有4.12或更高即可.\033[0m&quot;;echo
	sysctl net.ipv4.tcp_available_congestion_control
	echo -e &quot;\033[31m[↑]返回：net.ipv4.tcp_available_congestion_control = bbr cubic reno 即可.\033[0m&quot;;echo
	sysctl net.ipv4.tcp_congestion_control
	echo -e &quot;\033[31m[↑]返回：net.ipv4.tcp_congestion_control = bbr 即可.\033[0m&quot;;echo
	sysctl net.core.default_qdisc
	echo -e &quot;\033[31m[↑]返回：net.core.default_qdisc = fq 即可.\033[0m&quot;;echo
	lsmod | grep bbr
	echo -e &quot;\033[31m[↑]返回值有 tcp_bbr 模块即说明bbr已启动.\033[0m&quot;
}
#测速
Run_Speedtest(){
	read -p &quot;执行SpeedTest?[y/n]:&quot; SpeedTest
	case &quot;${SpeedTest}&quot; in
	y)
		chmod 777 /root/tools/speedtest.py
		cd /root/tools;./speedtest.py;cd /root;;
	*)
		echo &quot;跳过.&quot;;echo;;
	esac

	read -p &quot;执行UnixBench?[y/n]:&quot; UnixBench
	case &quot;${UnixBench}&quot; in
	y)
		chmod 777 /root/tools/unixbench.sh
		cd /root/tools;./unixbench.sh;cd /root;;
	*)
		echo &quot;跳过.&quot;;echo;;
	esac

	read -p &quot;执行Bench SH?[y/n]:&quot; Bench_SH
	case &quot;${Bench_SH}&quot; in
	y)
		wget -qO- bench.sh | bash;;
	*)
		echo &quot;跳过.&quot;;echo;;
	esac
}
#初始化
init(){
   apt-get -y update;apt-get install -y wget curl git lsof python-pip build-essential supervisor
	if [ ! -f /usr/bin/ssr ];then
		wget -O /root/ssr_file.zip &quot;https://github.com/qinghuas/ss-panel-and-ss-py-mu/archive/master.zip&quot;
		unzip /root/ssr_file.zip -d /root;mv /root/ss-panel-and-ss-py-mu-master/* /root
		cp /root/ssr.sh /usr/bin/ssr;chmod 777 /usr/bin/ssr
		rm -rf ssr_file.zip /root/ss-panel-and-ss-py-mu-master /root/picture /root/README.md /root/ssr.sh
		clear;echo &quot;INSTALL DONE,Hellow.&quot;;sleep 1
	fi
}

#安装bbr
Install_BBR(){
	bash /root/tools/bbr.sh
}

init

echo &quot;####################################################################
# Edition #  V1.0                                                  #
# From    #  @kylinshine                                           #
####################################################################
# [ID]  [TYPE]  # [DESCRIBE]                                       #
####################################################################
# [1] [Install] # [SS NODE] AND [BBR]                              #
# [2] [Change]  # [SS NODE INOF]                                   #
# [3] [Install] # [SS NODE]                                        #
# [4] [Install] # [BBR]                                            #
####################################################################
# [a]检查BBR状态 [b]Speedtest [c]安装Aria2                         #
####################################################################
# [x]重新加载 [y]更新脚本 [z]删除脚本 [about]关于脚本              #
####################################################################&quot;
read -p &quot;PLEASE SELECT OPTIONS:&quot; SSR_OPTIONS

clear;case &quot;${SSR_OPTIONS}&quot; in
	1)
	Install_ss_node
	Install_BBR;;
	2)
	Edit_ss_node_info;;
	3)
	Install_ss_node;;
	4)
	Install_BBR;;
	a)
	Check_BBR_installation_status;;
	b)
	Run_Speedtest;;
	c)
	Run_Speedtest_And_Bench_sh;;
	x)
	/usr/bin/ssr;;
	y)
	REINSTALL;;
	z)
	UNINSTALL;;
	about)
	cat /root/tools/about.txt;;
	node)
	GET_NODE_SH_FILE;;
	*)
	echo &quot;选项不在范围内,2s后将重新加载,请注意选择...&quot;;sleep 2
	/usr/bin/ssr;;
esac</rich_text></node><node name="群晖" prog_lang="custom-colors" readonly="False" tags="" unique_id="67"><rich_text>在群晖上安装python3的三方包
在商店直接安装python3,安装完python3的具体位置在 /volume1/@appstore/py3k/usr/local/bin下
然后切换root模式 sudo -i 输入密码进入root模式
wget https://bootstrap.pypa.io/ez_setup.py  #下载easy_install,出现503错误就用网页打开复制,新建ez.py 粘贴保存 上传到群晖
python3 ez_setup.py install #安装easy_install
/volume1/@appstore/py3k/usr/local/bin&gt;./easy_install pip #用easy_install 安装pip 注意路径 这个没有添加到环境变量，easy_install没有install参数
/volume1/@appstore/py3k/usr/local/bin&gt;./pip install requests  #用easy_install 安装pip 注意路径 这个没有添加到环境变量
/volume1/@appstore/py3k/usr/local/bin&gt; ./pip install requests pymysql 
执行py3脚本
nuhup python3 sync.py &amp;  #后台执行脚本
</rich_text></node><node name="Frp" prog_lang="sh" readonly="False" tags="" unique_id="69"><rich_text>root# cd /home 
root# wget -c https://github.com/fatedier/frp/releases/download/v0.21.0/frp_0.21.0_linux_amd64.tar.gz
root# tar xzvf frp_0.21.0_linux_amd64.tar.gz
root# mv frp_0.21.0_linux_amd64 frp
root# rm -rf frp_0.21.0_linux_amd64.tar.gz
root# chmod -R 755 frp</rich_text></node><node name="Ubuntu" prog_lang="sh" readonly="False" tags="" unique_id="53"><rich_text>
Ubuntu下部署flask
apt-get update #更新系统,Ubuntu上有python3
apt-get install nginx python3-pip #安装nginx和pip
pip3 install virtualenv #安装虚拟环境
virtualenv ENV #创建虚拟环境
cd ENV #创建虚拟环境目录
source ./bin/activate #激活虚拟环境
pip install flask gunicorn requests #虚拟环境中安装需要的包
deactivate #退出虚拟环境
cp default /etc/nginx/sites-available/default #拷贝nginx的配置,包含flask后端的代理
ln -s /etc/nginx/sites-available/default /etc/nginx/sites-enabled/default #应用配置文件
cp myflask.service /etc/systemd/system/myflask.service #将自定义的服务单复制到服务目录,所有可用的单元文件存放在 /usr/lib/systemd/system/ 和 /etc/systemd/system/ 目录（后者优先级更高）
systemctl enable myflask.service #设置自动启动
systemctl daemon-reload #加载可执行单元
systemctl start myflask.service #启动自定义服务

######################### default 配置#########################

server {
        listen 80;
        server_name yanwen.cf; # 外部地址

        location / {
                proxy_pass http://127.0.0.1:9000;   #falsk后端代理端口9000
                proxy_redirect     off;
                proxy_set_header   Host                 $http_host;
                proxy_set_header   X-Real-IP            $remote_addr;
                proxy_set_header   X-Forwarded-For      $proxy_add_x_forwarded_for;
                proxy_set_header   X-Forwarded-Proto    $scheme;
        }
		location /img { 
				alias /home/ubuntu/app/img;   #图片目录

		}
}
#########################myflask.service 脚本内容#########################
[Unit]
Description=The myflask service
After=network.target

[Service]
WorkingDirectory=/home/ubuntu/app
ExecStart=/bin/bash myflask.sh
Restart=always

[Install]
WantedBy=multi-user.target
#########################myflask.sh#########################
/home/ubuntu/app/myflask.sh脚本内容
#!/bin/sh
cd /home/ubuntu 
source ENV/bin/activate; #激活virtualenv环境
cd /home/ubuntu/app; # 进入flask项目目录
gunicorn -w 2 -b 127.0.0.1:9000 flaskApp:app; # 启动gunicorn服务
</rich_text></node></node><node name="Java" prog_lang="java" readonly="False" tags="java" unique_id="2"><rich_text>
</rich_text><node name="环境变量" prog_lang="sh" readonly="False" tags="" unique_id="29"><rich_text>###################################################设置java的jdk#################################################################
Dim MyVar
Set shell = CreateObject(&quot;Shell.Application&quot;)   
Set selFolder = shell.BrowseForFolder(0, &quot;请选择JAVA_HOME目录(如:D:/jdk1.5.0_16):&quot;, 0, ssfDRIVES)   
Set selFolderItem = selFolder.Self   
selPath = selFolderItem.Path   
dim wsh   
If (selPath  &lt;&gt;&quot;&quot;) Then
Set wsh = WScript.CreateObject(&quot;WScript.Shell&quot;)   
wsh.Environment(&quot;system&quot;).Item(&quot;JAVA_HOME&quot;)=selPath
wsh.Environment(&quot;system&quot;).Item(&quot;CLASSPATH&quot;)=wsh.Environment(&quot;system&quot;).Item(&quot;CLASSPATH&quot;)+&quot;;.;%JAVA_HOME%/lib/tools.jar;%JAVA_HOME%/lib/dt.jar;%JAVA_HOME%/bin;&quot;
wsh.Environment(&quot;system&quot;).Item(&quot;Path&quot;) = &quot;%JAVA_HOME%/bin;%JAVA_HOME%/jre/bin;&quot; +wsh.Environment(&quot;system&quot;).Item(&quot;path&quot;)
MyVar = MsgBox (&quot;恭喜，完成设置！&quot;, , &quot;MSG&quot;)
End If
###################################################设置maven#######################################################################
Dim MyVar
Set shell = CreateObject(&quot;Shell.Application&quot;)   
Set selFolder = shell.BrowseForFolder(0, &quot;请选择MAVEN_HOME目录(如:D:/MAVEN_HOME):&quot;, 0, ssfDRIVES)   
Set selFolderItem = selFolder.Self   
selPath = selFolderItem.Path   
dim wsh   
If (selPath  &lt;&gt;&quot;&quot;) Then
Set wsh = WScript.CreateObject(&quot;WScript.Shell&quot;)   
wsh.Environment(&quot;system&quot;).Item(&quot;MAVEN_HOME&quot;)=selPath
wsh.Environment(&quot;system&quot;).Item(&quot;Path&quot;) = &quot;%MAVEN_HOME%/bin;&quot; +wsh.Environment(&quot;system&quot;).Item(&quot;path&quot;)
MyVar = MsgBox (&quot;恭喜，完成设置！&quot;, , &quot;MSG&quot;)
End If
###################################################################################################################################
</rich_text></node><node name="Demo" prog_lang="java" readonly="False" tags="" unique_id="30"><rich_text>###################################################hello world###################################################################
public class Hello {

	public static void main(String[] args) {
		System.out.println(&quot;hello world&quot;);

	}

}
###################################################end###########################################################################
</rich_text></node><node name="pom" prog_lang="java" readonly="False" tags="" unique_id="68"><rich_text>###################################################maven的pom.xml的范例#################################################################

&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.github.hero&lt;/groupId&gt;
    &lt;artifactId&gt;TestJTDS&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;name&gt;TestJTDS&lt;/name&gt;
    &lt;!-- FIXME change it to the project's website --&gt;
    &lt;url&gt;http://www.example.com&lt;/url&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;net.sourceforge.jtds&lt;/groupId&gt;
            &lt;artifactId&gt;jtds&lt;/artifactId&gt;
            &lt;version&gt;1.3.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;commons-dbutils&lt;/groupId&gt;
            &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt;
            &lt;version&gt;1.7&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.2.0&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;shade&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;transformers&gt;
                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;
                                    &lt;mainClass&gt;com.github.hero.App&lt;/mainClass&gt;
                                &lt;/transformer&gt;
                            &lt;/transformers&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;</rich_text></node></node><node name="Database" prog_lang="plain-text" readonly="False" tags="database" unique_id="3"><rich_text>
</rich_text><node name="sqlserver" prog_lang="sql" readonly="False" tags="sqlserver" unique_id="4"><rich_text>
</rich_text><node name="动态语句" prog_lang="sql" readonly="False" tags="" unique_id="17"><rich_text>动态sql语句基本语法 
1 :普通SQL语句可以用Exec执行 
eg:
Select * from tableName 
Exec('select * from tableName') 
Exec sp_executesql N'select * from tableName'    -- 请注意字符串前一定要加N 

2 :字段名，表名，数据库名之类作为变量时，必须用动态SQL 

eg:
declare @fname varchar(20) 
set @fname = 'FiledName' 
Select @fname from tableName              -- 错误,不会提示错误，但结果为固定值FiledName,并非所要。 
Exec('select ' + @fname + ' from tableName')     -- 请注意 加号前后的 单引号的边上加空格 
当然将字符串改成变量的形式也可 
declare @fname varchar(20) 
set @fname = 'FiledName' --设置字段名 

declare @s varchar(1000) 
set @s = 'select ' + @fname + ' from tableName' 
Exec(@s)                -- 成功 
exec sp_executesql @s   -- 此句会报错 


-- declare @sql NVARCHAR(MAX)
declare @s Nvarchar(1000)  -- 注意此处改为nvarchar(1000) 
set @s = 'select ' + @fname + ' from tableName' 
Exec(@s)                -- 成功     
exec sp_executesql @s   -- 此句正确 

3. 输出参数 
declare @num int,@sqls nvarchar(4000) 
set @sqls='select count(*) from tableName' 
exec(@sqls) 
--如何将exec执行结果放入变量中？ 

declare @num int,@sqls nvarchar(4000) 
set @sqls='select @a=count(*) from tableName ' 
exec sp_executesql @sqls,N'@a int output',@num output 
select @num 
 
 
declare @sql NVARCHAR(MAX)
declare @i int
declare @rq NVARCHAR(10)
set @i=50
set @rq ='2013-01-03'
set @sql = 'update top ' + convert(NVARCHAR,@i) + ' from pf_ckhz  where rq ='''+@rq+''' and xtype=0' 
print @sql



DECLARE @IntVariable int;
DECLARE @SQLString nvarchar(500);
DECLARE @ParmDefinition nvarchar(500);
DECLARE @max_title varchar(30);

SET @IntVariable = 197;
SET @SQLString = N'SELECT @max_titleOUT = max(Title) FROM AdventureWorks.HumanResources.Employee WHERE ManagerID = @level';
SET @ParmDefinition = N'@level tinyint, @max_titleOUT varchar(30) OUTPUT';

EXECUTE sp_executesql @SQLString, @ParmDefinition, @level = @IntVariable, @max_titleOUT=@max_title OUTPUT;
SELECT @max_title;

@sqlstring ：就是你要执行的sql语句字符串
@ParmDefinition： @sqlstring里边用到的参数在这里声明 输出的参数要加output  
sp_executesql： 
第一个参数sqlstring 就是执行的sql字符串了
第二个参数@ParmDefinition是@sqlstring里边用到的参数在这里声明 输出的参数要加output  
最后的参数加output的参数是输出的参数（需要和外部的相对应的变量建立关联）
中间的参数就是@sqlstring 里边用到的参数（需要和外部的相对应的变量建立关联）
最后你可以 select 输出的参数 来查询(select @count)

</rich_text></node><node name="语法" prog_lang="sql" readonly="False" tags="" unique_id="18"><rich_text>SELECT CONVERT(varchar(100), GETDATE(), 0)--05 16 2006 10:57AM
SELECT CONVERT(varchar(100), GETDATE(), 1)--05/16/06
SELECT CONVERT(varchar(100), GETDATE(), 2)--06.05.16
SELECT CONVERT(varchar(100), GETDATE(), 3)--16/05/06
SELECT CONVERT(varchar(100), GETDATE(), 4)--16.05.06
SELECT CONVERT(varchar(100), GETDATE(), 5)--16-05-06
SELECT CONVERT(varchar(100), GETDATE(), 6)--16 05 06
SELECT CONVERT(varchar(100), GETDATE(), 7)--05 16, 06
SELECT CONVERT(varchar(100), GETDATE(), 8)--10:57:46
SELECT CONVERT(varchar(100), GETDATE(), 9)--05 16 2006 10:57:46:827AM
SELECT CONVERT(varchar(100), GETDATE(), 10)--05-16-06
SELECT CONVERT(varchar(100), GETDATE(), 11)--06/05/16
SELECT CONVERT(varchar(100), GETDATE(), 12)--060516
SELECT CONVERT(varchar(100), GETDATE(), 13)--16 05 2006 10:57:46:937
SELECT CONVERT(varchar(100), GETDATE(), 14)--10:57:46:967
SELECT CONVERT(varchar(100), GETDATE(), 20)--2006-05-16 10:57:47
SELECT CONVERT(varchar(100), GETDATE(), 21)--2006-05-16 10:57:47.157
SELECT CONVERT(varchar(100), GETDATE(), 22)--05/16/06 10:57:47 AM
SELECT CONVERT(varchar(100), GETDATE(), 23)--2006-05-16
SELECT CONVERT(varchar(100), GETDATE(), 24)--10:57:47
SELECT CONVERT(varchar(100), GETDATE(), 25)--2006-05-16 10:57:47.250
SELECT CONVERT(varchar(100), GETDATE(), 100)--05 16 2006 10:57AM
SELECT CONVERT(varchar(100), GETDATE(), 101)--05/16/2006
SELECT CONVERT(varchar(100), GETDATE(), 102)--2006.05.16
SELECT CONVERT(varchar(100), GETDATE(), 103)--16/05/2006
SELECT CONVERT(varchar(100), GETDATE(), 104)--16.05.2006
SELECT CONVERT(varchar(100), GETDATE(), 105)--16-05-2006
SELECT CONVERT(varchar(100), GETDATE(), 106)--16 05 2006
SELECT CONVERT(varchar(100), GETDATE(), 107)--05 16, 2006
SELECT CONVERT(varchar(100), GETDATE(), 108)--10:57:49
SELECT CONVERT(varchar(100), GETDATE(), 109)--05 16 2006 10:57:49:437AM
SELECT CONVERT(varchar(100), GETDATE(), 110)--05-16-2006
SELECT CONVERT(varchar(100), GETDATE(), 111)--2006/05/16
SELECT CONVERT(varchar(100), GETDATE(), 112)--20060516
SELECT CONVERT(varchar(100), GETDATE(), 113)--16 05 2006 10:57:49:513
SELECT CONVERT(varchar(100), GETDATE(), 114)--10:57:49:547
SELECT CONVERT(varchar(100), GETDATE(), 120)--2006-05-16 10:57:49
SELECT CONVERT(varchar(100), GETDATE(), 121)--2006-05-16 10:57:49:000
select convert(char(11),getdate(),21)--2015-06-30
select convert(varchar(10),getdate()-7,120) --返回当前日期的前7天日期 2015-06-03
select convert(varchar(20),getdate(),112)+left(replace(convert(varchar(20),getdate(),108),':',''),4)--返回时间格式为201303061700
select convert(char(8),dateadd(month,-1,getdate()),120)+'26'--返回上个月26日
SELECT obj.Name 存储过程名, sc.TEXT 存储过程内容  FROM syscomments sc  
INNER JOIN sysobjects obj ON sc.Id = obj.ID 
WHERE sc.TEXT LIKE '%自己要查的内容%'      --查询所有存储过程中有哪个字段或者表，一定要用模糊查询
select object_name(id),* from syscomments where text like '%自己要查的内容%'  --一样的效果查询存储过程</rich_text></node><node name="游标" prog_lang="sql" readonly="False" tags="" unique_id="24"><rich_text>###########################################补时空出库验收记录###########################################
declare cursor1 cursor for select kaipiaodjbh from pf_ckhz where djbh like 'xsa%' and rq&gt;'2016-06-07' order by rq
open cursor1
declare @djbh char(14)
declare @kaipiaodjbh char(14)
FETCH NEXT FROM cursor1 into @kaipiaodjbh
while @@FETCH_STATUS=0
begin
exec SOF_getmaxdjbh @fdbs='AAA',@biaoshi='GCK',@addflag=3,@mkbh='A5',@maxbh=@djbh output
insert into GSP_CKYS_HZ(djbh,rq,ontime,kaiprq,dwbh,bm,username,xgdjbh,yanshr)
select @djbh,rq, ontime,kaiprq,dwbh,bm,fuhr,kaipiaodjbh,fuhr from pf_ckhz where kaipiaodjbh=@kaipiaodjbh
insert into GSP_CKYS_MX(djbh,dj_sn,spid,pihao,sxrq,baozhiqi,shl,hshj,hsje,hw,xgdjbh,gsp_hege)
select @djbh,dj_sn,spid,pihao,sxrq,baozhiqi,shl,hshj,hsje,hw,xgdjbh,1 from pf_ckmx where xgdjbh=@kaipiaodjbh

FETCH NEXT FROM cursor1 into @kaipiaodjbh
end 
close cursor1 
deallocate cursor1</rich_text></node></node><node name="mysql" prog_lang="sql" readonly="False" tags="mysql" unique_id="5"><rich_text>下载mysql-5.7.9-winx64.zip 解压到 D:\Program Files (x86)\mysql位置
下载连接：http://dev.mysql.com/downloads/mysql/
设置环境变量 D:\Program Files (x86)\mysql\bin到环境变量path用脚本 
####################################################################################################################################################
Dim MyVar
Set shell = CreateObject(&quot;Shell.Application&quot;)
Set selFolder = shell.BrowseForFolder(0, &quot;选择Mysql的bin目录&quot;, 0, ssfDRIVES)
Set selFolderItem = selFolder.Self
MysqlPath = selFolderItem.Path
If (MysqlPath  &lt;&gt;&quot;&quot;) Then
dim wsh
Set wsh = WScript.CreateObject(&quot;WScript.Shell&quot;)
wsh.Environment(&quot;system&quot;).Item(&quot;Path&quot;) = MysqlPath +&quot;;&quot; +wsh.Environment(&quot;system&quot;).Item(&quot;path&quot;)
MyVar = MsgBox (&quot;恭喜，完成设置！&quot;, , &quot;MSG&quot;)
End If
####################################################################################################################################################
win7下权限不足的话，shift+右键 打开命令行在命令行运行该脚本
修改mysql的配置文件my-default.ini

basedir = D:\Program Files (x86)\mysql
datadir = D:\Program Files (x86)\mysql\data

然后初始化数据库
mysqld --initialize --user=mysql --console
初始化批处理：
####################################################################################################################################################
mysqld --initialize
mysqld -install
net start mysql
####################################################################################################################################################
卸载批处理：
####################################################################################################################################################
net stop mysql
mysqld remove
####################################################################################################################################################
记住产生的随机密码,记不住的话去data目录下有个文件时自己计算机名字.pid的日志文件自行查找
mysqld -install
net start mysql
mysql -u root -p 输入密码
进去后set password = password('root'); 修改密码即可</rich_text></node><node name="pgsql" prog_lang="plain-text" readonly="False" tags="" unique_id="54"><rich_text>initdb.exe -D ../data -E UTF-8 --locale=chs -U postgres -W #初始化
pg_ctl.exe -D &quot;D:\Program Files (x86)\pgsql\data&quot; -l logfile start #启动服务
createdb -h localhost -p 5432 -U postgres tumblr #建立数据库
pg_ctl.exe register -N PostgreSQL -D &quot;D:\Program Files (x86)\pgsql\data&quot; #注册为后台服务
##########################postgresql.conf##########################
listen_addresses = '*' #监听所有网络地址，为远程连接
port = 5432 #端口
max_connections = 500 #最大连接数
##########################pg_hba.conf##########################
host    all             all             0.0.0.0/0          md5
</rich_text></node></node><node name="git" prog_lang="sh" readonly="False" tags="git" unique_id="12"><rich_text>git init    #初始化目录
git add readme.txt   #把文件添加进去，实际上就是把文件修改添加到暂存区
git commit -m &quot;wrote a readme file&quot; #提交更改，实际上就是把暂存区的所有内容提交到当前分支,-m 后面是说明备注
git add file2.txt file3.txt  #一次可以添加多个文件
git status  #查看状态
git diff readme.txt #查看文件差异
git log #查看日志
git checkout -- file.txt #撤销修改有两种情况一种是文件自修改后还没有被放到暂存区，撤销修改就回到和版本库一模一样的状态
git reset --hard HEAD^ #回到上一个版本,在git-bash这种写法没问题,在windows环境 应该写成git reset --hard “HEAD^”
git reset --hard 3628164 #按照文件hash进行改变
git reflog #用来记录你的每一次命令，当无法查询hash id的时候可以用此命令查询
git remote add origin git@github.com:kylinshine/cherry.git #添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库
git remote rm origin #删除远程关联
git push -u origin master #由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，
                          #还会把本地的master分支和远程的master分支关联起来，在 以后的推送或者拉取时就可以简化命令
                          #推送成功后，可以立刻在GitHub页面中看到远程库的内容已经和本地一模一样：
git push origin master    #把本地master分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！
                          #要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git；
                          #关联后，使用命令git push -u origin master第一次推送master分支的所有内容；
                          #此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改；
git pull origin master    #拉取github的版本到本地
git clone git@github.com:kylinshine/gitskills.git #克隆复制远程git到本地
git checkout -b dev #我们创建dev分支，然后切换到dev分支
git branch dev #git checkout命令加上-b参数表示创建并切换，相当于上面两条命令,b 就是branch的意思
git checkout dev
git branch  #查看当前分支，命令会列出所有分支，当前分支前面会标一个*号
git merge dev #命令用于合并指定分支到当前分支
git branch -d dev #删除dev分支
git init  #删除了本地.git目录及其相关文件 先初始化在添加远程仓库 再pull即可,一般是本地和远程分支出现了差异
git remote add origin git@github.com:kylinshine/cherry.git #添加远程仓库
git pull origin master #拉取远程分支到本地，再更改再提交
git add filename.txt #添加文件
git commit -m &quot;wirite something&quot; #提交改变到本地
git push origin master #push到远程
</rich_text></node><node name="学习日志" prog_lang="python" readonly="False" tags="" unique_id="16"><rich_text>http://rainbird.blog.51cto.com/211214/225541/ #两台linux完美实现双机热备
https://www.douban.com/note/298777094/#在centos下实现mysql双机热备
http://mahua.jser.me/ # 一个在线编辑markdown文档的编辑器
http://blog.csdn.net/kaitiren/article/details/38513715 #markdown语法
http://www.jianshu.com/p/be9dd421fb8d #python web 部署：nginx + gunicorn + supervisor + flask 部署笔记
http://heimu360.applinzi.com/ #汤不热大数据
http://python.jobbole.com/82045/ #python多进程
http://www.haaker.cn/index.php/archives/5663  #routeros 4种vpn
http://openvpn.ustc.edu.cn/  #openvpn
http://demo.pyspider.org/ #一个国人python爬虫框架
https://github.com/Fuck-You-GFW #一个有dht的github
https://github.com/code4craft/ # 黄亿华github
http://webmagic.io/docs/zh/posts/ch2-install/first-project.html #黄亿华的webmagic中文docs
https://github.com/Mooooooon/Manegeoon #视频分类整理工具
https://github.com/wn0112/PPlayer #python+pyqt4 写的mp3播放器
https://github.com/brandonxiang/psm #一个python换源的程序
https://github.com/xianhu/PSpider #python写的一个简单爬虫框架
http://blog.chinaunix.net/uid-17291169-id-3241613.html #利用pyhton 生成PDF文件 
http://www.cnblogs.com/szerr/p/4574063.html #python写的的语音天气预报
https://github.com/tarampampam/nod32-update-mirror #nod32镜像更新github
https://github.com/phith0n/mooder #Mooder是一款开源、安全、简洁、强大的团队内部知识分享平台
https://github.com/phith0n/Minos #一个基于Tornado/mongodb/redis的社区系统
http://www.tuicool.com/articles/aAruqy #python多线程下载文件 分块下载 合并完成
https://github.com/FWC1994/Python-Crawler #多线程下载电影天堂
http://www.csdn.net/article/2013-05-02/2815101 #Python中的多线程编程的指南
http://docs.pythontab.com/ #python中文开发社区在线文档
http://www.tuicool.com/articles/byIF7f #pyqt下载文件
http://blog.csdn.net/pleasecallmewhy/article/details/8922826 #python爬虫相关文章
http://www.cnblogs.com/sink_cup/archive/2011/03/15/http_user_agent.html #ua大全
http://www.sharejs.com/codes/python/8310 #ua大全
https://segmentfault.com/blog/fxypython?tag=scrapy #scrapy学习
https://segmentfault.com/q/1010000006244984 #scrapy迭代爬
http://www.jianshu.com/p/83cd445de841 #scrapy相关学习
https://github.com/beilunyang/dhtCrawler #nodejs dht爬虫
https://github.com/dontcontactme/p2pspider #另一个dht nodejs爬虫
http://blog.sina.com.cn/s/blog_620987bf0102vkx7.html #python分析股票
http://tieba.baidu.com/p/4372344609 #windows下dht爬虫
http://www.cnblogs.com/huangxie/p/5550680.html #python dht爬虫
http://www.tuicool.com/articles/2u26fq7 #上面的文章镜像 dht爬虫
http://www.361way.com/python-stock-tushare/4579.html #python stock数据包tushare
https://github.com/78/ssbc #笑笑的手撕包菜dnt
https://zhuanlan.zhihu.com/p/22370315?refer=zimei #python支付接口
https://www.zhihu.com/question/21395276 #python第一个回答DL的相关学习
https://github.com/fzliu/style-transfer #基于python深度学习库Caffe的实现
https://github.com/anishathalye/neural-style #基于python深度学习库TensorFlow的实现
https://github.com/andersbll/neural_artistic_style #基于python深度学习库DeepPy的实现
http://ostagram.ru/queue_images?locale=en #一个DL画图的网站
http://blog.csdn.net/zhangzheng0413/article/details/41728869 #线程join和守护理解
http://www.cnblogs.com/qiyeboy/p/5693128.html #代理线程池的使用
https://github.com/qiyeboy/IPProxyPool #代理池的代码
http://blog.csdn.net/HanTangSongMing/article/details/24454453 #scrapy批量获取腾讯职位招聘</rich_text><node name="java学习日志" prog_lang="java" readonly="False" tags="" unique_id="45"><rich_text>https://github.com/liufeng0103/bnade #一个java写的魔兽世界拍卖行相关数据的查询和分析
http://download.csdn.net/detail/maoxiang/2197531 #java写的多线程下载
http://www.360doc.com/content/13/0224/00/7669533_267538568.shtml #JAVA定时执行任务(Tomcat 下的定时任务)
http://blog.csdn.net/l1028386804/article/details/52727297 #Java之——实现每天定时执行任务
http://download.csdn.net/detail/lbxx_64/7334323 #微信订阅号java源码
http://www.98key.com/ #myeclipse注册码
http://blog.csdn.net/qy20115549/article/details/52648631 #java多线程爬虫爬时光网
</rich_text></node></node><node name="计划" prog_lang="python" readonly="False" tags="" unique_id="15"><rich_text>1 学习linux磁盘管理 vg
2 数据分析分析销售图
3 dht种子网络
4 爬虫sfda
5 药检打印系统
6 学习python深度学习相关
7 写一个自动更新nod32更新包的程序
8 写一个自己电驴和磁力网站 java或者python
9 学习java
10 根据erp数据登录ftp删除不存在的药检</rich_text></node><node name="nod32" prog_lang="custom-colors" readonly="False" tags="" unique_id="31"><rich_text link="webs http://www.eset.hk/support/update_package/">http://www.eset.hk/support/update_package/</rich_text><rich_text background="#ffffff" foreground="#444444"> </rich_text><rich_text> #免cdkey离线更新包 可能不是最新的
</rich_text><rich_text link="webs http://isnet.grifon.info/nod/">http://isnet.grifon.info/nod/</rich_text><rich_text> #google 搜索 title:index of /nod
</rich_text><rich_text link="webs http://ftp.ivanovo.ac.ru/updates/eset/">http://ftp.ivanovo.ac.ru/updates/eset/</rich_text><rich_text> #同样一个发布离线更新包的网站

nod32注册表更改更新地址
Windows Registry Editor Version 5.00

[HKEY_LOCAL_MACHINE\SOFTWARE\ESET\ESET Security\CurrentVersion\Plugins\01000400\Profiles]
&quot;Active&quot;=&quot;@My profile&quot;
&quot;Enable&quot;=dword:00000001

[HKEY_LOCAL_MACHINE\SOFTWARE\ESET\ESET Security\CurrentVersion\Plugins\01000400\Profiles\@My profile]
&quot;SelectedServer&quot;=&quot;</rich_text><rich_text link="webs http://192.168.0.250:2221&quot;">http://192.168.0.250:2221&quot;</rich_text><rich_text>
&quot;MirrorFolder&quot;=&quot;C:\\ProgramData\\ESET\\ESET NOD32 Antivirus\\mirror&quot;
</rich_text></node><node name="tumblr blog" prog_lang="plain-text" readonly="False" tags="" unique_id="47"><rich_text>eroticsets2016
burymytool
favoritefemaleassholes
spreading-beauties
superb-pussy44
anal-baby-queen
lachattechaude
beauty-adoring
sosexysadie
b00bznbutts
stknca
bighotmommas
msthickcockluver
billyguitar77
waltlove53
buttsandpussies
pussyhottie
karteris
jm0n3y1
hypno-fantassy
pussylipsonly
eltepual
stuffmyholesxxx
perfectnooksandcrannies
paparatz
oralxo69
lonelyvanessa
pornvideo-xnxx
pure-teenies
dannygo
monocroam
awake-r
kjg1983
pussyfromtherear
sunstop-7
91bing
dadad
yuukishindou
xxoohy
yangyaojing
maydaylxy
saboten10
idoljp
deaavra
haeng76
erorian
yosisan
ero-archive
gluccosfer
whitetigerwelfare
sexeyyblog
asianidolworship
perfectcollectorcycle
joenemesis
mobius1
girls-squirt
squirtbetty
kitmen83
love2pl4y
69loco
ku777
mickd82
mmmm1118
love-sexy123
leslie-wjr
kaqiakaia
beauty-loli-girl
barvosa382
fhqpdls1
mychubbylady
luoliluoliyounvyounv
hpladyme
69pos
yeongeeeee
simple5213
fliiip
casalprocura
asswatcher10
edincomox-blog
godbells
clams-only
iamcompletelydonewiththis
sexy-sexy-womans
netobubu
wintererin
jp-youngxxx
hhhhhhhllli
sex
591fuli
yamato2520
1pondo-av
santinsan
hinbsksksbjdjd
o-cafe
inlove-woman
jkgirl
ziwei-ziwei
jietou-she
-----------------------------------------------------------------------
lzhd
love91porn
paoshen007
caoliushequ
deepandruff
eroticsets2020
eroticsets3
18plusclub
eroticsets4
xiongxiaomei
danielkaiiii
dregs1994
jdhbx
newbacktoo
x-nvshen
plzbvip
160188
yaonv
zhou666888
deepbulerable
aharw
acmedoll
plushflesh
hockeygirl91
superdirtysexythings
toko-blue
albenizdummy
victim-girls
mattspussyblog
giveheryourseed
impreg-kink
sunylub
find4221
isexpro-net
meetori
xzrrr
xxsdsjxx
xxfuckluvxx
joneyegirl
sg8634
sex101
clementyee71
stallionhierarchy
sevici69sebastian
casualteensexx
teenfucksleep
girlsyoulike
justnotyouraverageblog
danidanielsx
bustnutcum
tightpussyvirgin
awesomefreepornvideos
60fpsporn
comat59
cashmoneymd
ourothergeminiside
sexygirlwholifts
sexthing69
horny-girl-69
alwayys-hornyy
amantedeculo
unravelghost
overti8888
suckmyclitout
tianyimore
max345927588
mixmix666
meng900821
dariong-han
0mg1mg
chinese-cuckod
bbccav
colindanny
jasontaike
xuuuu
zccxcc
staneybia
sexcn
spidergif2
atomickittyface
/////////////////////////////////////////////////////
lhb2759
xchenglove
camemu1
sunlong1992
mnxs-m
fetishkorea
tomsun9733
kimsex19
petitkitten321
love-yoon-s-a
zhszzz
bunny-covers
a215441169
fyfyvyv
wztjwj
shaoye0913
keengod
sisongox
zane1993
shizuoyongzge
caoliusqzxdi
thomen
coolbeautiful-pussy
guestbookman
kdd6428
niillli
cuckold-netora...
bracum001
myhotgf-e
bamtokki-com
widyunky
srdzvipppp
huahuaergege
kve7ch
training5040
jreferry
fm925-nini
asdongdong
kamebrog
ssdgdhejdb
guo-chuan
11201puss
charmingj
zhling1994
godmmm
big-boobsgirls
emmaescapes
onlypyc
sexy-dobble
vinsonvv
ljfbin
doll
gregorylim99
jsnfbxjsj
trannytracker
netocop
h-video
waiweidingzhi
xiaomama113015 
jokerhans
bcgg
tugeku77
aaroncwolf
amerotic
boroughs
kim1306qq
sm-niceman
skbj7106
nmpnssb
aacbbc
bw7-7-44
amacer
denniskwok1
tsmannan
msaohb
ellocoperv
bjsir
hugochen1999
bat-brad-pitt
aoi46
mamapapaz
donjonyang
f545954150
seif-sexy-photo
porncosplay
tremblinglust
legjob
justperfectness
unresolvedmystery 
dadbibi
irishgamer1
zhibu899
21222245
liyatay
bestsghkgirls
leejjuu12
thekingofkids
xiaosaobi10
cow-happiness
1993yunyun
aixiaodemaomi
aq1903693296
baihuweitongwen
belial0123
bunnysluttywife
cz101600
dashan666
ddmayo
gogoylj
huoer
ivylovemaster
jerry
joeycatcat21
junnketsu
kebojj
kovkdmoo
lesliechan19
ll0202
llzzyy
mugousissi
nonogook
nuoer
nxi289
rrcomeon
sexy
sexyyc
temuermm
the
threesomewith
tutucindy
viewpro
wanwantingt
xingnumugou
xx1026
zhenaichen
honey886
liruipu
sluttyfayfay-921
piaoliangpigu
lr710666810
julkjilu
solomankin
u44002
thissexysecret
sllaavvee
haotianfuck420
puppygirlella
twwife
xiaoxifu2016
porisound
lingzai01
gongju-s2
heremysexypage
liumeng22
1024823
chengfb
mugou1998
linglingpapapa
fuckme018
mrlovelyqwerty
hariaiaiai
bill801
weltlee
sexcat6
yindangxiaomugou
m-ft
crazyjago
annoyinglyhopefulstarlight
qaqiiiii
zoikhem-lad-choye
canicumyouphoto
siwamugou
bobo-cccc1
yuyi0479
bishihuiyi
dream-wets
xiaowanzi-tang
qplovecr
zaixialvbu
stockingbaby
xhzxgz
keri-011
suegn-8471
390950355
yemao53113
nuanliu
xxgreenfrogxx
trainerofrose
szql123
greenhat1314
jingcexiatian
teenagedelusionparadise
jack07452
vx-wliu910123
weazcola1
vvvvvlbb
fuckwq
monkeygiraffe767
loo-ooo
itshopefulperfectionbouquetblog
nvxing
y2011803
abang1990
chinaoutdoors
833244
kkinaw3
redsexbitch
girlllfriend
masteranddoglead
sara-00-sara
zero-ppp
asura-anger
nanrenfengcai
missyourselfagain
tecnos96
landiskey
bellebodys
sweetlyman
qwd758
venustrap1910
matrix097
saotunnannan
shuimuqingfeng
chenge124
qi-c-c
lovely-annie
lovely-tracy
loveobedientlybitch1399
parrotblood
e1225815
pussynah
hesayiammiracler
kirkykirk
secret599
www
doubleher
subjugatecouple
xiaoxiaobai12138
yaoyaomei
yinqi-ntr
yinwoqi
yiyi2016kink
yohomiao
zhuchangan
xiaogougou
xiaokonglaoshi
xavieml
wlovelw0816
soulwaiting
tiffanypang16
soratum
saosaoqiuai
russy555
queenwen
queendanmo
qiqimeijiqi
pennysexy
pangxiaonuoyaogege
niexiaoxin
ntr1234
maomao1069
lonelypatients025
lgwwl7777
mypaperbagslut
wakaya1213
s721s721s721
nami0716
exposure-to-wife
xy730
yazu-sao
11122ppp
chengshaobin
littlesexroom
shijian99570
konstantine30
jsg-qi
ivanyao
catherinefe
htl69
hongwuye
hongjizhifeng
hlmmbaby
unasit
heisarah94
liliko99
half9999
ganodermabitch
fuckwife
frosty-land
dusadoggy
conanhomles
coco9669
cmizico
chinkogirl
chinatin
cc198704
cainv
blueskybjbj
ar-carcass
miaomiaof
akane88
albee0820
karenllover
ailuoli666
naiyinboby
pocky-girl
a-dirty
51lookatme
2368736283
applem9
mengquan1zhi
puppyjiangzhaozhuren
ellystrikesback
3023969727
tjsaoshuanghhh
cainongsinacom
hanyou322
dainifei
anmengmeng
woshiqinweia
lu-lunie
hahahaha13579
lovenanalin
reblogking1
madkinggg
kitty0020092009
ningbocouples
nvbao
grosserboss
angelskyjack
feiermm
bigpenis236
mxdwusi
hdhhbwj
jiaoqi900202
yzralbus
bunbunmylove
charlieweng
nudebatgirl
djbgsn520
huxudagua
318taotao
yanguoliang
johnny280
tony0103
lilovez
greyforesthu
mellowfesttriumph
livehaha
chnaqi
sexiaomin
lovejinerfan
naiyayingzi-queen
2379399169
fuktwgirl
slutty-asian-hotwife
lovewifeforcrazy
asdbit
subinpapa
saodiandian
xzxbxx
showtimecc
chenqin
2592532122
yayasez
cz10160000
nancymeng
cherryboomh
xbx123
asianbreeding
cnkeyes666
asianfiona
asiancuckoldlover
niceaaaaaa
yuyuxiaoxiao
deepsleepl
frgyhhhhh
sexrapesex20162016
panduolazazhi
shike-suzhi-88
cctv91porn
tianli2015
ccdjb
omnibusrrr
caoyuanbazhu
91ajiuajiu
nzzg
xnandly
luckystarjay
pangpang1987
bensonwoo
azhua
yusx3563
vindiamond
lelenvwang
clyl1024
leo1000000
keainanbaobao
nelochan
lao47
acccc09
mizhichasao
naughtykaoru
bally2
yourfavrose-deactivated20161122
xiaomiaomiao
masteroneyuan
biyangla
samsayy
slowlycolorfulengineer
gwnetori
yinniqi
ordinary12345
chxixi
moregoin
srlive
1684550354
lyiren
yin-wa
8haidaowang8
yeyeahyeah
hisashi1976
sexladyyyy
kissmango
lixiaoyumm
zhiniminmin
saolulu
apple69699
baixiaowen
yoursweetslovegirlworld
jinsefengbao
prinece879
sharon0415
mike-233
pnbcfakes
celebfakes27
maxx315
weiweiqiuai
moyra12
lsaobi
d53471
freeyun
eva5201314
redwei
akasakaandaya
matureslavewife
xiaoming-xiaotiandi
yourpussylover123456789
hdfrt
lovebabyzoz
raped-doll
mannmannmannmannmann
yuciii
asd7789789dsa
mengyi945
dirtytalk1105
sunrise668
sexyasdfgh1200
eyebrow7
bmwsl55
samsue2628
rp-mij
skykid520
anny1221
moduxiaopaowang
littlepumpkinbb
easybabyeloue
fy518fy
xinfenxj
thetoys
thecitynme
sasaw99
coolover
zzghns
crazyambro
dggniuv
exposebitch
manjessss
cbrivers
publicwifebutterfly
skymonk-s
yourwifeswetpussy
bigcake2012
tatalin323
exactlycoralchaos
nycwait4u
glaciergr
mingtoux
adrianyang00
david1604927
hardkingdominternet
singer-m
xiaosaomugou
the-lovingangelcollection
cold-sundae
tiantian695
goodbitch1
atomchen1
bersih76
bear4sex
nicaiwoainime
iamsoeasytocum
sexyfuckmeee
lover
lsaia30678
oreocool
yinjianfuqi
doridorijam
aco520
lolicornpop
jdjdhhdjdmd
zbtdbta485278
rixingqianli
chaojikaishen
chenyenhung
michelleslutworld
crazyengineerdonut
aa20120909
nude-vodka
yanhong
saopoi
sonyalai
slutgirl355033
comet633
meimeiyaoyao
buhuidjd
valiantlygli33tterysheep
noahy5843
mycharmingwife
51851878
x-sigmund-freud
wing-hkgirl2017
sasor1111
jocol
mastergangbang
moon123311
humiliatemeplz
233333333333333333333333
higher059874
siyaltsing
shigold
mrs1985
mixcouple
wife-hunter
zjdcj
tuatu
xxwife
thedesirediary
rtyfhre
xxoo8181
humbleboy
duplicitie
sm-nanzhu
ggxiansheng
uuuseee
beautyshaw
fzzzin
vikingtyou
ohhh-my-deer
76039
xiaobaiaisheying
whiteeeehana
wypgm
r65270
thesexykatsy
sweetie0909
aoooo1992
bb480
polarbear6ice
tenkokoko
zmziyo
zimindoudong
zhuhuiln150306
xusaobim
saomeitingting
sluttybitch3068
masterkama
scarcrowjunk
sherrynotthewine
yesnakedbutterfly
kandedaocaobudao
llvoehotwife
xifu
s-hoooooo
yuanweimugou
shanghaisexygirl
kool17
chenyuhan
mygirlfrienddaily
woshijugen
xiner
luziga456
bjqlbo
zhangjiayinqi
lamapartyx
tightpussyclub 
lsh-love 
arrow-jay
loli-001
6ningt 
jessicaspanties
toesnmore
gggggg3b0
hxm1024 
careless-pissing 
publicshowme
carnelian12
meetslut
phonicgiant
freeporntubemovies
xzwood
nude-wow
lovexlunaticox
hotslutvids
towoho 
riena-blog 
bb-puta 
bagg10zhou 
1995loli 
kim4128 
modengfashi 
afgcxd 
striker777 
girlsdaysexto 
qiuqiu225 
pornhub 
95yuri 
slutwifetoliet
4121251212 
loinway 
19961213yu 
sunnyxt
animated-pornstar-gif.
tang8re
sexy-japanese-chicks
gcxc
coral3la 
hyl9861 
yuyanxi 
quegduei3 
cyb5m
15926928531lqy 
zapopu
estopmain
heavens-beauties
xhamsters
dupreexxx
russianmalesinadultmovies
asiangirlsandrussianboys
johnstrongxxx
artemwashere
badasscathy 
sgsexyhornygirls 
waywardpretender 
j52655512 
lovemimiai 
fuckxiaoyuan99
popo2345
leesw2276
kyahan
nbwangqi
ww9110
zhinengyongshuzi
lmlmwj
pantykings23
just4meworld
goeatsomecookies
inoheadz
66666
zhikanfuli
kaolao230
xxoofuli
toni1113
stime856
chinacouple
pornhubbing
motherx
peoplespleasure
aliceisinlesbianwonderlandagain
yourporns
hotgirl-s
iorialpha 
mydeldostuff-v2 
maxlincanyue 
spidergif4 
obsessedabtazns 
dogmasterblog 
girlsyoulike 
makinwei 
sletov-makary33hdir
lesbovideo 
dajibagandie 
jhz7232 
javier-ga 
jjew18 
69times 
bull68venezia 
overdose-videos 
61612988 
vincentse 
3309366013sblog 
elbori1975 
20160218 
hebei-gg 
sshinelee 
wwwdotp 
jabbajine 
1024porn-10243433 
bruiyu 
dopeunsafe 
yuhanguyue 
dlwsai 
datdopamine2 
yunnmi22 
choihyeona 
scushiyan 
dutifullysuperbbluebird 
cris07111 
wsftyj 
extrordinarypornstuff
liting-lonely 
cumoverhere 
qifox 
hanchongli 
kinglight2014 
unravelghost 
willhu17 
mofeily 
mrsimpleing
nock-nock-nock
qwer9501
rea**
ryannmiku
scotthei
seni014
shollwe
smile67yt
sooooong2
superartlife
suzhuguaishushu
t-yossy
tkg-ab-msf
upeverdaydate
wanimal1980
wsong2014
wuxan1980
xxoo8
xxx1024
yokurebo
zhbq
davidtz8886
berrygirls
fish1025559
deepthoatwhore
50shadesofurine
zhc1997512
goldenboy89
myth4200
plaisiranal
sexthing69
sexychinesegirl
wcsfilm
kakadamianbao
frsfoogfffv
davidtz8886
papa360
lanfengsama
1qaz0okmmm
515880167
tse09gja
niuxiaojian
hyuk01
zhling1994
girgir111211-deactivated2017051
nitutou
colorfulhideoutwerewolfsworld
suckinnyc
ko419
xiaocccc
sherry12309
575786626
1286261230
5929744
ruoyouruowu
konbannookazuni
kitmen83
ho3ny
yyaanonyu
sdh777
lilyzhen123
zenra80
maydaylxy
binbin0505
qq576217788
chjhjj
cutejiu
vincent147147
xxooxxxx
ffmini
asmailgirl
zszszs112233
kmtftnx
422400799
78878878787
xiangzi321
outsider1996
lelemmm
alex5466
hpladyme
baoritian233
yourbalmain
gulanglingmo
liu5375
tremendouslandluminary
perfectcollectorcycle
thereisnonamehere
instagram19
theehentaiouji
parumkaron
speedman789-1-deactivated201610
nvxing
time2grool
sickbabybelle
king-t-stant
moko-lee
joenemesis
potential-dong-deactivated20171
cuntbooks
couseferlt
sosexysadie
xraydy
jingguanqibian
foundpoision
cccloli
aishoucangluolidewo
xiaoxifu2016
926453le
spidergif2
wjh923205664

ipppdi-deactivated20161204
alonso962
yaonv
ganodermabitch
usavich6969-deactivated20151217
asiansexcandy
caoliushequn
wuroubuhuanccc
hardway0923
6yuan
fuckyueyueeveryday
kirakira101-deactivated20171223
sweetlyman
a-l-e-x-a-n-d-e-r
shirocondom
sexy-bozii
asianfeast69
moeyuna
kakakaka101
u44002
dkssud67
55555q
cuteylovelove-deactivated201611
ok77
niceinsideview-deactivated20170
meetslut
minx-in-the-mirror
pussyshavings
zxc1995333
changshamugoumumu
kcorinnnn
spesialforces
av-niang
nowwold
aliceisinlesbianwonderlandagain
arustak74
luolimiao
harrythebigboobies-deactivated2
md-enigma
williueam-deactivated20171219
jobmrleehxx
lovely-seoul-girl
girlisalone-deactivated20171219
dajijidijia
yinyangshi
xiaoxiaobai12138
osquieroatodas
thomen-deactivated20171114
gonemo2015
rlaalwktl
lsaia30678</rich_text></node><node name="tumblr api" prog_lang="custom-colors" readonly="False" tags="" unique_id="49"><rich_text>#检索博客的信息
方法：https://</rich_text><rich_text background="#ffffff" foreground="#000000">api.tumblr.com/v2/blog/{blog-identifier}/info?api_key={key}</rich_text><rich_text>
请求参数： 博客地址 api_key
例子：https://api.tumblr.com/v2/blog/godbells.tumblr.com/info?api_key=9VhS4ytWYGNFnyYSItPKdZNxxlDkTh8hDI6IEBxCGmZCTuvrad
</rich_text><rich_text justification="left"></rich_text><rich_text>

#检索博客的头像
方法：https://</rich_text><rich_text background="#ffffff" foreground="#000000">api.tumblr.com/v2/blog/{blog-identifier}/avatar[/size]</rich_text><rich_text>
请求参数：博客地址 头像大小
例子：https://api.tumblr.com/v2/blog/david.tumblr.com/avatar/512
</rich_text><rich_text justification="left"></rich_text><rich_text>
#检索博客的喜欢
方法：https://</rich_text><rich_text background="#ffffff" foreground="#000000">api.tumblr.com/v2/blog/{blog-identifier}/likes?api_key={key}</rich_text><rich_text>
请求参数：limit每次获得20个 offset偏移最大是1000 before某个时间之后的帖子 after某个时间之前的帖子
例子：https://</rich_text><rich_text background="#ffffff" foreground="#000000">api.tumblr.com/v2/blog/godbells.tumblr.com/likes?api_key=</rich_text><rich_text>9VhS4ytWYGNFnyYSItPKdZNxxlDkTh8hDI6IEBxCGmZCTuvrad
</rich_text><rich_text justification="left"></rich_text><rich_text>
</rich_text><table char_offset="217" col_max="200" col_min="100"><row><cell>title</cell><cell>字符串</cell><cell>博客的名字博客的显示标题</cell></row><row><cell>posts</cell><cell>数字</cell><cell>博客帖子总数</cell></row><row><cell>name</cell><cell>字符串</cell><cell>博客用户</cell></row><row><cell>updated</cell><cell>数字</cell><cell>最近一篇文章的时间，以秒为单位</cell></row><row><cell>description</cell><cell>字符串</cell><cell>博客的描述</cell></row><row><cell>ask</cell><cell>布尔</cell><cell>博客是否允许提问</cell></row><row><cell>ask_anon</cell><cell>布尔</cell><cell>博客是否允许匿名问题</cell></row><row><cell>likes</cell><cell>数字</cell><cell>喜欢帖子的数量</cell></row><row><cell>is_blocked_from_primary</cell><cell>布尔</cell><cell>博客是否已被主叫用户的主博客阻止</cell></row><row><cell>响应字段</cell><cell>类型</cell><cell>描述</cell></row></table><table char_offset="372" col_max="500" col_min="100"><row><cell>avatar_ur</cell><cell>字符串</cell><cell>头像的地址,如果是html浏览器直接得到头像图片</cell></row><row><cell>响应字段</cell><cell>类型</cell><cell>描述</cell></row></table><table char_offset="639" col_max="500" col_min="100"><row><cell>liked_posts</cell><cell>数组</cell><cell>用户喜欢的帖子信息</cell></row><row><cell>liked_count</cell><cell>数字</cell><cell>喜欢帖子的总数</cell></row><row><cell>响应字段</cell><cell>类型</cell><cell>描述</cell></row></table></node><node name="待爬" prog_lang="custom-colors" readonly="False" tags="" unique_id="61"><rich_text>hnchk #图片总数:434789, 视频总数:200
davidaedwards #图片总数:427044, 视频总数:1264
teenfucksleep #视频总数:46952, 图片总数:193
netflixandchill #图片总数:47789, 视频总数:1564

















</rich_text></node><node name="pytumblr" prog_lang="sh" readonly="False" tags="" unique_id="51"><rich_text>pip install pytumblr #安装pytumblr包以及相关依赖
更改原有包的requests中相关方法,加入代理 那么就可以使用本地ssr代理访问tumblr使用提供的api
更改pytumblr包下面的request.py文件 加入代理
__proxies = {
        'http': 'http://127.0.0.1:1080',
        'https': 'http://127.0.0.1:1080',
    }
def get(self, url, params):
        &quot;&quot;&quot;
        Issues a GET request against the API, properly formatting the params

        :param url: a string, the url you are requesting
        :param params: a dict, the key-value of all the paramaters needed
                       in the request
        :returns: a dict parsed of the JSON response
        &quot;&quot;&quot;
        url = self.host + url
        if params:
            url = url + &quot;?&quot; + urllib.parse.urlencode(params)

        try:
            resp = requests.get(url, allow_redirects=False, headers=self.headers, auth=self.oauth,proxies=self.__proxies) #这里加入代理
        except TooManyRedirects as e:
            resp = e.response

        return self.json_parse(resp)

def post(self, url, params={}, files=[]):
        &quot;&quot;&quot;
        Issues a POST request against the API, allows for multipart data uploads

        :param url: a string, the url you are requesting
        :param params: a dict, the key-value of all the parameters needed
                       in the request
        :param files: a list, the list of tuples of files

        :returns: a dict parsed of the JSON response
        &quot;&quot;&quot;
        url = self.host + url
        try:
            if files:
                return self.post_multipart(url, params, files)                              #这个方法也需要更改
            else:
                data = urllib.parse.urlencode(params)
                if not PY3:
                    data = str(data)
                resp = requests.post(url, data=data, headers=self.headers, auth=self.oauth,proxies=self.__proxies) #这里加入代理
                return self.json_parse(resp)
        except HTTPError as e:
            return self.json_parse(e.response)

def post_multipart(self, url, params, files):
        &quot;&quot;&quot;
        Generates and issues a multipart request for data files

        :param url: a string, the url you are requesting
        :param params: a dict, a key-value of all the parameters
        :param files:  a dict, matching the form '{name: file descriptor}'

        :returns: a dict parsed from the JSON response
        &quot;&quot;&quot;
        resp = requests.post(
            url,
            data=params,
            params=params,
            files=files,
            headers=self.headers,
            allow_redirects=False,
            auth=self.oauth,
			proxies=self.__proxies    #这里加入代理
        )
        return self.json_parse(resp)
发布图片:
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808

import pytumblr
client = pytumblr.TumblrRestClient(
    '9VhS4ytWYGNFnyYSItPKdZNxxlDkTh8hDI6IEBxCGmZCTuvrad',
    'nWu7pm8HjNlpn0VPzNkcZKfLB15Y4VInUfrrBT5gpY5oj2q7Th',
    'lgnKPjMutSSTun3Tw6h9HsBRr02Rqw0Qj2uwlInlCqxYv5RsGx',
    'vdpYKS1s5epZNXmvhGcUUIfOL21U0ITrcE1jEppjimqA4N7MLT')

#a = client.follow('2ndpussyportrait.tumblr.com')
A = client.create_photo('godbells', state=&quot;published&quot;, tags=[&quot;testing&quot;, &quot;ok&quot;],
                        source=&quot;https://68.media.tumblr.com/b965fbb2e501610a29d80ffb6fb3e1ad/tumblr_n55vdeTse11rn1906o1_500.jpg&quot;)
# testReblog = client.reblog('2ndpussyportrait.tumblr.com', id=114604786040, reblog_key='frPLDmSW', comment='test')
print(A)
转发文章:
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# With Pycharm
# author: Kylin
# email: kylinshine@163.com
# QQ: 78619808

import pytumblr
client = pytumblr.TumblrRestClient(
    '9VhS4ytWYGNFnyYSItPKdZNxxlDkTh8hDI6IEBxCGmZCTuvrad',
    'nWu7pm8HjNlpn0VPzNkcZKfLB15Y4VInUfrrBT5gpY5oj2q7Th',
    'lgnKPjMutSSTun3Tw6h9HsBRr02Rqw0Qj2uwlInlCqxYv5RsGx',
    'vdpYKS1s5epZNXmvhGcUUIfOL21U0ITrcE1jEppjimqA4N7MLT')

#a = client.follow('2ndpussyportrait.tumblr.com')
# A = client.create_photo('godbells', state=&quot;published&quot;, tags=[&quot;testing&quot;, &quot;ok&quot;],
#                         source=&quot;https://68.media.tumblr.com/b965fbb2e501610a29d80ffb6fb3e1ad/tumblr_n55vdeTse11rn1906o1_500.jpg&quot;)
A = client.reblog('godbells',
                  id=168531815700,
                  reblog_key='nIiU8Jy1',
                  comment='&lt;/p&gt;还在为翻墙找不着梯子烦恼吗，还在为梯子速度不佳看视频卡顿烦恼吗，老哥梯子服务超过200人，速度快质量佳,买梯子就买老哥梯子！&lt;/font&gt;&lt;/font&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;&lt;a href=&quot;http://www.lgssr.org&quot;&gt;&lt;font style=&quot;vertical-align: inherit;&quot;&gt;点我注册&lt;/font&gt;&lt;/font&gt;')
print(A)

数据库去重:
select video from posts  #以video资源为分组对象,筛选出重复次数超过1的资源
where video&lt;&gt;''
group by video 
having count(*)&gt;1

select video from posts  #以video资源为分组对象,筛选出不重复的
where video&lt;&gt;''
group by video 
having count(*)=1

select id,reblogkey,video    #查出重复资源
from posts
where video in
(select video from posts  
where video&lt;&gt;''
group by video 
having count(*)&gt;1)



select max(id),video        #对重复的资源分组,找出id最大的
from posts
where video in
(select video from posts  
where video&lt;&gt;''
group by video 
having count(*)&gt;1)
group by video


select id,reblogkey,video from posts #筛选出资源重复后去重，保留id最大的一个,以及上面不重复的,就是最后去重的结果
where id in 
(select max(id),video        
from posts
where video in
(select video from posts  
where video&lt;&gt;''
group by video 
having count(*)&gt;1)
group by video)



CREATE table temp   #建立一个新表存放非重复资源
AS
select id,reblogkey,video from posts
where id in 
(select max(id) as id      
from posts
where video in
(select video from posts  
where video&lt;&gt;''
group by video 
having count(*)&gt;1)
group by video)


insert into temp   #把post01的非重复资源写入
select id,reblogkey,video from posts01
where id in 
(select max(id) as id      
from posts01
where video in
(select video from posts01  
where video&lt;&gt;''
group by video 
having count(*)&gt;1)
group by video)


INSERT into temp  #本身就不重复的资源 写入
select id,reblogkey,video from posts01
where video in 
(select video from posts01  
where video&lt;&gt;''
group by video 
having count(*)=1)

INSERT into remp  #本身就不重复的资源 写入
select id,reblogkey,video from posts
where video in 
(select video from posts  
where video&lt;&gt;''
group by video 
having count(*)=1)


select video from temp  #找出post和post01表重复的资源
where video&lt;&gt;''
group by video 
having count(*)&gt;1


CREATE table publish   #对reblog表进行清洗写入publish
as
select id,reblogkey,video from temp
where id in 
(select max(id) as id      
from temp
where video in
(select video from temp  
group by video 
having count(*)&gt;1)
group by video)

INSERT into publish  #本身就不重复的资源 写入
select id,reblogkey,video from temp
where video in 
(select video from temp 
group by video 
having count(*)=1)


CREATE table reblog   #最后清洗结果放入reblog
as
select distinct id,reblogkey,video from publish   #查询有没有重复

归纳:可以考虑用replace into 这样就避免重复插入的情况
















</rich_text></node><node name="rclone" prog_lang="custom-colors" readonly="False" tags="" unique_id="66"><rich_text>centos 上用rclone挂在ftp google drive 、dropbox
root# mkdir download
root# wget https://downloads.rclone.org/v1.43.1/rclone-v1.43.1-linux-amd64.zip
root# yum install -y unzip fuse #fuse用来挂载使用的
root# </rich_text><rich_text background="#f8f8f8" foreground="#000000">unzip rclone-v1.43.1-linux-amd64.zip
root# chmod -R 755 rclone-v1.43.1-linux-amd64
root# cp rclone-v1.43.1-linux-amd64/rclone /usr/bin/ 
root# rm -rf </rich_text><rich_text>rclone-v1.43.1-linux-amd64 rclone-v1.43.1-linux-amd64.zip
root# </rich_text><rich_text background="#f8f8f8" foreground="#000000">rclone config</rich_text><rich_text>
n/s/q&gt;n
name&gt; ftp
Storage&gt; 9
host &gt; test.test.com
user &gt;test
port &gt;21
y/g &gt; y
password&gt;123456
y/e/d &gt; y
e/n/d/r/c/s/q&gt;q
root# rclone lsd myftp:HY-NAS/temp   #远程服务器HY-NAS目录下的temp目录
root# </rich_text><rich_text background="#f7f7f7" foreground="#4d4d4c">rclone sync /home/tumblr/mytumblr/db/tumblr.db </rich_text><rich_text>myftp:HY-NAS/temp  #同步文件到远程ftp下
root# </rich_text><rich_text background="#f8f8f8" foreground="#000000">rclone mkdir myftp</rich_text><rich_text>:HY-NAS/test  #远程创建文件夹
root# </rich_text><rich_text background="#f8f8f8" foreground="#000000">rclone mount myftp:HY-NAS/temp  /tmp/</rich_text><rich_text>ftp  #挂在远程目录到本地
root# </rich_text><rich_text background="#f8f8f8" foreground="#000000">rclone copy /home/tumblr/mytumblr/db/tumblr.db </rich_text><rich_text>myftp:HY-NAS  #上传到远程ftp
root# </rich_text><rich_text background="#f8f8f8" foreground="#000000">rclone copy myftp:HY-NAS/tumblr.db /home/</rich_text><rich_text>  #下载远程到本地
root# nohup rclone mount myftp:HY-NAS/temp /tmp/ftp &amp; #后台挂载
root# screen -S rclone #开一个会话
root# rclone mount myftp:HY-NAS/temp /tmp/ftp #后台挂载,ctrl+A+D断开当前会话
bwg下载tumblr到mega
root#</rich_text><rich_text background="#f8f8f8" foreground="#000000">rclone config</rich_text><rich_text>
n/s/q&gt;n
name&gt;mega
Storage&gt; 15
user &gt;test
password&gt;123456
y/e/d &gt; y
e/n/d/r/c/s/q&gt;q
root# rclone lsd mega:   #读取目录,注意mega后面的有个冒号
root# cd home &amp; mkdir mega #建一个挂载的目录
root# screen -S rclone #开一个会话
root# rclone mount mega:/tumblr /home/mega #后台挂载,ctrl+A+D断开当前会话
root# screen -S down #开一个会话
root# wget -P /home/mega -c -i down.txt #用wget下载txt里的文件到mega,注意-P的大写,下载到挂载的目录即上传到了网盘






</rich_text></node><node name="zeronet" prog_lang="custom-colors" readonly="False" tags="" unique_id="62"><rich_text>zeronet的数据库在data文件夹 采用的json和sqlite两种格式来保存的
play这个站点的数据库是以下解释
select * from title where name like 'Black Panther%'
imdb_id:tt1825683  #imdb的电影id
name : Black Panther  #名字
data: 2018|Action, Adventure, Sci-Fi|2h14m|7.7|xjDjIWPwcPU|822,3922,296 #年|导演|时长|评分|youtube链接|822图片id 3922不知道反正和图片有关296是图片高
dl:720p:1.63GB:d6b8537a6d43718374625ae6fec65b3af91ad719|1080p:10.94GB:ae9ca24b383db173b4086127795d0de3031534a4|SD:0.99GB:751212cf4ac3e09f5de7b6ba90cef5bd1578f085 #清晰度|大小|磁力链接
json_id:24 #json这个表24id对应的真实json是movies_22</rich_text></node></cherrytree>